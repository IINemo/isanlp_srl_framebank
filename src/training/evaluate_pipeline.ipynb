{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.py\t\t pipeline_eval.ipynb\r\n",
      "errors_w2v_x_w2v.pkl\t prepare_dataset.ipynb\r\n",
      "evaluate pipeline.ipynb  prepare_dataset.py\r\n",
      "extract_features.ipynb\t README.md\r\n",
      "extract_features.py\t resplit data & retrain models.ipynb\r\n",
      "feature_modeling.py\t sample.log\r\n",
      "ling_parse.py\t\t train_framebank_parser.ipynb\r\n",
      "log_idx.txt\t\t training_utils.py\r\n",
      "log.txt\t\t\t train_model.ipynb\r\n",
      "ModelEvaluation.ipynb\t vectorization.py\r\n",
      "models.py\t\t vectorize_data.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint as print_\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "from numpy.random import RandomState\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from isanlp_srl_framebank.pipeline_default import PipelineDefault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "roleset44 = {\n",
    " 'содержание высказывания',\n",
    " 'говорящий',\n",
    " 'субъект социального отношения',\n",
    " 'субъект психологического состояния',\n",
    " 'содержание действия',\n",
    " 'агенс',\n",
    " 'тема',\n",
    " 'конечная точка',\n",
    " 'сфера',\n",
    " 'контрагент',\n",
    " 'субъект перемещения',\n",
    " 'причина',\n",
    " 'субъект поведения',\n",
    " 'ситуация в фокусе',\n",
    " 'исходный посессор',\n",
    " 'субъект физиологической реакции',\n",
    " 'адресат',\n",
    " 'пациенс',\n",
    " 'срок',\n",
    " 'источник звука',\n",
    " 'место',\n",
    " 'признак',\n",
    " 'потенциальная угроза',\n",
    " 'субъект ментального состояния',\n",
    " 'конечный посессор',\n",
    " 'результат',\n",
    " 'стимул',\n",
    " 'субъект восприятия',\n",
    " 'эффектор',\n",
    " 'траектория',\n",
    " 'содержание мысли',\n",
    " 'пациенс перемещения',\n",
    " 'каузатор',\n",
    " 'предмет высказывания',\n",
    " 'начальная точка',\n",
    " 'способ',\n",
    " 'пациенс социального отношения',\n",
    " 'статус',\n",
    " 'предмет мысли',\n",
    " 'цель',\n",
    " 'потенциальный пациенс',\n",
    " 'контрагент социального отношения',\n",
    " 'эталон',\n",
    " 'признак действия'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGUMENT_POSTAGS = {\n",
    "        'NOUN',\n",
    "        'PRON',\n",
    "        'ADJ',\n",
    "        'PROPN'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roles_pred(lemma, role_annot, part_id):\n",
    "    ann_sent = role_annot[part_id]\n",
    "    predicates = {}\n",
    "    arguments = {}\n",
    "    for event in ann_sent:\n",
    "        predicate = {\n",
    "            'lemma': lemma[part_id][event.pred[0]],\n",
    "        }\n",
    "        predicates[event.pred[0]] = predicate\n",
    "        arguments[event.pred[0]] = []\n",
    "        for arg in event.args:\n",
    "            argument = {\n",
    "                'tag': arg.tag,\n",
    "                'lemma': lemma[part_id][arg.begin],\n",
    "                'idx': arg.begin\n",
    "            }\n",
    "            arguments[event.pred[0]].append(argument)\n",
    "            \n",
    "    return predicates, arguments\n",
    "\n",
    "\n",
    "def get_example(corpus, ex_number, part_id):\n",
    "    words = []\n",
    "    for obj in corpus[ex_number][1][part_id]:\n",
    "        word = obj['form']\n",
    "        for symbol in ':;,.!?':\n",
    "            word = word.replace(' ' + symbol, symbol)\n",
    "        words.append(word)\n",
    "        \n",
    "    if words:\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return '_'  \n",
    "\n",
    "\n",
    "def get_roles_true(annot, corpus, ex_number, part_id):\n",
    "    predicates = {}\n",
    "    arguments = {}\n",
    "    postags = [item for sublist in annot['postag'] for item in sublist]\n",
    "    for i, obj in enumerate(corpus[ex_number][1][part_id]):\n",
    "        if 'rank' in obj:\n",
    "            if obj['rank'] == 'Предикат':\n",
    "                predicate = {\n",
    "                    'lemma': obj['lemma']\n",
    "                }\n",
    "                predicates[i] = predicate\n",
    "            else:\n",
    "                if 'lemma' not in obj:\n",
    "                    argument = {\n",
    "                        'lemma': obj['form'],\n",
    "                        'tag': obj['rolepred1'],\n",
    "                        'idx': i\n",
    "                    }\n",
    "                else:\n",
    "                    argument = {\n",
    "                        'lemma': obj['lemma'],\n",
    "                        'tag': obj['rolepred1'],\n",
    "                        'idx': i\n",
    "                    }\n",
    "                \n",
    "                argument['postag'] = postags[argument['idx']]\n",
    "                pred_id = obj['fillpred']\n",
    "                if pred_id not in arguments.keys():\n",
    "                    arguments[pred_id] = []\n",
    "                arguments[pred_id].append(argument)\n",
    "                \n",
    "    return predicates, arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_texts(corpus, ppl, n_samples=100):\n",
    "    if len(corpus) > n_samples:\n",
    "        np.random.seed(random_seed)\n",
    "        samples_idxs = np.random.choice(len(corpus), size=n_samples)\n",
    "    else:\n",
    "        samples_idxs = [_ for _ in range(len(corpus))]\n",
    "    \n",
    "    texts = [get_example(corpus, ex_num, 0) for ex_num in samples_idxs]\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_predictions(corpus, ppl, n_samples=100):\n",
    "    if len(corpus) > n_samples:\n",
    "        np.random.seed(random_seed)\n",
    "        samples_idxs = np.random.choice(len(corpus), size=n_samples)\n",
    "    else:\n",
    "        samples_idxs = [_ for _ in range(len(corpus))]\n",
    "    \n",
    "    texts = [get_example(corpus, ex_num, 0) for ex_num in samples_idxs]\n",
    "    \n",
    "    annotations = [ppl(text) for text in tqdm(texts, desc='Analyzing texts')]\n",
    "    pred_roles = [get_roles_pred(res['lemma'], res['srl'], 0) for res in annotations]\n",
    "    \n",
    "    true_roles = [get_roles_true(annotations[i], corpus, ex_num, 0) for i, ex_num in enumerate(samples_idxs)]\n",
    "    \n",
    "    \n",
    "    repl_roles = {\n",
    "        'агенс - субъект восприятия' : 'субъект восприятия',\n",
    "        'агенс - субъект ментального состояния' : 'субъект ментального состояния',\n",
    "        'результат / цель' : 'результат',\n",
    "        'место - пациенс' : 'место',\n",
    "        'говорящий - субъект психологического состояния' : 'субъект психологического состояния'\n",
    "    }\n",
    "    \n",
    "    for role, val in repl_roles.items():\n",
    "        for pair in true_roles:\n",
    "            for _, args in pair[1].items():\n",
    "                for arg in args:\n",
    "                    arg['tag'] = arg['tag'].replace(role, val)\n",
    "                    \n",
    "    return true_roles, pred_roles, texts\n",
    "\n",
    "\n",
    "def compute_metrics(y_pred, y_true, report_to=sys.stdout, roleset=roleset44, idxmatching=False):\n",
    "    true_positive = 0\n",
    "    condition_positive = 0\n",
    "    predicted_condition_positive = 0\n",
    "    error_examples = []\n",
    "    \n",
    "    print_func = lambda x: print(x, file=report_to)\n",
    "    \n",
    "    for i, (true_predicates, true_arguments) in enumerate(y_true):\n",
    "        print_func(f\"Inspecting example {i}\")\n",
    "        print_func(f\"Expecting true predicates {true_predicates}\")\n",
    "        print_func(f\"Expecting true arguments  {true_arguments}\")\n",
    "        \n",
    "        pred_predicates, pred_arguments = y_pred[i]\n",
    "\n",
    "        print_func(f\"Got predicted predicates  {pred_predicates}\")\n",
    "        print_func(f\"Got predicted arguments   {pred_arguments}\")\n",
    "        \n",
    "        print_func(\"-\"*60)\n",
    "        \n",
    "        for true_pred_idx, true_predicate in true_predicates.items():\n",
    "            if true_pred_idx in pred_predicates:\n",
    "                print_func(f\"Matched predicate {true_pred_idx} = {true_predicate}\")\n",
    "                \n",
    "                true = true_arguments[true_pred_idx]\n",
    "                pred_arguments_i = pred_arguments[true_pred_idx]\n",
    "                \n",
    "                true_arguments_i = []\n",
    "                \n",
    "                for idx, true_argument in enumerate(true):\n",
    "                    if true_argument['tag'] in roleset and true_argument['postag'] in ARGUMENT_POSTAGS:\n",
    "                        true_arguments_i.append(true[idx])\n",
    "                        \n",
    "                if true_arguments_i:\n",
    "                    print_func(f\"Expecting arguments  {true_arguments_i}\")\n",
    "                    print_func(f\"Got predicted        {pred_arguments_i}\")\n",
    "                    print_func(f\"Predicted Condition Positive = {len(pred_arguments_i)}\")\n",
    "                    print_func(f\"Condition Positive           = {len(true_arguments_i)}\")\n",
    "                    condition_positive += len(true_arguments_i)\n",
    "                    condition_positive_i = len(true_arguments_i)\n",
    "                    predicted_condition_positive += len(pred_arguments_i)\n",
    "\n",
    "                    true_positive_i = 0\n",
    "\n",
    "                    error_report = {\n",
    "                        'example_idx' : i,\n",
    "                        'predicate': true_predicate,\n",
    "                        'true_arguments' : true_arguments_i,\n",
    "                        'predicted_arguments': pred_arguments_i\n",
    "                    }\n",
    "\n",
    "                    for j, obj in enumerate(true_arguments_i):\n",
    "                        true_tag = obj['tag']\n",
    "                        true_lemma = obj['lemma']\n",
    "                        true_idx = obj['idx']\n",
    "                        for obj_pred in pred_arguments_i:\n",
    "                            if idxmatching:\n",
    "                                if obj_pred['idx'] == true_idx:\n",
    "                                    true_positive_i += 1\n",
    "                            else:\n",
    "                                if obj_pred['idx'] == true_idx and obj_pred['tag'] == true_tag:\n",
    "                                    true_positive_i += 1\n",
    "\n",
    "                    print_func(f\"True Positive = {true_positive_i}\")\n",
    "                    if true_positive_i != condition_positive_i:\n",
    "                        error_examples.append(error_report)\n",
    "\n",
    "                    true_positive += true_positive_i\n",
    "                \n",
    "        print_func(\"=\"*60)\n",
    "       \n",
    "    recall = true_positive/condition_positive\n",
    "    precision = true_positive/predicted_condition_positive\n",
    "    \n",
    "    return {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': 2 * ((precision*recall)/(precision+recall)),\n",
    "        'errors': error_examples\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_name = 'annotated_corpus_fixed+syntaxnet.json'\n",
    "#corpus_path = '../../data/cleared_corpus.json'\n",
    "corpus_path = '../../data/test_data.json'\n",
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6523"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import isanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/tchewik/isanlp_srl_framebank.git\n",
      "  Cloning https://github.com/tchewik/isanlp_srl_framebank.git to /tmp/pip-req-build-fktpanay\n",
      "  Running command git clone -q https://github.com/tchewik/isanlp_srl_framebank.git /tmp/pip-req-build-fktpanay\n",
      "  Running command git submodule update --init --recursive -q\n",
      "Building wheels for collected packages: isanl-srl-framebank\n",
      "  Building wheel for isanl-srl-framebank (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for isanl-srl-framebank: filename=isanl_srl_framebank-0.0.1-cp36-none-any.whl size=13502 sha256=9267a0b605f1081d98633170c9aa41256a21d7e9ea9fe4205ecb3351f829d31c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-tt7lzgos/wheels/53/85/ca/d8697e680129d7344263e11de6f9c43c55721e83038ff90d95\n",
      "Successfully built isanl-srl-framebank\n",
      "Installing collected packages: isanl-srl-framebank\n",
      "  Found existing installation: isanl-srl-framebank 0.0.1\n",
      "    Uninstalling isanl-srl-framebank-0.0.1:\n",
      "      Successfully uninstalled isanl-srl-framebank-0.0.1\n",
      "Successfully installed isanl-srl-framebank-0.0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install -U git+https://github.com/tchewik/isanlp_srl_framebank.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics of parser quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'vmh2.isa.ru'\n",
    "host2 = 'echistova.isa.ru'\n",
    "host3 = 'tsa05.isa.ru'\n",
    "\n",
    "ppl = PipelineDefault(address_morph=(host, 4333),\n",
    "                       address_syntax=(host, 4344),\n",
    "                       address_srl=(host3, 4335))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Мама мыла раму.',\n",
       " 'tokens': [<isanlp.annotation.Token at 0x7f031ad3e9e8>,\n",
       "  <isanlp.annotation.Token at 0x7f031ad3ef60>,\n",
       "  <isanlp.annotation.Token at 0x7f031ad3e860>,\n",
       "  <isanlp.annotation.Token at 0x7f031ad3e7b8>],\n",
       " 'sentences': [<isanlp.annotation.Sentence at 0x7f031ad3e470>],\n",
       " 'mystem_postag': [['S,жен,од=им,ед',\n",
       "   'V,несов,пе=прош,ед,изъяв,жен',\n",
       "   'S,жен,неод=вин,ед',\n",
       "   '']],\n",
       " 'lemma': [['мама', 'мыть', 'рама', '.']],\n",
       " 'syntax_dep_tree': [[<isanlp.annotation.WordSynt at 0x7f0317251dd8>,\n",
       "   <isanlp.annotation.WordSynt at 0x7f0317251f60>,\n",
       "   <isanlp.annotation.WordSynt at 0x7f03172517f0>,\n",
       "   <isanlp.annotation.WordSynt at 0x7f0317251cf8>]],\n",
       " 'morph': [[{'fPOS': 'NOUN',\n",
       "    'Gender': 'Fem',\n",
       "    'Animacy': 'Anim',\n",
       "    'Case': 'Nom',\n",
       "    'Number': 'Sing'},\n",
       "   {'fPOS': 'VERB',\n",
       "    'Aspect': 'Imp',\n",
       "    'Valency': 'TR',\n",
       "    'Tense': 'Past',\n",
       "    'Number': 'Sing',\n",
       "    'VerbForm': 'Fin',\n",
       "    'Gender': 'Fem'},\n",
       "   {'fPOS': 'NOUN',\n",
       "    'Gender': 'Fem',\n",
       "    'Animacy': 'Inan',\n",
       "    'Case': 'Acc',\n",
       "    'Number': 'Sing'},\n",
       "   {}]],\n",
       " 'postag': [['NOUN', 'VERB', 'NOUN', '']],\n",
       " 'srl': [[<isanlp.annotation.Event at 0x7f0332bc7630>]]}"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppl('Мама мыла раму.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (это) содержание мысли\n",
      "4 (ты) субъект психологического состояния\n",
      "13 (он) пациенс\n",
      "15 (катя) говорящий\n"
     ]
    }
   ],
   "source": [
    "res = ppl('- И за это тебя посадят, -- как бы сообразив, прервала его тётя Катя.')\n",
    "\n",
    "for event in res['srl'][0]:\n",
    "    for i in range(len(event.args)):\n",
    "        print(event.args[i].begin, f\"({res['lemma'][0][event.args[i].begin]})\", event.args[i].tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 (это) цель\n",
      "4 (ты) пациенс\n",
      "13 (он) пациенс\n",
      "15 (катя) говорящий\n"
     ]
    }
   ],
   "source": [
    "res = ppl('- И за это тебя посадят, -- как бы сообразив, прервала его тётя Катя.')\n",
    "\n",
    "for event in res['srl'][0]:\n",
    "    for i in range(len(event.args)):\n",
    "        print(event.args[i].begin, f\"({res['lemma'][0][event.args[i].begin]})\", event.args[i].tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_samples=400 (usually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All the action goes here\n",
    "\n",
    "Evaluate the pipeline on all the test data we used to evaluate the srl models performance. \n",
    "\n",
    "**1. Syntaxnet**\n",
    "- Current syntaxnet parser refuses to process more than ~720 texts in a row without throwing an error, just stucks;\n",
    "- 720 texts chunk processes ~15 min;\n",
    "- ``random_predictions`` function now does not randomize input data if n_samples==len(data);\n",
    "- ``get_example`` function now returns '\\_' instead of empty strings because of a bug in the pipeline: it stucks on empty texts **(how is it even possible for this function to return nothing? see example #1613 in test set)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.processor_syntaxnet_remote import ProcessorSyntaxNetRemote\n",
    "from isanlp import PipelineCommon\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "\n",
    "\n",
    "class PipelineDefault:\n",
    "    def __init__(self, address_morph, address_syntax, address_srl):\n",
    "        self._ppl = PipelineCommon([(ProcessorRemote(address_morph[0], address_morph[1], 'default'), \n",
    "                                     ['text'], \n",
    "                                     {'tokens' : 'tokens', \n",
    "                                      'sentences' : 'sentences',\n",
    "                                      'postag' : 'mystem_postag',\n",
    "                                      'lemma' : 'lemma'}),\n",
    "                                     (ProcessorSyntaxNetRemote(address_syntax[0], address_syntax[1]),\n",
    "                                      ['tokens', 'sentences'],\n",
    "                                      {'syntax_dep_tree' : 'syntax_dep_tree'}),\n",
    "                                     (ConverterMystemToUd(), \n",
    "                                      ['mystem_postag'],\n",
    "                                      {'morph' : 'morph',\n",
    "                                       'postag' : 'postag'}),\n",
    "                                     (ProcessorRemote(address_srl[0], address_srl[1], 'default'),\n",
    "                                      ['tokens', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "                                      {'srl' : 'srl'})])\n",
    "        self._name = 'default'\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self._ppl(*args, **kwargs)\n",
    "    \n",
    "    def get_processors(self):\n",
    "        return self._ppl.get_processors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = 'vmh2.isa.ru'\n",
    "host2 = 'echistova.isa.ru'\n",
    "host3 = 'tsa05.isa.ru'\n",
    "\n",
    "syntax_ports = [4340, 4341, 4342, 4343, 4344]\n",
    "\n",
    "ppls = PipelineDefault(address_morph=(host, 4333),\n",
    "                       address_syntax=(host, syntax_ports[0]),\n",
    "                       address_srl=(host3, 4335))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch syntax parser to #0, compute examples ##0-651/6523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3afba82eb4c244b1b19561a8e202ec0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Analyzing texts', max=652), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch syntax parser to #1, compute examples ##652-1303/6523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b11f4d55284e73a74fd5b946a73ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Analyzing texts', max=652), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch syntax parser to #2, compute examples ##1304-1955/6523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60f8b8275d84e51882c79da69febe19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Analyzing texts', max=652), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch syntax parser to #3, compute examples ##1956-2607/6523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "086a54817b1643bb88710e4145ffbc01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Analyzing texts', max=652), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch syntax parser to #4, compute examples ##2608-3259/6523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8a18cab7a843cfae80142079a829a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Analyzing texts', max=652), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Switch syntax parser to #0, compute examples ##3260-3911/6523\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5a158ea96d4400acb6005da26e64d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Analyzing texts', max=652), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = 0\n",
    "true_roles, pred_roles, texts = [], [], []\n",
    "number_of_chunks = 9\n",
    "\n",
    "for counter in range(number_of_chunks):\n",
    "    ppls = PipelineDefault(address_morph=(host, 4333),\n",
    "                           address_syntax=(host, syntax_ports[counter % len(syntax_ports)]),\n",
    "                           address_srl=(host3, 4335))\n",
    "    print(f\"Switch syntax parser to #{counter % len(syntax_ports)}, compute examples ##{start}-{start+len(corpus)//10-1}/{len(corpus)}\")\n",
    "    tmp_true, tmp_pred, tmp_texts = random_predictions(corpus[start:start+len(corpus)//10], ppls, n_samples=len(corpus)//10)\n",
    "    true_roles += tmp_true\n",
    "    pred_roles += tmp_pred\n",
    "    texts += tmp_texts\n",
    "    start += len(corpus) // 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'log_idx.txt'\n",
    "results = compute_metrics(y_pred=pred_roles, y_true=true_roles, \n",
    "                          report_to=open(log_path, 'w', encoding='utf-8'), \n",
    "                          idxmatching=True)\n",
    "\n",
    "copyres = dict(results)\n",
    "del copyres['errors']\n",
    "print_(copyres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'log_idx.txt'\n",
    "results = compute_metrics(y_pred=pred_roles, y_true=true_roles, \n",
    "                          report_to=open(log_path, 'w', encoding='utf-8'), \n",
    "                          idxmatching=False)\n",
    "\n",
    "copyres = dict(results)\n",
    "del copyres['errors']\n",
    "print_(copyres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. UDPipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/IINemo/isanlp.git@dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp import PipelineCommon\n",
    "from isanlp.processor_remote import ProcessorRemote\n",
    "from isanlp.ru.converter_mystem_to_ud import ConverterMystemToUd\n",
    "\n",
    "ppl = PipelineCommon([(ProcessorRemote(host, 4333, 'default'),\n",
    "                            ['text'],\n",
    "                            {'sentences' : 'sentences', \n",
    "                             'tokens' : 'tokens',\n",
    "                             'postag' : 'mystem_postags',\n",
    "                             'lemma' : 'lemma'}),\n",
    "                      (ConverterMystemToUd(), \n",
    "                                      ['mystem_postags'],\n",
    "                                      {'morph' : 'morph',\n",
    "                                       'postag' : 'postag'}),\n",
    "                      (ProcessorRemote(host, 5336, '0'), \n",
    "                            ['tokens', 'sentences'], \n",
    "                            {'syntax_dep_tree' : 'syntax_dep_tree'}),\n",
    "                      (ProcessorRemote(host3, 4335, 'default'),\n",
    "                            ['tokens', 'postag', 'morph', 'lemma', 'syntax_dep_tree'],\n",
    "                            {'srl' : 'srl'})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "400 samples: ~8 min on udpipe with w2v, ~20 min with elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384adf1981974908a6dd55fcf452da8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Analyzing texts', max=6523), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "true_roles, pred_roles, texts = random_predictions(corpus, ppl, n_samples=len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'log_idx.txt'\n",
    "results = compute_metrics(y_pred=pred_roles, y_true=true_roles, \n",
    "                          report_to=open(log_path, 'w', encoding='utf-8'), \n",
    "                          idxmatching=True)\n",
    "\n",
    "copyres = dict(results)\n",
    "del copyres['errors']\n",
    "print_(copyres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log_path = 'log_idx.txt'\n",
    "results = compute_metrics(y_pred=pred_roles, y_true=true_roles, \n",
    "                          report_to=open(log_path, 'w', encoding='utf-8'), \n",
    "                          idxmatching=False)\n",
    "\n",
    "copyres = dict(results)\n",
    "del copyres['errors']\n",
    "print_(copyres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors in the argument extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ff = pd.DataFrame(results['errors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neatify(text):\n",
    "    return text.replace(' ,', ',')\\\n",
    "                .replace(' .', '.')\\\n",
    "                .replace(' ) ', ') ')\\\n",
    "                .replace(' ( ', ' (')\\\n",
    "                .replace(' : ', ': ')\n",
    "                \n",
    "ff['text'] = ff.example_idx.map(lambda idx: neatify(texts[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff.to_pickle('errors_elmo_x_w2v.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annot = ppl(texts[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vars(annot[\"syntax_dep_tree\"][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, ann in enumerate(annot['morph'][0]):\n",
    "    print(f'{i}\\t{annot[\"lemma\"][0][i]}\\t{ann}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ann in enumerate(annot['morph'][0]):\n",
    "    print(f'{i}\\t{annot[\"tokens\"][i].text}\\t{ann}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ann in enumerate(annot['morph'][0]):\n",
    "    print(f'{i}\\t{annot[\"tokens\"][i].text}\\t{ann.get(\"fPOS\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, ann in enumerate(annot['syntax_dep_tree'][0]):\n",
    "    print(i, vars(ann))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for events in annot['srl']:\n",
    "    for event in events:\n",
    "        print(event.pred, [vars(arg) for arg in event.args])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
