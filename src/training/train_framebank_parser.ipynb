{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deeppavlov\n",
      "  Downloading https://files.pythonhosted.org/packages/03/4f/1f73825653f388ead9a9ea2f46cad51c92bd84a899ebf983906013e14d1c/deeppavlov-0.4.0-py3-none-any.whl (682kB)\n",
      "Requirement already satisfied: h5py==2.8.0 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from deeppavlov) (2.8.0)\n",
      "Collecting flask-cors==3.0.6 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/db/f3495569d5c3e2bdb9fb8a66c54503364abb6f35a9da2227cf5c9c50dc42/Flask_Cors-3.0.6-py2.py3-none-any.whl\n",
      "Collecting fuzzywuzzy==0.16.0 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/36/be990a35c7e8ed9dc176c43b5699cd971cec0b6f9ef858843374171df4f2/fuzzywuzzy-0.16.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pymorphy2-dicts-ru in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from deeppavlov) (2.4.404381.4453942)\n",
      "Collecting numpy==1.14.5 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
      "Collecting pytelegrambotapi==3.5.2 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/5a/7aab147b253f19e5ef007316f39cf693a63d5cd7f654c3805c76f6bde979/pyTelegramBotAPI-3.5.2.tar.gz (51kB)\n",
      "Collecting tqdm==4.23.4 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/93/24/6ab1df969db228aed36a648a8959d1027099ce45fad67532b9673d533318/tqdm-4.23.4-py2.py3-none-any.whl (42kB)\n",
      "Collecting pandas==0.23.1 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/57/eb/6ab533ea8e35e7dd159af6922ac1123d4565d89f3926ad9a6aa46530978f/pandas-0.23.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n",
      "Collecting overrides==1.9 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n",
      "Collecting flask==1.0.2 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/e7/08578774ed4536d3242b14dacb4696386634607af824ea997202cd0edb4b/Flask-1.0.2-py2.py3-none-any.whl (91kB)\n",
      "Collecting flasgger==0.9.1 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/4c/8735be27bebb88b0acbdc9db1d522583db10821aec3d3fb6112df0f41701/flasgger-0.9.1-py2.py3-none-any.whl (4.1MB)\n",
      "Collecting scikit-learn==0.19.1 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/3d/2d/9fbc7baa5f44bc9e88ffb7ed32721b879bfa416573e85031e16f52569bc9/scikit_learn-0.19.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\n",
      "Collecting requests==2.19.1 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
      "Collecting rusenttokenize==0.0.4 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/4c/e2aeee9cdcc266303289ad8c4acfdcf401781646bcc311ee2bf18f84d663/rusenttokenize-0.0.4-py3-none-any.whl\n",
      "Collecting pyopenssl==18.0.0 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/96/af/9d29e6bd40823061aea2e0574ccb2fcf72bfd6130ce53d32773ec375458c/pyOpenSSL-18.0.0-py2.py3-none-any.whl (53kB)\n",
      "Collecting nltk==3.2.5 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/87/76e691bbf1759ad6af5831649aae6a8d2fa184a1bcc71018ca6300399e5f/nltk-3.2.5.tar.gz (1.2MB)\n",
      "Requirement already satisfied: scipy==1.1.0 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from deeppavlov) (1.1.0)\n",
      "Collecting Cython==0.28.5 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/19/8e/32b280abb0947a96cdbb8329fb2014851a21fc1d099009f946ea8a8202c3/Cython-0.28.5-cp36-cp36m-manylinux1_x86_64.whl (3.4MB)\n",
      "Requirement already satisfied: pymorphy2==0.8 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from deeppavlov) (0.8)\n",
      "Collecting keras==2.2.0 (from deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
      "Requirement already satisfied: six in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from h5py==2.8.0->deeppavlov) (1.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from pandas==0.23.1->deeppavlov) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from pandas==0.23.1->deeppavlov) (2018.7)\n",
      "Collecting itsdangerous>=0.24 (from flask==1.0.2->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: Werkzeug>=0.14 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from flask==1.0.2->deeppavlov) (0.14.1)\n",
      "Requirement already satisfied: Jinja2>=2.10 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from flask==1.0.2->deeppavlov) (2.10)\n",
      "Requirement already satisfied: click>=5.1 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from flask==1.0.2->deeppavlov) (7.0)\n",
      "Requirement already satisfied: mistune in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from flasgger==0.9.1->deeppavlov) (0.8.4)\n",
      "Requirement already satisfied: jsonschema>=2.5.1 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from flasgger==0.9.1->deeppavlov) (2.6.0)\n",
      "Requirement already satisfied: PyYAML>=3.0 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from flasgger==0.9.1->deeppavlov) (3.13)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from requests==2.19.1->deeppavlov) (2.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from requests==2.19.1->deeppavlov) (2018.11.29)\n",
      "Collecting urllib3<1.24,>=1.21.1 (from requests==2.19.1->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from requests==2.19.1->deeppavlov) (3.0.4)\n",
      "Collecting cryptography>=2.2.1 (from pyopenssl==18.0.0->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/97/18/c6557f63a6abde34707196fb2cad1c6dc0dbff25a200d5044922496668a4/cryptography-2.7-cp34-abi3-manylinux1_x86_64.whl (2.3MB)\n",
      "Requirement already satisfied: docopt>=0.6 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from pymorphy2==0.8->deeppavlov) (2.4.393442.3710985)\n",
      "Requirement already satisfied: dawg-python>=0.7 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from pymorphy2==0.8->deeppavlov) (0.7.2)\n",
      "Collecting keras-preprocessing==1.0.1 (from keras==2.2.0->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
      "Collecting keras-applications==1.0.2 (from keras==2.2.0->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from Jinja2>=2.10->flask==1.0.2->deeppavlov) (1.1.0)\n",
      "Collecting cffi!=1.11.3,>=1.8 (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/bf/6aa1925384c23ffeb579e97a5569eb9abce41b6310b329352b8252cee1c3/cffi-1.12.3-cp36-cp36m-manylinux1_x86_64.whl (430kB)\n",
      "Collecting asn1crypto>=0.21.0 (from cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
      "Collecting pycparser (from cffi!=1.11.3,>=1.8->cryptography>=2.2.1->pyopenssl==18.0.0->deeppavlov)\n",
      "  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\n",
      "Building wheels for collected packages: pytelegrambotapi, overrides, nltk, pycparser\n",
      "  Running setup.py bdist_wheel for pytelegrambotapi: started\n",
      "  Running setup.py bdist_wheel for pytelegrambotapi: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/b0/a0/fa/c3539fd47aa9f834230d64039c4bc620463bc7afc39b0f3f35\n",
      "  Running setup.py bdist_wheel for overrides: started\n",
      "  Running setup.py bdist_wheel for overrides: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n",
      "  Running setup.py bdist_wheel for nltk: started\n",
      "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/da/8c/38/a8a36581975f8d03275c49960019f955e4d19fd14ae7e42d3d\n",
      "  Running setup.py bdist_wheel for pycparser: started\n",
      "  Running setup.py bdist_wheel for pycparser: finished with status 'done'\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
      "Successfully built pytelegrambotapi overrides nltk pycparser\n",
      "Installing collected packages: itsdangerous, flask, flask-cors, fuzzywuzzy, numpy, urllib3, requests, pytelegrambotapi, tqdm, pandas, overrides, flasgger, scikit-learn, rusenttokenize, pycparser, cffi, asn1crypto, cryptography, pyopenssl, nltk, Cython, keras-preprocessing, keras-applications, keras, deeppavlov\n",
      "  Found existing installation: numpy 1.15.4\n",
      "    Uninstalling numpy-1.15.4:\n",
      "      Successfully uninstalled numpy-1.15.4\n",
      "  Found existing installation: urllib3 1.24.1\n",
      "    Uninstalling urllib3-1.24.1:\n",
      "      Successfully uninstalled urllib3-1.24.1\n",
      "  Found existing installation: requests 2.20.1\n",
      "    Uninstalling requests-2.20.1:\n",
      "      Successfully uninstalled requests-2.20.1\n",
      "  Found existing installation: tqdm 4.28.1\n",
      "    Uninstalling tqdm-4.28.1:\n",
      "      Successfully uninstalled tqdm-4.28.1\n",
      "  Found existing installation: pandas 0.23.4\n",
      "    Uninstalling pandas-0.23.4:\n",
      "      Successfully uninstalled pandas-0.23.4\n",
      "  Found existing installation: scikit-learn 0.20.1\n",
      "    Uninstalling scikit-learn-0.20.1:\n",
      "      Successfully uninstalled scikit-learn-0.20.1\n",
      "  Found existing installation: nltk 3.4\n",
      "    Uninstalling nltk-3.4:\n",
      "      Successfully uninstalled nltk-3.4\n",
      "  Found existing installation: Cython 0.29.1\n",
      "    Uninstalling Cython-0.29.1:\n",
      "      Successfully uninstalled Cython-0.29.1\n",
      "  Found existing installation: Keras-Preprocessing 1.0.5\n",
      "    Uninstalling Keras-Preprocessing-1.0.5:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.0.5\n",
      "  Found existing installation: Keras-Applications 1.0.6\n",
      "    Uninstalling Keras-Applications-1.0.6:\n",
      "      Successfully uninstalled Keras-Applications-1.0.6\n",
      "  Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "Successfully installed Cython-0.28.5 asn1crypto-0.24.0 cffi-1.12.3 cryptography-2.7 deeppavlov-0.4.0 flasgger-0.9.1 flask-1.0.2 flask-cors-3.0.6 fuzzywuzzy-0.16.0 itsdangerous-1.1.0 keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1 nltk-3.2.5 numpy-1.14.5 overrides-1.9 pandas-0.23.1 pycparser-2.19 pyopenssl-18.0.0 pytelegrambotapi-3.5.2 requests-2.19.1 rusenttokenize-0.0.4 scikit-learn-0.19.1 tqdm-4.23.4 urllib3-1.23\n",
      "Collecting tensorflow-hub\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/be/f18c352d84382d9c795a0f37eaf16d42ace7d161fbb0ad20bdcd5e550015/tensorflow_hub-0.5.0-py2.py3-none-any.whl (78kB)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from tensorflow-hub) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from tensorflow-hub) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from tensorflow-hub) (1.14.5)\n",
      "Requirement already satisfied: setuptools in /root/.pyenv/versions/3.6.7/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow-hub) (39.0.1)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow-gpu 1.12.0 has requirement keras-applications>=1.0.6, but you'll have keras-applications 1.0.2 which is incompatible.\n",
      "tensorflow-gpu 1.12.0 has requirement keras-preprocessing>=1.0.5, but you'll have keras-preprocessing 1.0.1 which is incompatible.\n",
      "imbalanced-learn 0.4.3 has requirement scikit-learn>=0.20, but you'll have scikit-learn 0.19.1 which is incompatible.\n",
      "You are using pip version 18.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 18.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install deeppavlov\n",
    "pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:14:55.909843",
     "start_time": "2017-02-18T12:14:54.905927"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use only one GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../isanlp/src/')\n",
    "sys.path.append('../../src/isanlp_srl_framebank/')\n",
    "sys.path.append('../../libs/')\n",
    "sys.path.append('../../libs/pylingtools/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress tensorflow memory appetites\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.log_device_placement=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:14:56.607418",
     "start_time": "2017-02-18T12:14:55.911842"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available GPUs\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import isanlp\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(31)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_corpus_path = '../../data/cleared_corpus.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cleared_corpus_path, 'r') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_data_path = '../../data/results_final_fixed.pckl'\n",
    "with open(ling_data_path, 'rb') as f:\n",
    "    ling_data_cache = pickle.load(f)\n",
    "\n",
    "ling_data_cache = {k: v for k,v in ling_data_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_examples = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation_repr import CSentence\n",
    "from convert_corpus_to_brat import make_text\n",
    "\n",
    "\n",
    "def find_address_by_offset(offset, ling_ann):\n",
    "    for tok_num, tok in enumerate(ling_ann['tokens']):\n",
    "        if tok.begin <= offset and offset < tok.end:\n",
    "            break\n",
    "    \n",
    "    for sent_num, sent in enumerate(ling_ann['sentences']):\n",
    "        if sent.begin <= tok_num and tok_num < sent.end:\n",
    "            break\n",
    "    \n",
    "    return sent_num, tok_num - sent.begin\n",
    "\n",
    "\n",
    "def process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, example):\n",
    "    feature_sets = list()\n",
    "    \n",
    "    text, offset_index = make_text(example, 0)\n",
    "    ling_ann = ling_cache[ex_id]\n",
    "    \n",
    "    pred_offset = offset_index[(pred[0], pred[1])]\n",
    "    pred_ling_sent, pred_ling_word = find_address_by_offset(pred_offset, ling_ann)\n",
    "    \n",
    "    for arg in args:\n",
    "        arg_offset = offset_index[(arg[0], arg[1])]\n",
    "        arg_ling_sent, arg_ling_word = find_address_by_offset(arg_offset, ling_ann)\n",
    "        \n",
    "        fb_pred_word = example[pred[0]][pred[1]]\n",
    "        fb_arg_word = example[arg[0]][arg[1]]\n",
    "        \n",
    "        if arg_ling_sent != pred_ling_sent:\n",
    "            error_examples[ex_id] = {\n",
    "                'reason': 'sent_mismatch',\n",
    "                'arg': arg_ling_sent,\n",
    "                'pred': pred_ling_sent\n",
    "            }\n",
    "            continue\n",
    "            \n",
    "        sentence = ling_ann['sentences'][pred_ling_sent]\n",
    "        tokens = [tok.text for tok in ling_ann['tokens']]\n",
    "        tokens = tokens[sentence.begin:sentence.end]\n",
    "        \n",
    "        role = fb_arg_word['rolepred1']\n",
    "\n",
    "        features = feature_extractor.extract_features(pred_ling_word, \n",
    "                                                      arg_ling_word, \n",
    "                                                      ling_ann['postag'][arg_ling_sent],\n",
    "                                                      ling_ann['morph'][arg_ling_sent],\n",
    "                                                      ling_ann['lemma'][arg_ling_sent],\n",
    "                                                      ling_ann['syntax_dep_tree'][arg_ling_sent])\n",
    "\n",
    "                    \n",
    "        feature_sets.append((features, role, ex_id, tokens, arg_ling_word, pred_ling_word))\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def process_example(feature_extractor, ling_cache, ex_id, sentences):\n",
    "    pred = None\n",
    "    args = list()\n",
    "    for sent_num, sent in enumerate(sentences):\n",
    "        for word_num, word in enumerate(sent):\n",
    "            if 'rank' in word and word['rank'] == 'Предикат':\n",
    "                pred = (sent_num, word_num)\n",
    "            elif 'rolepred1' in word:\n",
    "                args.append((sent_num, word_num))\n",
    "    \n",
    "    return process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, sentences)\n",
    "\n",
    "\n",
    "num_of_errors = 0\n",
    "def prepare_train_data(examples, ling_data_cache, feature_extractor):\n",
    "    feature_sets = []\n",
    "    for ex_num, (ex_id, ex) in tqdm(list(enumerate(examples))):                \n",
    "        feature_sets += process_example(feature_extractor, ling_data_cache, ex_id, ex)\n",
    "\n",
    "    print('Number of training examples:', len(feature_sets))\n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model_path_root = '../../data/models_new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!: Choose feature model here\n",
    "from isanlp_srl_framebank.processor_srl_framebank import FeatureModelDefault\n",
    "feature_model = FeatureModelDefault()\n",
    "main_model_path = os.path.join(main_model_path_root, 'known_preds')\n",
    "\n",
    "# from isanlp_srl_framebank.processor_srl_framebank import FeatureModelUnknownPredicates\n",
    "# feature_model = FeatureModelUnknownPredicates()\n",
    "# main_model_path = os.path.join(main_model_path_root, 'unknown_preds')\n",
    "\n",
    "with open(os.path.join(main_model_path, 'feature_model.pckl'), 'wb') as f:\n",
    "    pickle.dump(feature_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'begin': 0, 'end': 17}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vars(ling_data_cache['30587']['sentences'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dba41c87e924c5283f51556933dd14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training examples: 57552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animacy_arg</th>\n",
       "      <th>Aspect_arg</th>\n",
       "      <th>Gender_arg</th>\n",
       "      <th>Number_arg</th>\n",
       "      <th>Tense_arg</th>\n",
       "      <th>Valency_arg</th>\n",
       "      <th>VerbForm_arg</th>\n",
       "      <th>arg_address</th>\n",
       "      <th>arg_case</th>\n",
       "      <th>arg_lemma</th>\n",
       "      <th>...</th>\n",
       "      <th>dist</th>\n",
       "      <th>ex_id</th>\n",
       "      <th>prd_address</th>\n",
       "      <th>pred_lemma</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>prepos</th>\n",
       "      <th>rel_pos</th>\n",
       "      <th>role</th>\n",
       "      <th>syn_link_name</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13454</th>\n",
       "      <td>Inan</td>\n",
       "      <td></td>\n",
       "      <td>Neut</td>\n",
       "      <td>Sing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>30</td>\n",
       "      <td>Acc</td>\n",
       "      <td>лицо_NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>137553</td>\n",
       "      <td>27</td>\n",
       "      <td>продемонстрировать_VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>-1.0</td>\n",
       "      <td>атрибут</td>\n",
       "      <td>obj</td>\n",
       "      <td>[На, той, встрече, Адель, Нассиф, ,, поняв, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3891</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td></td>\n",
       "      <td>как_CONJ</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>31005</td>\n",
       "      <td>11</td>\n",
       "      <td>велено_ADV</td>\n",
       "      <td>ADV</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>содержание высказывания</td>\n",
       "      <td>case</td>\n",
       "      <td>[[, [, Сопровождающий, ,, что, с, усердием, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Masc</td>\n",
       "      <td>Sing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>Nom</td>\n",
       "      <td>он_PRON</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40649</td>\n",
       "      <td>0</td>\n",
       "      <td>беседовать_VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>-1.0</td>\n",
       "      <td>говорящий</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>[Беседовать, он, мог, только, на, отвлеченные,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Animacy_arg Aspect_arg Gender_arg Number_arg Tense_arg Valency_arg  \\\n",
       "13454        Inan                  Neut       Sing                         \n",
       "3891                                                                       \n",
       "2879                               Masc       Sing                         \n",
       "\n",
       "      VerbForm_arg  arg_address arg_case  arg_lemma  \\\n",
       "13454                        30      Acc  лицо_NOUN   \n",
       "3891                          8            как_CONJ   \n",
       "2879                          1      Nom    он_PRON   \n",
       "\n",
       "                             ...                         dist   ex_id  \\\n",
       "13454                        ...                          3.0  137553   \n",
       "3891                         ...                          3.0   31005   \n",
       "2879                         ...                          1.0   40649   \n",
       "\n",
       "      prd_address               pred_lemma pred_pos prepos rel_pos  \\\n",
       "13454          27  продемонстрировать_VERB     VERB           -1.0   \n",
       "3891           11               велено_ADV      ADV            1.0   \n",
       "2879            0          беседовать_VERB     VERB           -1.0   \n",
       "\n",
       "                          role syn_link_name  \\\n",
       "13454                  атрибут           obj   \n",
       "3891   содержание высказывания          case   \n",
       "2879                 говорящий         nsubj   \n",
       "\n",
       "                                                  tokens  \n",
       "13454  [На, той, встрече, Адель, Нассиф, ,, поняв, ,,...  \n",
       "3891   [[, [, Сопровождающий, ,, что, с, усердием, ,,...  \n",
       "2879   [Беседовать, он, мог, только, на, отвлеченные,...  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets = prepare_train_data(examples, ling_data_cache, feature_model)\n",
    "\n",
    "data_for_pandas = []\n",
    "for example in feature_sets:\n",
    "    data_for_pandas_ex = {}\n",
    "    data_for_pandas_ex['role'] = example[1]\n",
    "    data_for_pandas_ex['ex_id'] = example[2]\n",
    "    data_for_pandas_ex['tokens'] = example[3]\n",
    "    data_for_pandas_ex['arg_address'] = example[4]\n",
    "    data_for_pandas_ex['prd_address'] = example[5]\n",
    "    for elem in example[0]:\n",
    "        for subelem in elem:\n",
    "            if subelem is not None:\n",
    "                data_for_pandas_ex.update(subelem)\n",
    "    \n",
    "    data_for_pandas.append(data_for_pandas_ex)\n",
    "    \n",
    "pd_data = pd.DataFrame(data_for_pandas)\n",
    "del data_for_pandas\n",
    "pd_data = pd_data.sample(frac=1)\n",
    "pd_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Argument position: 9\n",
      "Argument lemma: мало_NUM\n",
      "--\n",
      "Predicat position: 7\n",
      "Predicat lemma: становиться_VERB\n",
      "--\n",
      "Distance 2\n",
      "--\n",
      "Sentence tokens: [(0, 'Сначала'), (1, 'он'), (2, 'оставил'), (3, 'пятерых'), (4, ','), (5, 'потом'), (6, 'их'), (7, 'становилось'), (8, 'всё'), (9, 'меньше'), (10, '.')]\n",
      "------------------------------------------------------------\n",
      "Argument position: 4\n",
      "Argument lemma: ты_PRON\n",
      "--\n",
      "Predicat position: 6\n",
      "Predicat lemma: петь_VERB\n",
      "--\n",
      "Distance 2\n",
      "--\n",
      "Sentence tokens: [(0, '--'), (1, 'Ну'), (2, 'не'), (3, 'надо'), (4, 'тебе'), (5, 'сейчас'), (6, 'петь'), (7, '.')]\n"
     ]
    }
   ],
   "source": [
    "N_verify = 2\n",
    "for i in np.random.choice(len(pd_data), size=N_verify):\n",
    "    print(\"-\"*60)\n",
    "    obj = pd_data.iloc[i]\n",
    "    print(f\"Argument position: {obj.arg_address}\")\n",
    "    print(f\"Argument lemma: {obj.arg_lemma}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Predicat position: {obj.prd_address}\")\n",
    "    print(f\"Predicat lemma: {obj.pred_lemma}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Distance {int(obj.dist)}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Sentence tokens: {list(enumerate(obj.tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ling_data_cache\n",
    "del examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Убрать все фичи предикатного слова для модели Unknown Predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.046180",
     "start_time": "2017-02-18T12:16:27.932519"
    }
   },
   "outputs": [],
   "source": [
    "y_stat = pd_data.loc[:, 'role'].value_counts()\n",
    "drop_ys = y_stat[y_stat < 180].index\n",
    "clear_data = pd_data.drop(pd_data[pd_data.loc[:, 'role'].isin(drop_ys)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.523645",
     "start_time": "2017-02-18T12:16:28.047978"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of roles:  44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "агенс                                 6147\n",
       "пациенс                               5362\n",
       "тема                                  3656\n",
       "субъект психологического состояния    3250\n",
       "субъект перемещения                   3011\n",
       "причина                               2502\n",
       "говорящий                             2365\n",
       "место                                 2185\n",
       "содержание действия                   1874\n",
       "содержание мысли                      1817\n",
       "содержание высказывания               1792\n",
       "конечная точка                        1772\n",
       "результат                             1452\n",
       "пациенс перемещения                   1356\n",
       "стимул                                1271\n",
       "субъект ментального состояния         1223\n",
       "адресат                                941\n",
       "субъект восприятия                     901\n",
       "контрагент                             831\n",
       "эффектор                               739\n",
       "субъект социального отношения          598\n",
       "начальная точка                        588\n",
       "предмет высказывания                   548\n",
       "способ                                 531\n",
       "конечный посессор                      506\n",
       "цель                                   454\n",
       "сфера                                  376\n",
       "признак                                366\n",
       "источник звука                         359\n",
       "субъект поведения                      339\n",
       "ситуация в фокусе                      322\n",
       "контрагент социального отношения       318\n",
       "субъект физиологической реакции        310\n",
       "предмет мысли                          303\n",
       "потенциальный пациенс                  290\n",
       "статус                                 265\n",
       "пациенс социального отношения          261\n",
       "срок                                   255\n",
       "эталон                                 255\n",
       "признак действия                       243\n",
       "каузатор                               223\n",
       "исходный посессор                      217\n",
       "потенциальная угроза                   197\n",
       "траектория                             180\n",
       "Name: role, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repl_roles = {\n",
    "    'агенс - субъект восприятия' : 'субъект восприятия',\n",
    "    'агенс - субъект ментального состояния' : 'субъект ментального состояния',\n",
    "    'результат / цель' : 'результат',\n",
    "    'место - пациенс' : 'место',\n",
    "    'говорящий - субъект психологического состояния' : 'субъект психологического состояния'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_single_region(data, rep, val):\n",
    "    data.loc[:, 'role'] = data.loc[:, 'role'].str.replace(rep, val)\n",
    "\n",
    "\n",
    "for rep, val in repl_roles.items():\n",
    "    normalize_single_region(clear_data, rep, val)\n",
    "    \n",
    "number_of_roles = len(clear_data.loc[:, 'role'].value_counts().index)\n",
    "print('Number of roles: ', number_of_roles)\n",
    "clear_data.loc[:, 'role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['содержание высказывания', 'говорящий',\n",
       "       'субъект социального отношения',\n",
       "       'субъект психологического состояния', 'содержание действия',\n",
       "       'агенс', 'тема', 'конечная точка', 'сфера', 'контрагент',\n",
       "       'субъект перемещения', 'причина', 'субъект поведения',\n",
       "       'ситуация в фокусе', 'исходный посессор',\n",
       "       'субъект физиологической реакции', 'адресат', 'пациенс', 'срок',\n",
       "       'источник звука', 'место', 'признак', 'потенциальная угроза',\n",
       "       'субъект ментального состояния', 'конечный посессор', 'результат',\n",
       "       'стимул', 'субъект восприятия', 'эффектор', 'траектория',\n",
       "       'содержание мысли', 'пациенс перемещения', 'каузатор',\n",
       "       'предмет высказывания', 'начальная точка', 'способ',\n",
       "       'пациенс социального отношения', 'статус', 'предмет мысли', 'цель',\n",
       "       'потенциальный пациенс', 'контрагент социального отношения',\n",
       "       'эталон', 'признак действия'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_data.role.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.578144",
     "start_time": "2017-02-18T12:16:28.525204"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52751, 20)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_orig = clear_data.loc[:, 'role']\n",
    "X_orig = clear_data.drop('role', axis = 1)\n",
    "X_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:29.690583",
     "start_time": "2017-02-18T12:16:28.580765"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "y = label_encoder.fit_transform(y_orig)\n",
    "\n",
    "with open(main_model_path + '/label_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52751, 44)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:36.643690",
     "start_time": "2017-02-18T12:16:29.701186"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size:  300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings_path = '../../data/ruscorpora_upos_skipgram_300_5_2018.vec'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embeddings_path, binary=False)\n",
    "print('Embedding size: ', embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:36.671658",
     "start_time": "2017-02-18T12:16:36.645470"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def make_embeded_form(word):\n",
    "    if word:\n",
    "        #return word[1].encode('utf8')\n",
    "        return u\"{}_{}\".format(word[1], word[0])\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "class Embedder_map:\n",
    "    def __init__(self, embeddings, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_ = embeddings\n",
    "\n",
    "    def __call__(self, i):  \n",
    "        result = np.zeros(embeddings.vector_size)\n",
    "        \n",
    "        ARG_SPECIAL_TAG = None  # ??\n",
    "\n",
    "        word = self.X_[i]\n",
    "        if embeddings.vocab.get(word):\n",
    "            return embeddings[word]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def embed(X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_map(embeddings, X), list(range(len(X))), 1000)\n",
    "    pool.close()\n",
    "#     embedder = Embedder_map(embeddings, X)\n",
    "#     result =[embedder(i) for i in range(2)]\n",
    "#     #result = [embedder(i) for i in range(len(X))]\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg',\n",
       "       'Valency_arg', 'VerbForm_arg', 'arg_address', 'arg_case', 'arg_lemma',\n",
       "       'arg_pos', 'dist', 'ex_id', 'prd_address', 'pred_lemma', 'pred_pos',\n",
       "       'prepos', 'rel_pos', 'syn_link_name', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:18:10.418972",
     "start_time": "2017-02-18T12:16:36.673158"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.5 s, sys: 3.4 s, total: 21.9 s\n",
      "Wall time: 24.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "arg_embedded = embed(X_orig['arg_lemma'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52751"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arg_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:33.028839",
     "start_time": "2017-02-18T12:18:10.420741"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 3.74 s, total: 22 s\n",
      "Wall time: 24.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_embedded = embed(X_orig['pred_lemma'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/w2v_verbs.npy\", pred_embedded)\n",
    "np.save(\"../../data/w2v_args.npy\", arg_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_embedded = np.load(\"../../data/w2v_verbs.npy\")\n",
    "arg_embedded = np.load(\"../../data/w2v_args.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:33.045522",
     "start_time": "2017-02-18T12:22:33.030953"
    }
   },
   "outputs": [],
   "source": [
    "class Embedder_single_map:\n",
    "    def __init__(self, embeddings, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_ = embeddings\n",
    "\n",
    "    def __call__(self, i):\n",
    "        #word = make_embeded_form(self.X_[i])\n",
    "        word = self.X_[i]\n",
    "        if word in self.embeddings_:\n",
    "            return self.embeddings_[word]\n",
    "        else:\n",
    "            return np.zeros((self.embeddings_.vector_size,))\n",
    "\n",
    "        \n",
    "def embed_single(embeddings, X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_single_map(embeddings, X), X.index, 1000)\n",
    "    pool.close()\n",
    "        \n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ELMoEmbedder(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-wiki_600k_steps.tar.gz\", elmo_output_names=['elmo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<deeppavlov.models.embedders.elmo_embedder.ELMoEmbedder at 0x7ff33a8c9710>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.3390257 , -0.3563082 , -0.5748834 , ...,  0.76325905,\n",
       "          0.94826674,  0.8298109 ],\n",
       "        [-1.1582128 , -0.9295433 ,  0.13361324, ...,  0.35102528,\n",
       "          0.27033824,  0.06720511],\n",
       "        [-0.04592073, -0.46310368,  0.0504637 , ...,  0.29876482,\n",
       "         -0.652665  ,  0.79909015],\n",
       "        [-0.29748228, -0.4363746 ,  0.33843917, ..., -0.637612  ,\n",
       "         -0.842638  ,  0.07921451],\n",
       "        [-0.6751006 , -0.24317859, -0.90687317, ..., -0.24283595,\n",
       "          0.21287943, -0.35696185]], dtype=float32)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo(['Я не хочу домой .'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.33903   , -0.3545407 , -0.57610404, ...,  0.9724996 ,\n",
       "          0.27039424,  0.83574843]], dtype=float32)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo([['Я']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.71904266,  0.22552139,  0.2757006 , ..., -0.31051713,\n",
       "         -0.5760167 ,  0.50546265]], dtype=float32)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo([['собака']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.337023  , -0.35688448, -0.5789683 , ...,  0.82202846,\n",
       "         1.2211776 ,  0.8066084 ],\n",
       "       [-1.1719527 , -0.9044735 ,  0.13157941, ...,  0.7761724 ,\n",
       "         0.57176316, -0.33491123],\n",
       "       [-0.06295706, -0.45263326,  0.05356532, ...,  1.321588  ,\n",
       "        -0.15975067, -0.07389995],\n",
       "       ...,\n",
       "       [ 0.10455199, -0.43030205, -0.09874936, ...,  0.29800093,\n",
       "        -0.6528196 ,  0.7991791 ],\n",
       "       [-0.22349963, -0.49061188,  0.21470943, ..., -0.63820523,\n",
       "        -0.84188545,  0.07996868],\n",
       "       [-0.6350095 , -0.4391315 , -1.1107223 , ..., -0.24298908,\n",
       "         0.21316466, -0.35761094]], dtype=float32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo(['Я не хочу , я не хочу , я не хочу домой .'.split()])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.3398    , -0.35434005, -0.57634723, ...,  0.82231534,\n",
       "          1.2208288 ,  0.80663276],\n",
       "        [-1.1603794 , -0.93078136,  0.13379881, ...,  0.7763979 ,\n",
       "          0.57154197, -0.3350079 ],\n",
       "        [-0.04483984, -0.462993  ,  0.05080022, ...,  1.3217599 ,\n",
       "         -0.15995294, -0.07382467],\n",
       "        ...,\n",
       "        [ 0.1079851 , -0.43433216, -0.10255129, ...,  0.2989173 ,\n",
       "         -0.65271735,  0.7993736 ],\n",
       "        [-0.21839234, -0.49216184,  0.21128058, ..., -0.6376457 ,\n",
       "         -0.8423133 ,  0.07939322],\n",
       "        [-0.62942576, -0.4276948 , -1.1144581 , ..., -0.24306296,\n",
       "          0.21245189, -0.35704273]], dtype=float32)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elmo(['Я не хочу , я не хочу , я не хочу домой .'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = 'что_PRON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'что'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value[:value.rfind('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_embed(embeddings, tokens, word_idx1, word_idx2):\n",
    "    embedded = embeddings([tokens])[0]\n",
    "    return embedded[min(word_idx1, len(tokens)-1)], embedded[min(word_idx2, len(tokens)-1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = X_orig.iloc[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verb_idx = obj.prd_address\n",
    "arg_idx = obj.arg_address\n",
    "tokens = obj.tokens\n",
    "embed_verb, embed_arg = elmo_embed(elmo, tokens, verb_idx, arg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e936fabf1245eda03b0f4af153b615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2h 40min 22s, sys: 16min 19s, total: 2h 56min 41s\n",
      "Wall time: 1h 21min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedded_verbs = []\n",
    "embedded_args  = []\n",
    "for i in tqdm(range(len(X_orig))):\n",
    "    try:\n",
    "        if i % 100 == 0:\n",
    "            with open(\"./log.txt\", 'a', encoding='utf-8') as log:\n",
    "                print(f\"Processed {i} examples\", file=log)\n",
    "        obj = X_orig.iloc[i]\n",
    "        verb_idx = obj.prd_address\n",
    "        arg_idx = obj.arg_address\n",
    "        tokens = obj.tokens\n",
    "        embed_verb, embed_arg = elmo_embed(elmo, tokens, verb_idx, arg_idx)\n",
    "        embedded_verbs.append(embed_verb)\n",
    "        embedded_args.append(embed_arg)\n",
    "    except Exception as e:\n",
    "        with open(\"./log.txt\", 'a', encoding='utf-8') as log:\n",
    "            print(f\"Error while processing example {i}={X_orig.iloc[i]}: {e}\", file=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52751, 1024) (52751, 1024)\n"
     ]
    }
   ],
   "source": [
    "e_verbs = np.stack(embedded_verbs)\n",
    "e_args  = np.stack(embedded_args)\n",
    "\n",
    "print(e_verbs.shape, e_args.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/elmo_verbs.npy\", e_verbs)\n",
    "np.save(\"../../data/elmo_args.npy\", e_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9361637 ,  0.46197   , -0.8599031 , ..., -0.46175578,\n",
       "        0.61335385, -0.33758122], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_verbs[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_verbs = np.load(\"../../data/elmo_verbs.npy\")\n",
    "e_args = np.load(\"../../data/elmo_args.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = X_orig.drop(['tokens'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.653831",
     "start_time": "2017-02-18T12:22:48.759036"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category features:\n",
      " ['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg', 'Valency_arg', 'VerbForm_arg', 'arg_case', 'arg_pos', 'dist', 'prd_address', 'pred_lemma', 'pred_pos', 'prepos', 'syn_link_name']\n",
      "Not category features:\n",
      " ['rel_pos']\n",
      "(52751, 817)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#morph_feats = ['pos', 'case', 'anim', 'vform', 'zform', 'shform', 'pform', 'vvform', 'nform', 'time']\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep', 'link_name'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "#all_feats = ['pred_lemma', 'rel_pos', 'pred_pos', 'arg_case', 'syn_link_name', 'arg_pos', 'prepos', 'dist']\n",
    "\n",
    "#categ_feats = [e for e in all_feats if X_orig[e].dtype in [str, object]]\n",
    "#not_categ = [e for e in all_feats if e not in categ_feats]\n",
    "\n",
    "#pred_lemma_vectorizer.fit_transform(X_orig.loc[:, ['pred_lemma']].to_dict(orient = 'records'))\n",
    "\n",
    "not_categ_features = {'arg_address', 'ex_id', 'rel_pos', 'arg_lemma'}\n",
    "categ_feats = [name for name in X_orig.columns if name not in not_categ_features] \n",
    "not_categ = ['rel_pos']\n",
    "print('Category features:\\n', categ_feats)\n",
    "print('Not category features:\\n', not_categ)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# X_train[categorical_cols] = X_train[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "# one_hot_feats = vectorizer.fit_transform(X_orig[categ_feats].to_dict(orient='records'))\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "one_hot_feats = vectorizer.fit_transform(X_orig[categ_feats].to_dict(orient='records'))\n",
    "print(one_hot_feats.shape)\n",
    "\n",
    "with open(main_model_path + '/feature_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/labnpnpels.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.924596",
     "start_time": "2017-02-18T12:22:52.655580"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52751, 818)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_categ_columns = np.concatenate(tuple(X_orig.loc[:, e].as_matrix().reshape(-1, 1) for e in not_categ), axis =1)\n",
    "plain_features = np.concatenate((one_hot_feats, not_categ_columns), axis = 1)\n",
    "plain_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/plain_features.npy\", plain_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.938339",
     "start_time": "2017-02-18T12:22:52.926218"
    }
   },
   "outputs": [],
   "source": [
    "del not_categ_columns\n",
    "del one_hot_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Convolution1D, Dropout, MaxPooling1D\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Permute\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Bidirectional\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:23:00.054095",
     "start_time": "2017-02-18T12:23:00.016955"
    }
   },
   "outputs": [],
   "source": [
    "def construct_plain_model(input_shape):\n",
    "    print('Plain model.')\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(600, \n",
    "                          #input_shape=(plain_features.shape[1],), \n",
    "                          input_shape = input_shape,\n",
    "                          activation = 'relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(400))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(number_of_roles))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('softmax'))\n",
    "    \n",
    "    plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return plain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:23:00.102016",
     "start_time": "2017-02-18T12:23:00.055842"
    }
   },
   "outputs": [],
   "source": [
    "def construct_plain_model_sparse(categ_size, emb_size, number_of_roles):    \n",
    "    input_plain = Input(shape=(categ_size,), name = 'input_categorical')\n",
    "    input_pred_embed = Input(shape=(emb_size,), name = 'pred_embed')\n",
    "    input_arg_embed = Input(shape=(emb_size,), name = 'arg_embed')\n",
    "    \n",
    "    plain = Dense(400)(input_plain)\n",
    "    plain = BatchNormalization()(plain)\n",
    "    plain = Activation('relu')(plain)\n",
    "    \n",
    "    def embed_submodel(inpt):\n",
    "        embed = Dense(100)(inpt)\n",
    "        embed = BatchNormalization()(embed)\n",
    "        embed = Activation('relu')(embed)\n",
    "        return embed\n",
    "    \n",
    "    embed_pred = embed_submodel(input_pred_embed)\n",
    "    embed_arg = embed_submodel(input_arg_embed)\n",
    "    \n",
    "    final = Concatenate(axis = 1)([embed_pred, embed_arg, plain])\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(400)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('relu')(final)\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(number_of_roles)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('softmax')(final)\n",
    "    \n",
    "    model = Model([input_arg_embed, input_pred_embed, input_plain], final)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with in-domain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for experiments with in-domain test. Do not use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T10:45:48.365172",
     "start_time": "2017-02-15T10:45:04.358434"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 600)               491400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 44)                17644     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 44)                176       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 44)                0         \n",
      "=================================================================\n",
      "Total params: 751,220\n",
      "Trainable params: 750,332\n",
      "Non-trainable params: 888\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 47475 samples, validate on 5276 samples\n",
      "Epoch 1/15\n",
      "47475/47475 [==============================] - 1s 29us/step - loss: 3.0228 - acc: 0.2661 - val_loss: 2.8990 - val_acc: 0.4917\n",
      "Epoch 2/15\n",
      "47475/47475 [==============================] - 1s 13us/step - loss: 1.5339 - acc: 0.6754 - val_loss: 2.2493 - val_acc: 0.6986\n",
      "Epoch 3/15\n",
      "47475/47475 [==============================] - 1s 20us/step - loss: 1.1492 - acc: 0.7376 - val_loss: 1.3880 - val_acc: 0.7390\n",
      "Epoch 4/15\n",
      "47475/47475 [==============================] - 2s 42us/step - loss: 0.9955 - acc: 0.7569 - val_loss: 1.0099 - val_acc: 0.7455\n",
      "Epoch 5/15\n",
      "47475/47475 [==============================] - 1s 24us/step - loss: 0.9006 - acc: 0.7698 - val_loss: 0.9131 - val_acc: 0.7559\n",
      "Epoch 6/15\n",
      "47475/47475 [==============================] - 1s 26us/step - loss: 0.8321 - acc: 0.7804 - val_loss: 0.8514 - val_acc: 0.7625\n",
      "Epoch 7/15\n",
      "47475/47475 [==============================] - 1s 16us/step - loss: 0.7771 - acc: 0.7891 - val_loss: 0.8507 - val_acc: 0.7631\n",
      "Epoch 8/15\n",
      "47475/47475 [==============================] - 1s 19us/step - loss: 0.7362 - acc: 0.7952 - val_loss: 0.8114 - val_acc: 0.7674\n",
      "Epoch 9/15\n",
      "47475/47475 [==============================] - 1s 23us/step - loss: 0.6967 - acc: 0.8028 - val_loss: 0.7916 - val_acc: 0.7712\n",
      "Epoch 10/15\n",
      "47475/47475 [==============================] - 2s 32us/step - loss: 0.6646 - acc: 0.8087 - val_loss: 0.7924 - val_acc: 0.7688\n",
      "Epoch 11/15\n",
      "47475/47475 [==============================] - 1s 23us/step - loss: 0.6413 - acc: 0.8142 - val_loss: 0.7926 - val_acc: 0.7701\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff1a2d09908>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = construct_plain_model((plain_features.shape[1],))\n",
    "print(model.summary())\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit(plain_features, y, epochs=15, batch_size=300, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = load_model(os.path.join(main_model_path, 'neural_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pred_embed (InputLayer)         (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arg_embed (InputLayer)          (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_categorical (InputLayer)  (None, 818)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          102500      pred_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 100)          102500      arg_embed[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 400)          327600      input_categorical[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 100)          400         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 100)          400         dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 400)          1600        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 100)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 100)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 400)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 600)          0           activation_5[0][0]               \n",
      "                                                                 activation_6[0][0]               \n",
      "                                                                 activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 600)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 400)          240400      dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 400)          1600        dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 400)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 400)          0           activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 44)           17644       dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 44)           176         dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 44)           0           batch_normalization_8[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 794,820\n",
      "Trainable params: 792,732\n",
      "Non-trainable params: 2,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T10:49:20.090543",
     "start_time": "2017-02-16T10:49:19.585615"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47475 samples, validate on 5276 samples\n",
      "Epoch 1/16\n",
      "47475/47475 [==============================] - 2s 49us/step - loss: 2.4000 - acc: 0.4630 - val_loss: 1.7841 - val_acc: 0.6439\n",
      "Epoch 2/16\n",
      "47475/47475 [==============================] - 1s 21us/step - loss: 1.3658 - acc: 0.7244 - val_loss: 1.3433 - val_acc: 0.7437\n",
      "Epoch 3/16\n",
      "47475/47475 [==============================] - 1s 27us/step - loss: 1.0149 - acc: 0.7835 - val_loss: 1.0009 - val_acc: 0.7883\n",
      "Epoch 4/16\n",
      "47475/47475 [==============================] - 1s 27us/step - loss: 0.8338 - acc: 0.8126 - val_loss: 0.8103 - val_acc: 0.8069\n",
      "Epoch 5/16\n",
      "47475/47475 [==============================] - 1s 23us/step - loss: 0.7097 - acc: 0.8348 - val_loss: 0.7319 - val_acc: 0.8146\n",
      "Epoch 6/16\n",
      "47475/47475 [==============================] - 1s 22us/step - loss: 0.6244 - acc: 0.8506 - val_loss: 0.6865 - val_acc: 0.8258\n",
      "Epoch 7/16\n",
      "47475/47475 [==============================] - 1s 30us/step - loss: 0.5628 - acc: 0.8605 - val_loss: 0.6413 - val_acc: 0.8290\n",
      "Epoch 8/16\n",
      "47475/47475 [==============================] - 1s 20us/step - loss: 0.5051 - acc: 0.8718 - val_loss: 0.6252 - val_acc: 0.8364\n",
      "Epoch 9/16\n",
      "47475/47475 [==============================] - 1s 23us/step - loss: 0.4587 - acc: 0.8850 - val_loss: 0.6136 - val_acc: 0.8258\n",
      "Epoch 10/16\n",
      "47475/47475 [==============================] - 1s 29us/step - loss: 0.4160 - acc: 0.8920 - val_loss: 0.6100 - val_acc: 0.8342\n",
      "Epoch 11/16\n",
      "47475/47475 [==============================] - 1s 23us/step - loss: 0.3818 - acc: 0.9006 - val_loss: 0.5943 - val_acc: 0.8374\n",
      "Epoch 12/16\n",
      "47475/47475 [==============================] - 2s 40us/step - loss: 0.3549 - acc: 0.9066 - val_loss: 0.6066 - val_acc: 0.8336\n",
      "Epoch 13/16\n",
      "47475/47475 [==============================] - 1s 19us/step - loss: 0.3234 - acc: 0.9150 - val_loss: 0.5786 - val_acc: 0.8374\n",
      "Epoch 14/16\n",
      "47475/47475 [==============================] - 1s 23us/step - loss: 0.3028 - acc: 0.9198 - val_loss: 0.5684 - val_acc: 0.8419\n",
      "Epoch 15/16\n",
      "47475/47475 [==============================] - 1s 31us/step - loss: 0.2793 - acc: 0.9251 - val_loss: 0.5783 - val_acc: 0.8364\n",
      "Epoch 16/16\n",
      "47475/47475 [==============================] - 2s 37us/step - loss: 0.2571 - acc: 0.9326 - val_loss: 0.6005 - val_acc: 0.8376\n"
     ]
    }
   ],
   "source": [
    "model = construct_plain_model_sparse(plain_features.shape[1], 1024, y.shape[1])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([e_args, e_verbs, plain_features], y, epochs=16, batch_size=300, \n",
    "          validation_split = 0.1, shuffle=True, callbacks = [early_stopping])\n",
    "model.save(os.path.join(main_model_path, 'neural_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context (NA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T23:18:09.873456",
     "start_time": "2017-02-15T23:07:16.496170"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = construct_graph_lstm_model((plain_features.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([arg_context_embedded, pred_context_embedded, embedded_args, embedded_verbs, plain_features], y, \n",
    "          epochs=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiements with out-of-domain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for out of domain experiments. Do not use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T14:21:13.954628",
     "start_time": "2017-02-17T14:21:13.936038"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_out_of_domain(model, X_train, y_train, X_test, y_test):\n",
    "    final_res = list()\n",
    "    N_ITERATIONS = 5\n",
    "    for i in xrange(N_ITERATIONS):\n",
    "        print('Eval iter:', i + 1, '/', N_ITERATIONS)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                       patience=2, verbose=0, mode='auto')\n",
    "        model.fit(X_train, y_train, nb_epoch=15, \n",
    "                  batch_size=64, validation_split = 0.1, \n",
    "                  shuffle=True, callbacks = [early_stopping],\n",
    "                 verbose = 0)\n",
    "\n",
    "        ev_res = evaluate_model(model, X_test, y_test)\n",
    "        print()\n",
    "        print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))\n",
    "        final_res.append(ev_res)\n",
    "    \n",
    "    return np.array(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:26:02.646008",
     "start_time": "2017-02-18T12:26:02.630666"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    keras_eval = model.evaluate(X_test, y_test)\n",
    "    pred = model.predict(X_test).argmax(axis = 1)\n",
    "    f1_micro = f1_score(pred, y_test.argmax(axis = 1), average = 'micro')\n",
    "    f1_macro = f1_score(pred, y_test.argmax(axis = 1), average = 'macro')\n",
    "    accur = accuracy_score(pred, y_test.argmax(axis = 1))\n",
    "    \n",
    "    return np.array(list(keras_eval) + [f1_micro, f1_macro, accur])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T10:15:31.636957",
     "start_time": "2017-02-17T10:13:07.614626"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model((ind_plain_features.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit(ind_plain_features, ind_y, nb_epoch=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "\n",
    "#model.evaluate(ood_plain_features, ood_y)\n",
    "ev_res = evaluate_model(model, [ood_plain_features], ood_y)\n",
    "print()\n",
    "print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:32:19.012435",
     "start_time": "2017-02-17T11:26:21.034200"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model((ind_plain_features.shape[1],))\n",
    "model_eval = evaluate_out_of_domain(model, ind_plain_features, ind_y, ood_plain_features, ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:51:12.792397",
     "start_time": "2017-02-17T11:50:35.226172"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, nb_epoch=20, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "#model.evaluate([ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "\n",
    "ev_res = evaluate_model(model, [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "print()\n",
    "print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T16:02:37.654028",
     "start_time": "2017-02-17T15:54:39.172343"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, \n",
    "                                    [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T13:21:40.582402",
     "start_time": "2017-02-17T13:21:21.643939"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_graph_lstm_model((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([\n",
    "           #ind_arg_context, \n",
    "        #ind_pred_context,   \n",
    "        ind_arg_embed, \n",
    "        ind_pred_embed, \n",
    "        ind_plain_features,\n",
    "        ind_pred_context], \n",
    "#model.fit([ind_arg_context, ind_pred_context, ind_arg_embed, ind_pred_embed, ind_plain_features], \n",
    "           ind_y, nb_epoch=6, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "\n",
    "#model.evaluate([ood_arg_context, ood_pred_context, ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "model.evaluate([\n",
    "    #    ood_arg_context, \n",
    "    #    ood_pred_context,\n",
    "        ood_arg_embed, \n",
    "        ood_pred_embed,\n",
    "        ood_plain_features,\n",
    "        ood_pred_context\n",
    "    ], ood_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T15:41:36.049234",
     "start_time": "2017-02-17T15:25:34.248413"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_graph_lstm_model((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_pred_embed, ind_plain_features, ind_pred_context], ind_y, \n",
    "                                    [ood_arg_embed, ood_pred_embed, ood_plain_features, ood_pred_context], ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for model comparision. Do not use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:43:55.066960",
     "start_time": "2017-02-17T11:43:54.950449"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, *args, **kwargs):\n",
    "    model.fit(X_train, y_train, *args, **kwargs)\n",
    "    \n",
    "    keras_eval = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    pred = model.predict(X_test).argmax(axis = 1)\n",
    "    f1_micro = f1_score(pred, y_test.argmax(axis = 1), average = 'micro')\n",
    "    f1_macro = f1_score(pred, y_test.argmax(axis = 1), average = 'macro')\n",
    "    accur = accuracy_score(pred, y_test.argmax(axis = 1))\n",
    "    \n",
    "    return list(keras_eval) + [f1_micro, f1_macro, accur]\n",
    "    \n",
    "\n",
    "def custom_cross_val(cr_f, X, y, cv, *args, **kwargs):\n",
    "    cr_f().summary()\n",
    "    eval_res = list()\n",
    "    for i, (train, test) in enumerate(cv.split(y)):\n",
    "        model = cr_f()\n",
    "        print('Running Fold', i+1, '/', cv.n_splits)\n",
    "        eval1 = train_and_evaluate_model(model, \n",
    "                                         [X[j][train] for j in range(len(X))], y[train], \n",
    "                                         [X[j][test] for j in range(len(X))], y[test], \n",
    "                                         *args, **kwargs)\n",
    "        \n",
    "        print()\n",
    "        print('Fold result: ', eval1)\n",
    "        eval_res.append(eval1)\n",
    "    \n",
    "    return np.array(eval_res)\n",
    "\n",
    "\n",
    "def describe_cv_result(cv_res):\n",
    "    print(cv_res)\n",
    "    mean_cv_res = cv_res.mean(axis = 0)\n",
    "    std_cv_res = cv_res.std(axis = 0)\n",
    "    print('Mean')\n",
    "    print(pd.DataFrame([mean_cv_res], columns = ['loss', 'keras_accur', 'micro_f1', 'macro_f1', 'accur']))\n",
    "    print('Std')\n",
    "    print(pd.DataFrame([std_cv_res], columns = ['loss', 'keras_accur', 'micro_f1', 'macro_f1', 'accur']))\n",
    "    \n",
    "    \n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T16:16:18.702196",
     "start_time": "2017-02-16T16:07:42.457676"
    }
   },
   "outputs": [],
   "source": [
    "curr_features = np.concatenate((plain_features, e_verbs, e_args), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : construct_plain_model((curr_features.shape[1],)), \n",
    "                          [curr_features], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T15:36:12.375033",
     "start_time": "2017-02-16T15:27:40.793790"
    }
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_plain_model((plain_features.shape[1],)), \n",
    "                          [plain_features], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T14:48:33.877302",
     "start_time": "2017-02-16T14:45:18.196997"
    }
   },
   "outputs": [],
   "source": [
    "single_chunk = np.concatenate((e_args, e_verbs, plain_features), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : construct_plain_model((single_chunk.shape[1],)), \n",
    "                          [single_chunk], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T16:23:57.011641",
     "start_time": "2017-02-17T16:07:49.454278"
    }
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_plain_model_sparse(plain_features.shape[1], \n",
    "                                                                embeddings.vector_size, \n",
    "                                                                y.shape[1]), \n",
    "                          [embedded_args, embedded_verbs, plain_features], y, \n",
    "                          cv = cv, epochs=13, batch_size=300,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T14:16:02.109004",
     "start_time": "2017-02-16T13:49:33.683794"
    }
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_graph_lstm_model((plain_features.shape[1],)), \n",
    "                          [arg_context_embedded, \n",
    "                           pred_context_embedded, \n",
    "                           embedded_args, \n",
    "                           embedded_verbs,\n",
    "                           plain_features], y, \n",
    "                          cv = cv, epochs=6, batch_size=64, validation_split = 0., \n",
    "                          shuffle=True)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:59:05.065644",
     "start_time": "2017-02-18T12:59:05.006966"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(X_orig.ex_id.unique(), test_size=0.2, random_state=42)\n",
    "train_ids = set(train_ids.tolist())\n",
    "test_ids = set(test_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T13:04:58.313461",
     "start_time": "2017-02-18T13:04:58.267195"
    }
   },
   "outputs": [],
   "source": [
    "train_selector_pd = X_orig.ex_id.isin(train_ids)\n",
    "test_selector_pd = X_orig.ex_id.isin(test_ids)\n",
    "train_selector = train_selector_pd.values\n",
    "test_selector = test_selector_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T11:29:41.203903",
     "start_time": "2017-02-18T11:28:02.698301"
    }
   },
   "outputs": [],
   "source": [
    "train_data = {k : data[k] for k in train_ids}\n",
    "test_data = {k : data[k] for k in test_ids}\n",
    "\n",
    "with open(os.path.join(main_model_path, 'train_data.json'), 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(os.path.join(main_model_path, 'test_data.json'), 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T13:00:50.036868",
     "start_time": "2017-02-18T12:59:10.294248"
    }
   },
   "outputs": [],
   "source": [
    "def select_from_nparray_list(nparray_list, selector):\n",
    "    return [e[selector] for e in nparray_list]\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], embeddings.vector_size, y.shape[1])\n",
    "model.fit(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], train_selector),\n",
    "          select_from_nparray_list([y], train_selector), \n",
    "          epochs=10, batch_size=64, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "model.evaluate(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector), \n",
    "               select_from_nparray_list([y], test_selector))\n",
    "model.save(os.path.join(main_model_path, 'neural_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:55:42.486066",
     "start_time": "2017-02-18T15:55:39.780097"
    }
   },
   "outputs": [],
   "source": [
    "# Hold-out evaluation.\n",
    "\n",
    "evaluate_model(model,\n",
    "               select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector), \n",
    "               select_from_nparray_list([y], test_selector)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:56:50.418354",
     "start_time": "2017-02-18T15:56:40.777599"
    }
   },
   "outputs": [],
   "source": [
    "pred = model.predict(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector))\n",
    "\n",
    "test_examples_to_store = X_orig.loc[test_selector_pd[test_selector_pd].index, :].loc[:, ['arg_address', 'ex_id']]\n",
    "test_data = {k : data[k] for k in test_ids}\n",
    "\n",
    "\n",
    "for index, (pd_index, row) in enumerate(test_examples_to_store.iterrows()):\n",
    "    ex = test_data[row['ex_id']]\n",
    "    arg_addr = row['arg_address']\n",
    "    sent = ex[arg_addr[0]]\n",
    "    token = sent[arg_addr[1]]\n",
    "    cl = pred[index]\n",
    "    predicted_role = label_encoder.inverse_transform(np.array([cl]))[0]\n",
    "    actual_role = label_encoder.inverse_transform(np.array([select_from_nparray_list([y], test_selector)[0][index]]))[0]\n",
    "    \n",
    "    token['rolepred1'] = actual_role\n",
    "    token['rolepred2'] = predicted_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:59:40.060358",
     "start_time": "2017-02-18T15:59:26.421186"
    }
   },
   "outputs": [],
   "source": [
    "with open('./test_data_ann_1.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brat convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts results to brat annotation for inspecting.\n",
    "# Needs framebank_preprocessing from http://nlp.isa.ru/framebank_parser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T17:15:09.301844",
     "start_time": "2017-02-18T17:14:58.429103"
    }
   },
   "outputs": [],
   "source": [
    "!python2.7 ./framebank_preprocessing/convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./brat_ann2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T17:27:49.179234",
     "start_time": "2017-02-18T17:27:30.320938"
    }
   },
   "outputs": [],
   "source": [
    "!python2.7 ./framebank_preprocessing/convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./syntaxnet_1/ --converter=syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=../ && python2.7 ./convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./brat_ann2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "51px",
    "width": "313px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "505px",
    "left": "0px",
    "right": "1122px",
    "top": "110px",
    "width": "158px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
