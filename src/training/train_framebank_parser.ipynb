{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:14:55.909843",
     "start_time": "2017-02-18T12:14:54.905927"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use only one GPU\n",
    "#import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../isanlp/src/')\n",
    "sys.path.append('../../src/isanlp_srl_framebank/')\n",
    "sys.path.append('../../libs/')\n",
    "sys.path.append('../../libs/pylingtools/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress tensorflow memory appetites\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:14:56.607418",
     "start_time": "2017-02-18T12:14:55.911842"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available GPUs\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import isanlp\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(31)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:43:32.199510",
     "start_time": "2017-02-18T12:42:24.396928"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples:  59861\n"
     ]
    }
   ],
   "source": [
    "input_data_path = '../../data/preprocessed_framebank/annotated_corpus.json'\n",
    "\n",
    "with open(input_data_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "print('Number of examples: ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:05.953030",
     "start_time": "2017-02-18T12:16:03.543901"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of verbs:  803\n",
      "Number of left verbs:  572\n"
     ]
    }
   ],
   "source": [
    "from convert_corpus_to_brat import make_text, create_verb_example_index\n",
    "\n",
    "min_n_examples = 10\n",
    "\n",
    "verb_index = create_verb_example_index(data)\n",
    "print('Original number of verbs: ', len(verb_index))\n",
    "\n",
    "stat = sorted([(verb, len(examples)) for verb, examples in verb_index.items()], \n",
    "              key = lambda x: x[1], reverse=True)\n",
    "\n",
    "verbs_to_keep = [verb for verb, count in stat if count >= min_n_examples]\n",
    "print('Number of left verbs: ', len(verbs_to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:06.010150",
     "start_time": "2017-02-18T12:16:05.954506"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of framebank examples left:  32612\n"
     ]
    }
   ],
   "source": [
    "examples = list()\n",
    "\n",
    "for verb in verbs_to_keep:\n",
    "    indexes = verb_index[verb]\n",
    "    \n",
    "    for ind in indexes:\n",
    "        examples.append((ind, data[ind]))\n",
    "\n",
    "print('Number of framebank examples left: ', len(examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_corpus_path = '../../data/cleared_corpus.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cleared_corpus_path, 'w') as f:\n",
    "    json.dump(examples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cleared_corpus_path, 'r') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_data_path = '../../data/results_final_fixed.pckl'\n",
    "with open(ling_data_path, 'rb') as f:\n",
    "    ling_data = pickle.load(f)\n",
    "\n",
    "ling_data_cache = {k: v for k,v in ling_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_examples = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation_repr import CSentence\n",
    "from convert_corpus_to_brat import make_text\n",
    "\n",
    "\n",
    "def find_address_by_offset(offset, ling_ann):\n",
    "    for tok_num, tok in enumerate(ling_ann['tokens']):\n",
    "        if tok.begin <= offset and offset < tok.end:\n",
    "            break\n",
    "    \n",
    "    for sent_num, sent in enumerate(ling_ann['sentences']):\n",
    "        if sent.begin <= tok_num and tok_num < sent.end:\n",
    "            break\n",
    "    \n",
    "    return sent_num, tok_num - sent.begin\n",
    "\n",
    "\n",
    "def process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, example):\n",
    "    feature_sets = list()\n",
    "    \n",
    "    text, offset_index = make_text(example, 0)\n",
    "    ling_ann = ling_cache[ex_id]\n",
    "    \n",
    "    pred_offset = offset_index[(pred[0], pred[1])]\n",
    "    pred_ling_sent, pred_ling_word = find_address_by_offset(pred_offset, ling_ann)\n",
    "    \n",
    "    for arg in args:\n",
    "        arg_offset = offset_index[(arg[0], arg[1])]\n",
    "        arg_ling_sent, arg_ling_word = find_address_by_offset(arg_offset, ling_ann)\n",
    "        \n",
    "        #print(\"-\"*20)\n",
    "        #print('ex_id: ', ex_id)\n",
    "        #print('ling_ann_sent: ', arg_ling_sent)\n",
    "        #print('total number of postags: ', len(ling_ann['postag']))\n",
    "        #print('total number of morph featues: ', len(ling_ann['morph']))\n",
    "        #print('total number of lemmas: ', len(ling_ann['lemma']))\n",
    "        #print('total number of syntax trees: ', len(ling_ann['syntax_dep_tree']))\n",
    "        \n",
    "        lens = {\n",
    "            'len_postags' : len(ling_ann['postag']),\n",
    "            'len_morph' : len(ling_ann['morph']),\n",
    "            'len_lemma' : len(ling_ann['lemma']),\n",
    "            'len_syntax' : len(ling_ann['syntax_dep_tree'])\n",
    "        }\n",
    "        \n",
    "        #print(\"-\"*20)\n",
    "        #print(ex_id)\n",
    "        #print(lens)\n",
    "        #print(\"arg_ling_sent: \", arg_ling_sent)\n",
    "        \n",
    "        if arg_ling_sent > min(lens.values()) or len(set(lens.values())) != 1:\n",
    "            lens['len_arg_ling_sent'] = arg_ling_sent\n",
    "            if ex_id not in error_examples:\n",
    "                error_examples[ex_id] = []\n",
    "            error_examples[ex_id].append((ex_id, lens, \"length mismatch\"))\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        fb_pred_word = example[pred[0]][pred[1]]\n",
    "        fb_arg_word = example[arg[0]][arg[1]]\n",
    "        \n",
    "        role = fb_arg_word['rolepred1']\n",
    "\n",
    "        if arg_ling_sent != pred_ling_sent:\n",
    "            global num_of_errors\n",
    "            num_of_errors += 1\n",
    "            # We miss some examples due to mistakes in framebank or discrepancy in \n",
    "            # automatica annotation of sentences.\n",
    "            print('Error #{}'.format(num_of_errors))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            features = feature_extractor.extract_features(pred_ling_word, \n",
    "                                                      arg_ling_word, \n",
    "                                                      ling_ann['postag'][arg_ling_sent],\n",
    "                                                      ling_ann['morph'][arg_ling_sent],\n",
    "                                                      ling_ann['lemma'][arg_ling_sent],\n",
    "                                                      ling_ann['syntax_dep_tree'][arg_ling_sent])\n",
    "        except Exception as e:\n",
    "            lens['len_arg_ling_sent'] = arg_ling_sent\n",
    "            if ex_id not in error_examples:\n",
    "                error_examples[ex_id] = []\n",
    "            error_examples[ex_id].append((ex_id, lens, str(e)))\n",
    "            continue\n",
    "            \n",
    "                    \n",
    "        feature_sets.append((features, role, ex_id, arg))\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def process_example(feature_extractor, ling_cache, ex_id, sentences):\n",
    "    pred = None\n",
    "    args = list()\n",
    "    for sent_num, sent in enumerate(sentences):\n",
    "        for word_num, word in enumerate(sent):\n",
    "            if 'rank' in word and word['rank'] == 'Предикат':\n",
    "                pred = (sent_num, word_num)\n",
    "            elif 'rolepred1' in word:\n",
    "                args.append((sent_num, word_num))\n",
    "    \n",
    "    return process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, sentences)\n",
    "\n",
    "\n",
    "num_of_errors = 0\n",
    "def prepare_train_data(examples, ling_data_cache, feature_extractor):\n",
    "    feature_sets = []\n",
    "    for ex_num, (ex_id, ex) in enumerate(examples):    \n",
    "        if ex_num % 100 == 0:\n",
    "            print('{0:.2f}%'.format((ex_num / len(examples)) * 100.))\n",
    "            \n",
    "        feature_sets += process_example(feature_extractor, ling_data_cache, ex_id, ex)\n",
    "\n",
    "    print('Number of training examples:', len(feature_sets))\n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model_path_root = '../../data/models_new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!: Choose feature model here\n",
    "from isanlp_srl_framebank.processor_srl_framebank import FeatureModelDefault\n",
    "feature_model = FeatureModelDefault()\n",
    "main_model_path = os.path.join(main_model_path_root, 'known_preds')\n",
    "\n",
    "# from isanlp_srl_framebank.processor_srl_framebank import FeatureModelUnknownPredicates\n",
    "# feature_model = FeatureModelUnknownPredicates()\n",
    "# main_model_path = os.path.join(main_model_path_root, 'unknown_preds')\n",
    "\n",
    "#with open(os.path.join(main_model_path, 'feature_model.pckl'), 'wb') as f:\n",
    "#    pickle.dump(feature_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00%\n",
      "Error #1\n",
      "0.31%\n",
      "0.61%\n",
      "0.92%\n",
      "1.23%\n",
      "1.53%\n",
      "1.84%\n",
      "2.15%\n",
      "2.45%\n",
      "2.76%\n",
      "3.07%\n",
      "3.37%\n",
      "3.68%\n",
      "3.99%\n",
      "4.29%\n",
      "4.60%\n",
      "4.91%\n",
      "5.21%\n",
      "5.52%\n",
      "5.83%\n",
      "6.13%\n",
      "Error #2\n",
      "Error #3\n",
      "Error #4\n",
      "6.44%\n",
      "6.75%\n",
      "7.05%\n",
      "7.36%\n",
      "7.67%\n",
      "7.97%\n",
      "8.28%\n",
      "8.59%\n",
      "8.89%\n",
      "9.20%\n",
      "9.51%\n",
      "Error #5\n",
      "9.81%\n",
      "Error #6\n",
      "10.12%\n",
      "10.43%\n",
      "Error #7\n",
      "Error #8\n",
      "10.73%\n",
      "Error #9\n",
      "11.04%\n",
      "11.35%\n",
      "11.65%\n",
      "11.96%\n",
      "12.27%\n",
      "12.57%\n",
      "12.88%\n",
      "13.19%\n",
      "13.49%\n",
      "13.80%\n",
      "14.11%\n",
      "Error #10\n",
      "Error #11\n",
      "Error #12\n",
      "Error #13\n",
      "Error #14\n",
      "Error #15\n",
      "Error #16\n",
      "Error #17\n",
      "14.41%\n",
      "Error #18\n",
      "14.72%\n",
      "15.03%\n",
      "15.33%\n",
      "15.64%\n",
      "15.95%\n",
      "Error #19\n",
      "16.25%\n",
      "16.56%\n",
      "16.86%\n",
      "17.17%\n",
      "17.48%\n",
      "17.78%\n",
      "18.09%\n",
      "18.40%\n",
      "18.70%\n",
      "19.01%\n",
      "19.32%\n",
      "19.62%\n",
      "19.93%\n",
      "20.24%\n",
      "20.54%\n",
      "20.85%\n",
      "Error #20\n",
      "21.16%\n",
      "21.46%\n",
      "21.77%\n",
      "22.08%\n",
      "22.38%\n",
      "22.69%\n",
      "23.00%\n",
      "23.30%\n",
      "23.61%\n",
      "23.92%\n",
      "24.22%\n",
      "24.53%\n",
      "24.84%\n",
      "25.14%\n",
      "25.45%\n",
      "25.76%\n",
      "26.06%\n",
      "26.37%\n",
      "26.68%\n",
      "26.98%\n",
      "27.29%\n",
      "27.60%\n",
      "27.90%\n",
      "Error #21\n",
      "Error #22\n",
      "Error #23\n",
      "Error #24\n",
      "Error #25\n",
      "Error #26\n",
      "28.21%\n",
      "28.52%\n",
      "28.82%\n",
      "29.13%\n",
      "29.44%\n",
      "29.74%\n",
      "30.05%\n",
      "Error #27\n",
      "Error #28\n",
      "30.36%\n",
      "Error #29\n",
      "Error #30\n",
      "30.66%\n",
      "30.97%\n",
      "31.28%\n",
      "31.58%\n",
      "31.89%\n",
      "32.20%\n",
      "32.50%\n",
      "32.81%\n",
      "33.12%\n",
      "33.42%\n",
      "33.73%\n",
      "34.04%\n",
      "34.34%\n",
      "34.65%\n",
      "34.96%\n",
      "35.26%\n",
      "35.57%\n",
      "35.88%\n",
      "36.18%\n",
      "36.49%\n",
      "Error #31\n",
      "36.80%\n",
      "37.10%\n",
      "37.41%\n",
      "37.72%\n",
      "38.02%\n",
      "38.33%\n",
      "38.64%\n",
      "38.94%\n",
      "39.25%\n",
      "39.56%\n",
      "39.86%\n",
      "40.17%\n",
      "40.48%\n",
      "40.78%\n",
      "41.09%\n",
      "41.40%\n",
      "41.70%\n",
      "42.01%\n",
      "42.32%\n",
      "42.62%\n",
      "42.93%\n",
      "43.24%\n",
      "43.54%\n",
      "43.85%\n",
      "44.16%\n",
      "44.46%\n",
      "44.77%\n",
      "45.08%\n",
      "45.38%\n",
      "Error #32\n",
      "Error #33\n",
      "Error #34\n",
      "45.69%\n",
      "46.00%\n",
      "46.30%\n",
      "46.61%\n",
      "46.92%\n",
      "47.22%\n",
      "47.53%\n",
      "Error #35\n",
      "Error #36\n",
      "Error #37\n",
      "Error #38\n",
      "Error #39\n",
      "Error #40\n",
      "Error #41\n",
      "47.84%\n",
      "48.14%\n",
      "Error #42\n",
      "Error #43\n",
      "48.45%\n",
      "Error #44\n",
      "48.76%\n",
      "49.06%\n",
      "49.37%\n",
      "49.67%\n",
      "Error #45\n",
      "49.98%\n",
      "50.29%\n",
      "50.59%\n",
      "50.90%\n",
      "51.21%\n",
      "51.51%\n",
      "51.82%\n",
      "52.13%\n",
      "52.43%\n",
      "52.74%\n",
      "53.05%\n",
      "53.35%\n",
      "53.66%\n",
      "53.97%\n",
      "54.27%\n",
      "54.58%\n",
      "54.89%\n",
      "55.19%\n",
      "Error #46\n",
      "55.50%\n",
      "55.81%\n",
      "56.11%\n",
      "56.42%\n",
      "56.73%\n",
      "57.03%\n",
      "57.34%\n",
      "57.65%\n",
      "Error #47\n",
      "Error #48\n",
      "Error #49\n",
      "Error #50\n",
      "Error #51\n",
      "Error #52\n",
      "Error #53\n",
      "57.95%\n",
      "58.26%\n",
      "58.57%\n",
      "58.87%\n",
      "59.18%\n",
      "59.49%\n",
      "59.79%\n",
      "60.10%\n",
      "60.41%\n",
      "60.71%\n",
      "61.02%\n",
      "61.33%\n",
      "61.63%\n",
      "61.94%\n",
      "Error #54\n",
      "62.25%\n",
      "62.55%\n",
      "62.86%\n",
      "63.17%\n",
      "63.47%\n",
      "63.78%\n",
      "64.09%\n",
      "64.39%\n",
      "64.70%\n",
      "65.01%\n",
      "65.31%\n",
      "65.62%\n",
      "65.93%\n",
      "66.23%\n",
      "66.54%\n",
      "66.85%\n",
      "67.15%\n",
      "67.46%\n",
      "67.77%\n",
      "68.07%\n",
      "68.38%\n",
      "Error #55\n",
      "Error #56\n",
      "Error #57\n",
      "Error #58\n",
      "Error #59\n",
      "Error #60\n",
      "Error #61\n",
      "Error #62\n",
      "Error #63\n",
      "Error #64\n",
      "Error #65\n",
      "Error #66\n",
      "Error #67\n",
      "Error #68\n",
      "Error #69\n",
      "68.69%\n",
      "68.99%\n",
      "69.30%\n",
      "69.61%\n",
      "69.91%\n",
      "70.22%\n",
      "70.53%\n",
      "70.83%\n",
      "71.14%\n",
      "71.45%\n",
      "71.75%\n",
      "72.06%\n",
      "72.37%\n",
      "72.67%\n",
      "72.98%\n",
      "73.29%\n",
      "73.59%\n",
      "73.90%\n",
      "74.21%\n",
      "74.51%\n",
      "Error #70\n",
      "74.82%\n",
      "75.13%\n",
      "Error #71\n",
      "75.43%\n",
      "75.74%\n",
      "76.05%\n",
      "76.35%\n",
      "76.66%\n",
      "76.97%\n",
      "77.27%\n",
      "77.58%\n",
      "77.89%\n",
      "78.19%\n",
      "78.50%\n",
      "78.81%\n",
      "Error #72\n",
      "79.11%\n",
      "79.42%\n",
      "79.73%\n",
      "80.03%\n",
      "80.34%\n",
      "80.65%\n",
      "80.95%\n",
      "81.26%\n",
      "Error #73\n",
      "Error #74\n",
      "81.57%\n",
      "81.87%\n",
      "82.18%\n",
      "82.48%\n",
      "82.79%\n",
      "83.10%\n",
      "Error #75\n",
      "83.40%\n",
      "83.71%\n",
      "84.02%\n",
      "Error #76\n",
      "84.32%\n",
      "84.63%\n",
      "84.94%\n",
      "85.24%\n",
      "85.55%\n",
      "85.86%\n",
      "86.16%\n",
      "86.47%\n",
      "86.78%\n",
      "87.08%\n",
      "87.39%\n",
      "87.70%\n",
      "88.00%\n",
      "Error #77\n",
      "88.31%\n",
      "88.62%\n",
      "88.92%\n",
      "89.23%\n",
      "89.54%\n",
      "89.84%\n",
      "90.15%\n",
      "90.46%\n",
      "90.76%\n",
      "91.07%\n",
      "91.38%\n",
      "Error #78\n",
      "91.68%\n",
      "91.99%\n",
      "92.30%\n",
      "92.60%\n",
      "92.91%\n",
      "93.22%\n",
      "93.52%\n",
      "93.83%\n",
      "94.14%\n",
      "94.44%\n",
      "94.75%\n",
      "95.06%\n",
      "95.36%\n",
      "95.67%\n",
      "95.98%\n",
      "96.28%\n",
      "96.59%\n",
      "96.90%\n",
      "97.20%\n",
      "97.51%\n",
      "97.82%\n",
      "Error #79\n",
      "98.12%\n",
      "98.43%\n",
      "98.74%\n",
      "99.04%\n",
      "Error #80\n",
      "Error #81\n",
      "99.35%\n",
      "99.66%\n",
      "99.96%\n",
      "Number of training examples: 35370\n"
     ]
    }
   ],
   "source": [
    "feature_sets = prepare_train_data(examples, ling_data_cache, feature_model)\n",
    "\n",
    "data_for_pandas = []\n",
    "for example in feature_sets:\n",
    "    data_for_pandas_ex = {}\n",
    "    data_for_pandas_ex['role'] = example[1]\n",
    "    data_for_pandas_ex['ex_id'] = example[2]\n",
    "    data_for_pandas_ex['arg_address'] = example[3]\n",
    "    for elem in example[0]:\n",
    "        for subelem in elem:\n",
    "            if subelem is not None:\n",
    "                data_for_pandas_ex.update(subelem)\n",
    "    \n",
    "    data_for_pandas.append(data_for_pandas_ex)\n",
    "    \n",
    "pd_data = pd.DataFrame(data_for_pandas)\n",
    "pd_data = pd_data.sample(frac=1)\n",
    "pd_data[:10]\n",
    "del data_for_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.046180",
     "start_time": "2017-02-18T12:16:27.932519"
    }
   },
   "outputs": [],
   "source": [
    "y_stat = pd_data.loc[:, 'role'].value_counts()\n",
    "drop_ys = y_stat[y_stat < 180].index\n",
    "clear_data = pd_data.drop(pd_data[pd_data.loc[:, 'role'].isin(drop_ys)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.523645",
     "start_time": "2017-02-18T12:16:28.047978"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of roles:  34\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "агенс                                 3775\n",
       "пациенс                               3309\n",
       "тема                                  2436\n",
       "субъект перемещения                   1886\n",
       "субъект психологического состояния    1723\n",
       "причина                               1572\n",
       "место                                 1472\n",
       "говорящий                             1159\n",
       "содержание действия                   1136\n",
       "конечная точка                        1130\n",
       "содержание мысли                      1105\n",
       "содержание высказывания                973\n",
       "пациенс перемещения                    867\n",
       "стимул                                 790\n",
       "результат                              775\n",
       "субъект ментального состояния          601\n",
       "адресат                                531\n",
       "контрагент                             499\n",
       "эффектор                               492\n",
       "субъект восприятия                     431\n",
       "предмет высказывания                   382\n",
       "субъект социального отношения          376\n",
       "начальная точка                        366\n",
       "способ                                 331\n",
       "конечный посессор                      320\n",
       "источник звука                         261\n",
       "признак                                240\n",
       "цель                                   240\n",
       "сфера                                  226\n",
       "ситуация в фокусе                      216\n",
       "контрагент социального отношения       194\n",
       "субъект поведения                      194\n",
       "срок                                   182\n",
       "статус                                 181\n",
       "Name: role, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repl_roles = {\n",
    "    'агенс - субъект восприятия' : 'субъект восприятия',\n",
    "    'агенс - субъект ментального состояния' : 'субъект ментального состояния',\n",
    "    'результат / цель' : 'результат',\n",
    "    'место - пациенс' : 'место',\n",
    "    'говорящий - субъект психологического состояния' : 'субъект психологического состояния'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_single_region(data, rep, val):\n",
    "    data.loc[:, 'role'] = data.loc[:, 'role'].str.replace(rep, val)\n",
    "\n",
    "\n",
    "for rep, val in repl_roles.items():\n",
    "    normalize_single_region(clear_data, rep, val)\n",
    "    \n",
    "number_of_roles = len(clear_data.loc[:, 'role'].value_counts().index)\n",
    "print('Number of roles: ', number_of_roles)\n",
    "clear_data.loc[:, 'role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.578144",
     "start_time": "2017-02-18T12:16:28.525204"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30371, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_orig = clear_data.loc[:, 'role']\n",
    "X_orig = clear_data.drop('role', axis = 1)\n",
    "X_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:29.690583",
     "start_time": "2017-02-18T12:16:28.580765"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "y = label_encoder.fit_transform(y_orig)\n",
    "\n",
    "with open(main_model_path + '/label_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:36.643690",
     "start_time": "2017-02-18T12:16:29.701186"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size:  300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings_path = '../../data/embeddings/ruscorpora_upos_skipgram_300_5_2018.vec'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embeddings_path, binary=False)\n",
    "print('Embedding size: ', embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:36.671658",
     "start_time": "2017-02-18T12:16:36.645470"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def make_embeded_form(word):\n",
    "    if word:\n",
    "        #return word[1].encode('utf8')\n",
    "        return u\"{}_{}\".format(word[1], word[0])\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "class Embedder_map:\n",
    "    def __init__(self, embeddings, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_ = embeddings\n",
    "\n",
    "    def __call__(self, i):  \n",
    "        result = np.zeros((len(self.X_[0]), \n",
    "                           self.embeddings_.vector_size))\n",
    "\n",
    "        for j in range(len(self.X_[0])):\n",
    "            word = self.X_[i][j]\n",
    "            tag = word[0] if word else str()\n",
    "            \n",
    "            if tag == ARG_SPECIAL_TAG or tag == ARG_SPECIAL_TAG:\n",
    "                result[j, :] = np.ones(self.embeddings_.vector_size)\n",
    "            elif word and word in embeddings:\n",
    "                result[j, :] = self.embeddings_[word]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def embed(X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_map(embeddings, X), X.index, 1000)\n",
    "    pool.close()\n",
    "    return np.asarray(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:18:10.418972",
     "start_time": "2017-02-18T12:16:36.673158"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "arg_context_embedded = embed(X_orig.loc[:, 'arg_context_lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:33.028839",
     "start_time": "2017-02-18T12:18:10.420741"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pred_context_embedded = embed(X_orig.loc[:, 'pred_context_lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:33.045522",
     "start_time": "2017-02-18T12:22:33.030953"
    }
   },
   "outputs": [],
   "source": [
    "class Embedder_single_map:\n",
    "    def __init__(self, embeddings, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_ = embeddings\n",
    "\n",
    "    def __call__(self, i):\n",
    "        #word = make_embeded_form(self.X_[i])\n",
    "        word = self.X_[i]\n",
    "        if word in self.embeddings_:\n",
    "            return self.embeddings_[word]\n",
    "        else:\n",
    "            return np.zeros((self.embeddings_.vector_size,))\n",
    "\n",
    "        \n",
    "def embed_single(embeddings, X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_single_map(embeddings, X), X.index, 1000)\n",
    "    pool.close()\n",
    "        \n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:40.521526",
     "start_time": "2017-02-18T12:22:33.047988"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30371, 300)\n",
      "362\n",
      "(41,)\n",
      "CPU times: user 23.2 s, sys: 14.8 s, total: 38 s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# embedded_verbs = embed_single(pd.Series(list(zip(X_orig.pred_pos, X_orig.pred_lemma)), \n",
    "#                                         index = X_orig.index))\n",
    "embedded_verbs = embed_single(embeddings, X_orig.pred_lemma)\n",
    "print(embedded_verbs.shape)\n",
    "print((np.linalg.norm(embedded_verbs, axis = 1) < 0.001).sum())\n",
    "print(clear_data[(np.linalg.norm(embedded_verbs, axis = 1) < 0.001)].pred_lemma.value_counts().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:48.662074",
     "start_time": "2017-02-18T12:22:40.817873"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30371, 300)\n",
      "10342\n",
      "CPU times: user 21.9 s, sys: 14.7 s, total: 36.7 s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# embedded_args = embed_single(pd.Series(list(zip(X_orig.arg_pos, X_orig.arg_lemma)), \n",
    "#                                        index = X_orig.index))\n",
    "embedded_args = embed_single(embeddings, X_orig.arg_lemma)\n",
    "print(embedded_args.shape)\n",
    "print((np.linalg.norm(embedded_args, axis = 1) < 0.001).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg',\n",
       "       'Valency_arg', 'VerbForm_arg', 'arg_address', 'arg_case', 'arg_lemma',\n",
       "       'arg_pos', 'dist', 'ex_id', 'pred_lemma', 'pred_pos', 'prepos',\n",
       "       'rel_pos', 'syn_link_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.653831",
     "start_time": "2017-02-18T12:22:48.759036"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category features:\n",
      " ['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg', 'Valency_arg', 'VerbForm_arg', 'arg_case', 'arg_pos', 'dist', 'pred_lemma', 'pred_pos', 'prepos', 'syn_link_name']\n",
      "Not category features:\n",
      " ['rel_pos']\n",
      "(30371, 941)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "#morph_feats = ['pos', 'case', 'anim', 'vform', 'zform', 'shform', 'pform', 'vvform', 'nform', 'time']\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep', 'link_name'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "#all_feats = ['pred_lemma', 'rel_pos', 'pred_pos', 'arg_case', 'syn_link_name', 'arg_pos', 'prepos', 'dist']\n",
    "\n",
    "#categ_feats = [e for e in all_feats if X_orig[e].dtype in [str, object]]\n",
    "#not_categ = [e for e in all_feats if e not in categ_feats]\n",
    "\n",
    "#pred_lemma_vectorizer.fit_transform(X_orig.loc[:, ['pred_lemma']].to_dict(orient = 'records'))\n",
    "\n",
    "not_categ_features = {'arg_address', 'ex_id', 'rel_pos', 'arg_lemma'}\n",
    "categ_feats = [name for name in X_orig.columns if name not in not_categ_features] \n",
    "not_categ = ['rel_pos']\n",
    "print('Category features:\\n', categ_feats)\n",
    "print('Not category features:\\n', not_categ)\n",
    "\n",
    "vectorizer = DictVectorizer(sparse = False)\n",
    "one_hot_feats = vectorizer.fit_transform(X_orig.loc[:, categ_feats].to_dict(orient = 'records'))\n",
    "print(one_hot_feats.shape)\n",
    "\n",
    "with open(main_model_path + '/feature_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.924596",
     "start_time": "2017-02-18T12:22:52.655580"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.4/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30371, 942)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_categ_columns = np.concatenate(tuple(X_orig.loc[:, e].as_matrix().reshape(-1, 1) for e in not_categ), axis =1)\n",
    "plain_features = np.concatenate((one_hot_feats, not_categ_columns), axis = 1)\n",
    "plain_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.938339",
     "start_time": "2017-02-18T12:22:52.926218"
    }
   },
   "outputs": [],
   "source": [
    "del not_categ_columns\n",
    "del one_hot_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Convolution1D, Dropout, MaxPooling1D\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Permute\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Bidirectional\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:59.798574",
     "start_time": "2017-02-18T12:22:59.779209"
    }
   },
   "outputs": [],
   "source": [
    "def construct_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Convolution1D(nb_filter=128, \n",
    "                            filter_length=2, \n",
    "                            border_mode='same', \n",
    "                            activation='relu', \n",
    "                            input_shape = (seq_embeded.shape[1], \n",
    "                                           get_embeddings_length(embeddings))))\n",
    "\n",
    "    #model.add(MaxPooling1D(pool_length=2))\n",
    "    model.add(LSTM(80))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(60, activation='tanh'))\n",
    "    model.add(Dense(number_of_roles, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:59.838778",
     "start_time": "2017-02-18T12:22:59.800378"
    }
   },
   "outputs": [],
   "source": [
    "def construct_simple_attentional_model():\n",
    "    units = 80\n",
    "    _input = Input(shape = (arg_context_embedded.shape[1], \n",
    "                            get_embeddings_length(embeddings)), dtype = 'float')\n",
    "\n",
    "    conv = Convolution1D(nb_filter=128, \n",
    "                        filter_length=2, \n",
    "                        border_mode='same', \n",
    "                        activation='relu')(_input)\n",
    "\n",
    "    activations = LSTM(units, return_sequences=True)(conv)\n",
    "\n",
    "    # compute importance for each step\n",
    "    attention = TimeDistributed(Dense(1, activation='tanh'))(activations) \n",
    "    #attention = Dense(6, activation='tanh')(activations) \n",
    "    attention = Flatten()(attention)\n",
    "    attention = Activation('softmax')(attention)\n",
    "    attention = RepeatVector(units)(attention)\n",
    "    attention = Permute([2, 1])(attention)\n",
    "\n",
    "    # apply the attention\n",
    "    sent_representation = merge([activations, attention], mode='mul')\n",
    "    sent_representation = Lambda(lambda xin: K.sum(xin, axis=1))(sent_representation)\n",
    "\n",
    "    #dn = Dense(100, activation = 'tanh')(sent_representation)\n",
    "    #probabilities = Dense(number_of_roles, activation='softmax')(dn)\n",
    "    probabilities = Dense(number_of_roles, activation='softmax')(sent_representation)\n",
    "\n",
    "    model = Model(input=_input, output=probabilities)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:59.886182",
     "start_time": "2017-02-18T12:22:59.840571"
    }
   },
   "outputs": [],
   "source": [
    "def construct_graph_bidirectional_model():\n",
    "    print('Bidirectional model.')\n",
    "    \n",
    "    arg_context_model = Sequential()\n",
    "    arg_context_model.add(Convolution1D(nb_filter=150, \n",
    "                                        filter_length=2, \n",
    "                                        border_mode='same', \n",
    "                                        activation='relu',\n",
    "                                        input_shape = (arg_context_embedded.shape[1], \n",
    "                                                       get_embeddings_length(embeddings))))\n",
    "    arg_context_model.add(Bidirectional(LSTM(100), merge_mode = 'sum'))\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(700, \n",
    "                          input_shape=(plain_features.shape[1],), \n",
    "                          activation = 'relu'))\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    final = Sequential()\n",
    "    final.add(Merge([arg_context_model, plain_model], mode = 'concat', concat_axis=1))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    #final.add(Dense(300, activation = 'relu'))\n",
    "    final.add(Dense(300))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('relu'))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    final.add(Dense(number_of_roles))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('softmax'))\n",
    "    \n",
    "    final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:59.952522",
     "start_time": "2017-02-18T12:22:59.887939"
    }
   },
   "outputs": [],
   "source": [
    "def construct_graph_attentional_model():\n",
    "    print('Context attentional model.')\n",
    "    \n",
    "    def construct_attentional_part(context_length):\n",
    "        _input = Input(shape = (context_length, \n",
    "                                get_embeddings_length(embeddings)), dtype = 'float')\n",
    "\n",
    "        conv = Convolution1D(nb_filter=200, \n",
    "                            filter_length=2, \n",
    "                            border_mode='same', \n",
    "                            activation='relu')(_input)\n",
    "\n",
    "        units = 100\n",
    "        activations = LSTM(units, return_sequences=True)(conv)\n",
    "\n",
    "        # compute importance for each step\n",
    "        attention = TimeDistributed(Dense(1, activation='tanh'))(activations)  \n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(units)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "        # apply the attention\n",
    "        seq_repr = merge([activations, attention], mode='mul')\n",
    "        seq_repr = Lambda(lambda xin: K.sum(xin, axis=1))(seq_repr)\n",
    "        seq_model = Model(input=_input, output=seq_repr)\n",
    "        \n",
    "        return seq_model\n",
    "    \n",
    "    arg_context_model = construct_attentional_part(arg_context_embedded.shape[1])\n",
    "    pred_context_model = construct_attentional_part(pred_context_embedded.shape[1])\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(800, \n",
    "                          input_shape=(plain_features.shape[1],), \n",
    "                          activation = 'relu'))\n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    final = Sequential()\n",
    "    final.add(Merge([arg_context_model, pred_context_model, plain_model], \n",
    "                    mode = 'concat', concat_axis=1))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    #final.add(Dense(300, activation = 'relu'))\n",
    "    final.add(Dense(400))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('relu'))\n",
    "    final.add(Dropout(0.3))\n",
    "    \n",
    "    final.add(Dense(number_of_roles))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('softmax'))\n",
    "    #final.add(Dense(number_of_roles, activation = 'softmax'))\n",
    "#    final.add(BatchNormalization())\n",
    "    #final.add(Activation('softmax'), W_regularizer=l2(0.01))\n",
    "    #final.add(Dense(number_of_roles, activation='softmax', W_regularizer = l2(0.01)))\n",
    "    \n",
    "    final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:23:00.015195",
     "start_time": "2017-02-18T12:22:59.954295"
    }
   },
   "outputs": [],
   "source": [
    "def construct_graph_lstm_model(plain_features_shape):\n",
    "    print('Context model.')\n",
    "    \n",
    "    def create_embed_model():\n",
    "        embed_model = Sequential()\n",
    "        embed_model.add(Dense(100, input_shape = (get_embeddings_length(embeddings), )))\n",
    "        embed_model.add(BatchNormalization())\n",
    "        embed_model.add(Activation('relu'))\n",
    "        return embed_model\n",
    "    \n",
    "    def construct_attentional_part(context_length):\n",
    "        seq_model = Sequential()\n",
    "        seq_model.add(Convolution1D(nb_filter=50, \n",
    "                                    filter_length=1, \n",
    "                                    border_mode='same', \n",
    "                                    activation='relu',\n",
    "                                    input_shape = (context_length, \n",
    "                                                   get_embeddings_length(embeddings))))\n",
    "#         seq_model.add(Masking(mask_value=0., input_shape = (context_length, \n",
    "#                                                             get_embeddings_length(embeddings))))\n",
    "        #seq_model.add(Masking(mask_value=1.))\n",
    "        seq_model.add(Bidirectional(LSTM(50), merge_mode='sum'))\n",
    "        #seq_model.add(LSTM(100))\n",
    "        seq_model.add(Dense(50))\n",
    "        seq_model.add(BatchNormalization())\n",
    "        seq_model.add(Activation('relu'))\n",
    "        \n",
    "        return seq_model\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    #arg_context_model = construct_attentional_part(arg_context_embedded.shape[1])\n",
    "    pred_context_model = construct_attentional_part(pred_context_embedded.shape[1])\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(400, input_shape = plain_features_shape))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    arg_embed_model = create_embed_model()\n",
    "    pred_embed_model = create_embed_model()\n",
    "    \n",
    "    ###############################\n",
    "    \n",
    "    final1 = Sequential()\n",
    "    final1.add(Merge([\n",
    "  #              arg_context_model, \n",
    "                     #pred_context_model,\n",
    "                     arg_embed_model,\n",
    "                     pred_embed_model,\n",
    "                     plain_model], \n",
    "                    mode = 'concat', concat_axis=1))\n",
    "    final1.add(Dropout(0.3))\n",
    "    \n",
    "    final1.add(Dense(400))\n",
    "    final1.add(BatchNormalization())\n",
    "    final1.add(Activation('relu'))\n",
    "    final1.add(Dropout(0.3))\n",
    "    \n",
    "    final = Sequential()\n",
    "    final.add(Merge([final1, pred_context_model], mode = 'concat', concat_axis = 1))\n",
    "    \n",
    "    final.add(Dense(number_of_roles))\n",
    "    final.add(BatchNormalization())\n",
    "    final.add(Activation('softmax'))\n",
    "    \n",
    "    final.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:23:00.054095",
     "start_time": "2017-02-18T12:23:00.016955"
    }
   },
   "outputs": [],
   "source": [
    "def construct_plain_model(input_shape):\n",
    "    print('Plain model.')\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(600, \n",
    "                          #input_shape=(plain_features.shape[1],), \n",
    "                          input_shape = input_shape,\n",
    "                          activation = 'relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(400))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(number_of_roles))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('softmax'))\n",
    "    \n",
    "    plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return plain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:23:00.102016",
     "start_time": "2017-02-18T12:23:00.055842"
    }
   },
   "outputs": [],
   "source": [
    "def construct_plain_model_sparse(categ_size, emb_size, number_of_roles):    \n",
    "    input_plain = Input(shape=(categ_size,), name = 'input_categorical')\n",
    "    input_pred_embed = Input(shape=(emb_size,), name = 'pred_embed')\n",
    "    input_arg_embed = Input(shape=(emb_size,), name = 'arg_embed')\n",
    "    \n",
    "    plain = Dense(400)(input_plain)\n",
    "    plain = BatchNormalization()(plain)\n",
    "    plain = Activation('relu')(plain)\n",
    "    \n",
    "    def embed_submodel(inpt):\n",
    "        embed = Dense(100)(inpt)\n",
    "        embed = BatchNormalization()(embed)\n",
    "        embed = Activation('relu')(embed)\n",
    "        return embed\n",
    "    \n",
    "    embed_pred = embed_submodel(input_pred_embed)\n",
    "    embed_arg = embed_submodel(input_arg_embed)\n",
    "    \n",
    "    final = Concatenate(axis = 1)([embed_pred, embed_arg, plain])\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(400)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('relu')(final)\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(number_of_roles)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('softmax')(final)\n",
    "    \n",
    "    model = Model([input_arg_embed, input_pred_embed, input_plain], final)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with in-domain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for experiments with in-domain test. Do not use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T10:45:48.365172",
     "start_time": "2017-02-15T10:45:04.358434"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 600)               565800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 34)                13634     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 34)                136       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 34)                0         \n",
      "=================================================================\n",
      "Total params: 821,570\n",
      "Trainable params: 820,702\n",
      "Non-trainable params: 868\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 27333 samples, validate on 3038 samples\n",
      "Epoch 1/15\n",
      "27333/27333 [==============================] - 3s 104us/step - loss: 2.8129 - acc: 0.2740 - val_loss: 3.0625 - val_acc: 0.4819\n",
      "Epoch 2/15\n",
      "27333/27333 [==============================] - 2s 59us/step - loss: 1.6620 - acc: 0.6489 - val_loss: 2.7789 - val_acc: 0.6695\n",
      "Epoch 3/15\n",
      "27333/27333 [==============================] - 2s 58us/step - loss: 1.1971 - acc: 0.7468 - val_loss: 2.3659 - val_acc: 0.7413\n",
      "Epoch 4/15\n",
      "27333/27333 [==============================] - 2s 57us/step - loss: 1.0134 - acc: 0.7743 - val_loss: 1.8510 - val_acc: 0.7620\n",
      "Epoch 5/15\n",
      "27333/27333 [==============================] - 2s 56us/step - loss: 0.8799 - acc: 0.7972 - val_loss: 1.3903 - val_acc: 0.7699\n",
      "Epoch 6/15\n",
      "27333/27333 [==============================] - 2s 58us/step - loss: 0.7948 - acc: 0.8123 - val_loss: 1.0495 - val_acc: 0.7762\n",
      "Epoch 7/15\n",
      "27333/27333 [==============================] - 2s 58us/step - loss: 0.7162 - acc: 0.8260 - val_loss: 0.9071 - val_acc: 0.7818\n",
      "Epoch 8/15\n",
      "27333/27333 [==============================] - 2s 58us/step - loss: 0.6636 - acc: 0.8340 - val_loss: 0.8318 - val_acc: 0.7837\n",
      "Epoch 9/15\n",
      "27333/27333 [==============================] - 2s 58us/step - loss: 0.6143 - acc: 0.8448 - val_loss: 0.8016 - val_acc: 0.7818\n",
      "Epoch 10/15\n",
      "27333/27333 [==============================] - 2s 58us/step - loss: 0.5705 - acc: 0.8542 - val_loss: 0.7748 - val_acc: 0.7880\n",
      "Epoch 11/15\n",
      "27333/27333 [==============================] - 2s 57us/step - loss: 0.5306 - acc: 0.8602 - val_loss: 0.7671 - val_acc: 0.7814\n",
      "Epoch 12/15\n",
      "27333/27333 [==============================] - 2s 57us/step - loss: 0.5068 - acc: 0.8663 - val_loss: 0.7751 - val_acc: 0.7808\n",
      "Epoch 13/15\n",
      "27333/27333 [==============================] - 2s 60us/step - loss: 0.4798 - acc: 0.8732 - val_loss: 0.7764 - val_acc: 0.7860\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x7fe978b41668>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = construct_plain_model((plain_features.shape[1],))\n",
    "print(model.summary())\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit(plain_features, y, epochs=15, batch_size=300, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T10:49:20.090543",
     "start_time": "2017-02-16T10:49:19.585615"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27333 samples, validate on 3038 samples\n",
      "Epoch 1/16\n",
      "27333/27333 [==============================] - 3s 109us/step - loss: 2.4376 - acc: 0.4047 - val_loss: 3.0106 - val_acc: 0.4470\n",
      "Epoch 2/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 1.5193 - acc: 0.6818 - val_loss: 2.5690 - val_acc: 0.6643\n",
      "Epoch 3/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 1.1941 - acc: 0.7476 - val_loss: 1.9987 - val_acc: 0.7475\n",
      "Epoch 4/16\n",
      "27333/27333 [==============================] - 2s 60us/step - loss: 1.0121 - acc: 0.7764 - val_loss: 1.4612 - val_acc: 0.7732\n",
      "Epoch 5/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 0.8896 - acc: 0.7973 - val_loss: 1.0874 - val_acc: 0.7880\n",
      "Epoch 6/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 0.8041 - acc: 0.8119 - val_loss: 0.9153 - val_acc: 0.7933\n",
      "Epoch 7/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 0.7283 - acc: 0.8266 - val_loss: 0.8118 - val_acc: 0.7992\n",
      "Epoch 8/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 0.6714 - acc: 0.8352 - val_loss: 0.7717 - val_acc: 0.7982\n",
      "Epoch 9/16\n",
      "27333/27333 [==============================] - 2s 62us/step - loss: 0.6170 - acc: 0.8477 - val_loss: 0.7493 - val_acc: 0.8032\n",
      "Epoch 10/16\n",
      "27333/27333 [==============================] - 2s 60us/step - loss: 0.5775 - acc: 0.8538 - val_loss: 0.7111 - val_acc: 0.8058\n",
      "Epoch 11/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 0.5419 - acc: 0.8628 - val_loss: 0.7066 - val_acc: 0.8032\n",
      "Epoch 12/16\n",
      "27333/27333 [==============================] - 2s 62us/step - loss: 0.5115 - acc: 0.8665 - val_loss: 0.6905 - val_acc: 0.8061\n",
      "Epoch 13/16\n",
      "27333/27333 [==============================] - 2s 62us/step - loss: 0.4805 - acc: 0.8741 - val_loss: 0.6943 - val_acc: 0.8022\n",
      "Epoch 14/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 0.4500 - acc: 0.8821 - val_loss: 0.6888 - val_acc: 0.8071\n",
      "Epoch 15/16\n",
      "27333/27333 [==============================] - 2s 60us/step - loss: 0.4251 - acc: 0.8884 - val_loss: 0.6832 - val_acc: 0.8097\n",
      "Epoch 16/16\n",
      "27333/27333 [==============================] - 2s 61us/step - loss: 0.4119 - acc: 0.8897 - val_loss: 0.6822 - val_acc: 0.8022\n"
     ]
    }
   ],
   "source": [
    "model = construct_plain_model_sparse(plain_features.shape[1], embeddings.vector_size, y.shape[1])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([embedded_args, embedded_verbs, plain_features], y, epochs=16, batch_size=300, \n",
    "          validation_split = 0.1, shuffle=True, callbacks = [early_stopping])\n",
    "model.save(os.path.join(main_model_path, 'neural_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T23:18:09.873456",
     "start_time": "2017-02-15T23:07:16.496170"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context model.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pred_context_embedded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-bb3db1ae881b>\u001b[0m in \u001b[0;36mconstruct_graph_lstm_model\u001b[0;34m(plain_features_shape)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#arg_context_model = construct_attentional_part(arg_context_embedded.shape[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mpred_context_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_attentional_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_context_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m###############################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_context_embedded' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = construct_graph_lstm_model((plain_features.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([arg_context_embedded, pred_context_embedded, embedded_args, embedded_verbs, plain_features], y, \n",
    "          epochs=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiements with out-of-domain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for out of domain experiments. Do not use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T14:21:13.954628",
     "start_time": "2017-02-17T14:21:13.936038"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_out_of_domain(model, X_train, y_train, X_test, y_test):\n",
    "    final_res = list()\n",
    "    N_ITERATIONS = 5\n",
    "    for i in xrange(N_ITERATIONS):\n",
    "        print('Eval iter:', i + 1, '/', N_ITERATIONS)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                       patience=2, verbose=0, mode='auto')\n",
    "        model.fit(X_train, y_train, nb_epoch=15, \n",
    "                  batch_size=64, validation_split = 0.1, \n",
    "                  shuffle=True, callbacks = [early_stopping],\n",
    "                 verbose = 0)\n",
    "\n",
    "        ev_res = evaluate_model(model, X_test, y_test)\n",
    "        print()\n",
    "        print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))\n",
    "        final_res.append(ev_res)\n",
    "    \n",
    "    return np.array(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:26:02.646008",
     "start_time": "2017-02-18T12:26:02.630666"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    keras_eval = model.evaluate(X_test, y_test)\n",
    "    pred = model.predict(X_test).argmax(axis = 1)\n",
    "    f1_micro = f1_score(pred, y_test.argmax(axis = 1), average = 'micro')\n",
    "    f1_macro = f1_score(pred, y_test.argmax(axis = 1), average = 'macro')\n",
    "    accur = accuracy_score(pred, y_test.argmax(axis = 1))\n",
    "    \n",
    "    return np.array(list(keras_eval) + [f1_micro, f1_macro, accur])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T10:15:31.636957",
     "start_time": "2017-02-17T10:13:07.614626"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind_plain_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-1c86164ffa53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_plain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_plain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model.fit(ind_plain_features, ind_y, nb_epoch=15, batch_size=64, validation_split = 0.1, \n\u001b[1;32m      4\u001b[0m           shuffle=True, callbacks = [early_stopping])\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ind_plain_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = construct_plain_model((ind_plain_features.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit(ind_plain_features, ind_y, nb_epoch=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "\n",
    "#model.evaluate(ood_plain_features, ood_y)\n",
    "ev_res = evaluate_model(model, [ood_plain_features], ood_y)\n",
    "print()\n",
    "print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:32:19.012435",
     "start_time": "2017-02-17T11:26:21.034200"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind_plain_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-3d528230dc12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_plain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_plain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_out_of_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_plain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mood_plain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mood_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdescribe_cv_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ind_plain_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = construct_plain_model((ind_plain_features.shape[1],))\n",
    "model_eval = evaluate_out_of_domain(model, ind_plain_features, ind_y, ood_plain_features, ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:51:12.792397",
     "start_time": "2017-02-17T11:50:35.226172"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind_plain_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-4d5f4251cadb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_plain_model_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_plain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit([ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, nb_epoch=20, batch_size=64, validation_split = 0.1, \n\u001b[1;32m      5\u001b[0m           shuffle=True, callbacks = [early_stopping])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ind_plain_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, nb_epoch=20, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "#model.evaluate([ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "\n",
    "ev_res = evaluate_model(model, [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "print()\n",
    "print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T16:02:37.654028",
     "start_time": "2017-02-17T15:54:39.172343"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind_plain_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-3bc4dc908d2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_plain_model_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_plain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model_eval = evaluate_out_of_domain(model, \n\u001b[1;32m      4\u001b[0m                                     \u001b[0;34m[\u001b[0m\u001b[0mind_arg_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_pred_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_plain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ind_plain_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, \n",
    "                                    [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T13:21:40.582402",
     "start_time": "2017-02-17T13:21:21.643939"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind_plain_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-37510d149ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_graph_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_plain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.fit([\n\u001b[1;32m      5\u001b[0m            \u001b[0;31m#ind_arg_context,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ind_plain_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = construct_graph_lstm_model((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([\n",
    "           #ind_arg_context, \n",
    "        #ind_pred_context,   \n",
    "        ind_arg_embed, \n",
    "        ind_pred_embed, \n",
    "        ind_plain_features,\n",
    "        ind_pred_context], \n",
    "#model.fit([ind_arg_context, ind_pred_context, ind_arg_embed, ind_pred_embed, ind_plain_features], \n",
    "           ind_y, nb_epoch=6, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "\n",
    "#model.evaluate([ood_arg_context, ood_pred_context, ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "model.evaluate([\n",
    "    #    ood_arg_context, \n",
    "    #    ood_pred_context,\n",
    "        ood_arg_embed, \n",
    "        ood_pred_embed,\n",
    "        ood_plain_features,\n",
    "        ood_pred_context\n",
    "    ], ood_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T15:41:36.049234",
     "start_time": "2017-02-17T15:25:34.248413"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ind_plain_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-40f22d132761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_graph_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mind_plain_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m model_eval = evaluate_out_of_domain(model, \n\u001b[1;32m      4\u001b[0m                                     \u001b[0;34m[\u001b[0m\u001b[0mind_arg_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_pred_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_plain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_pred_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mind_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                     [ood_arg_embed, ood_pred_embed, ood_plain_features, ood_pred_context], ood_y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ind_plain_features' is not defined"
     ]
    }
   ],
   "source": [
    "model = construct_graph_lstm_model((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_pred_embed, ind_plain_features, ind_pred_context], ind_y, \n",
    "                                    [ood_arg_embed, ood_pred_embed, ood_plain_features, ood_pred_context], ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for model comparision. Do not use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:43:55.066960",
     "start_time": "2017-02-17T11:43:54.950449"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, *args, **kwargs):\n",
    "    model.fit(X_train, y_train, *args, **kwargs)\n",
    "    \n",
    "    keras_eval = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    pred = model.predict(X_test).argmax(axis = 1)\n",
    "    f1_micro = f1_score(pred, y_test.argmax(axis = 1), average = 'micro')\n",
    "    f1_macro = f1_score(pred, y_test.argmax(axis = 1), average = 'macro')\n",
    "    accur = accuracy_score(pred, y_test.argmax(axis = 1))\n",
    "    \n",
    "    return list(keras_eval) + [f1_micro, f1_macro, accur]\n",
    "    \n",
    "\n",
    "def custom_cross_val(cr_f, X, y, cv, *args, **kwargs):\n",
    "    cr_f().summary()\n",
    "    eval_res = list()\n",
    "    for i, (train, test) in enumerate(cv.split(y)):\n",
    "        model = cr_f()\n",
    "        print('Running Fold', i+1, '/', cv.n_splits)\n",
    "        eval1 = train_and_evaluate_model(model, \n",
    "                                         [X[j][train] for j in range(len(X))], y[train], \n",
    "                                         [X[j][test] for j in range(len(X))], y[test], \n",
    "                                         *args, **kwargs)\n",
    "        \n",
    "        print()\n",
    "        print('Fold result: ', eval1)\n",
    "        eval_res.append(eval1)\n",
    "    \n",
    "    return np.array(eval_res)\n",
    "\n",
    "\n",
    "def describe_cv_result(cv_res):\n",
    "    print(cv_res)\n",
    "    mean_cv_res = cv_res.mean(axis = 0)\n",
    "    std_cv_res = cv_res.std(axis = 0)\n",
    "    print('Mean')\n",
    "    print(pd.DataFrame([mean_cv_res], columns = ['loss', 'keras_accur', 'micro_f1', 'macro_f1', 'accur']))\n",
    "    print('Std')\n",
    "    print(pd.DataFrame([std_cv_res], columns = ['loss', 'keras_accur', 'micro_f1', 'macro_f1', 'accur']))\n",
    "    \n",
    "    \n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T16:16:18.702196",
     "start_time": "2017-02-16T16:07:42.457676"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'no_lemma_plain_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-34c687a37e26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcurr_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_lemma_plain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedded_verbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m cv_res = custom_cross_val(lambda : construct_plain_model((curr_features.shape[1],)), \n\u001b[1;32m      3\u001b[0m                           \u001b[0;34m[\u001b[0m\u001b[0mcurr_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                           \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                           validation_split = 0., shuffle=True, verbose = 0)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'no_lemma_plain_features' is not defined"
     ]
    }
   ],
   "source": [
    "curr_features = np.concatenate((no_lemma_plain_features, embedded_verbs), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : construct_plain_model((curr_features.shape[1],)), \n",
    "                          [curr_features], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T15:36:12.375033",
     "start_time": "2017-02-16T15:27:40.793790"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plain model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 600)               565800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 600)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 400)               240400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 34)                13634     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 34)                136       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 34)                0         \n",
      "=================================================================\n",
      "Total params: 821,570\n",
      "Trainable params: 820,702\n",
      "Non-trainable params: 868\n",
      "_________________________________________________________________\n",
      "Plain model.\n",
      "Running Fold 1 / 5\n",
      "6075/6075 [==============================] - 0s 74us/step\n",
      "\n",
      "Fold result:  [0.7787424363226557, 0.7822222222810911, 0.7822222222222223, 0.7516953200874452, 0.7822222222222223]\n",
      "Plain model.\n",
      "Running Fold 2 / 5\n",
      "6074/6074 [==============================] - 0s 65us/step\n",
      "\n",
      "Fold result:  [0.8150516212241682, 0.7782351004869325, 0.778235100428054, 0.7599974018105824, 0.778235100428054]\n",
      "Plain model.\n",
      "Running Fold 3 / 5\n"
     ]
    }
   ],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_plain_model((plain_features.shape[1],)), \n",
    "                          [plain_features], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T14:48:33.877302",
     "start_time": "2017-02-16T14:45:18.196997"
    }
   },
   "outputs": [],
   "source": [
    "single_chunk = np.concatenate((embedded_args, embedded_verbs, plain_features), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : construct_plain_model((single_chunk.shape[1],)), \n",
    "                          [single_chunk], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T16:23:57.011641",
     "start_time": "2017-02-17T16:07:49.454278"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pred_embed (InputLayer)         (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arg_embed (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_categorical (InputLayer)  (None, 990)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_92 (Dense)                (None, 100)          30100       pred_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_93 (Dense)                (None, 100)          30100       arg_embed[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_91 (Dense)                (None, 400)          396400      input_categorical[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 100)          400         dense_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 100)          400         dense_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 400)          1600        dense_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 100)          0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 100)          0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 400)          0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 600)          0           activation_77[0][0]              \n",
      "                                                                 activation_78[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_49 (Dropout)            (None, 600)          0           concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_94 (Dense)                (None, 400)          240400      dropout_49[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 400)          1600        dense_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 400)          0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_50 (Dropout)            (None, 400)          0           activation_79[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_95 (Dense)                (None, 44)           17644       dropout_50[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 44)           176         dense_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 44)           0           batch_normalization_80[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 718,820\n",
      "Trainable params: 716,732\n",
      "Non-trainable params: 2,088\n",
      "__________________________________________________________________________________________________\n",
      "Running Fold 1 / 5\n",
      "10541/10541 [==============================] - 2s 204us/step\n",
      "\n",
      "Fold result:  [0.6577002111221217, 0.8126363722606963, 0.8126363722606963, 0.7853088545072477, 0.8126363722606963]\n",
      "Running Fold 2 / 5\n",
      "10540/10540 [==============================] - 2s 212us/step\n",
      "\n",
      "Fold result:  [0.6710936888119301, 0.804838709654799, 0.8048387096774193, 0.7726367565472689, 0.8048387096774193]\n",
      "Running Fold 3 / 5\n",
      "10540/10540 [==============================] - 2s 223us/step\n",
      "\n",
      "Fold result:  [0.6850881543229608, 0.8060721062844799, 0.8060721062618595, 0.7772395776418514, 0.8060721062618595]\n",
      "Running Fold 4 / 5\n",
      "10540/10540 [==============================] - 2s 231us/step\n",
      "\n",
      "Fold result:  [0.6682090597767983, 0.8074952561443626, 0.8074952561669829, 0.770870075588247, 0.8074952561669829]\n",
      "Running Fold 5 / 5\n",
      "10540/10540 [==============================] - 3s 237us/step\n",
      "\n",
      "Fold result:  [0.6917032712777154, 0.8018026565238692, 0.8018026565464895, 0.7733449592452944, 0.8018026565464895]\n",
      "[[0.65770021 0.81263637 0.81263637 0.78530885 0.81263637]\n",
      " [0.67109369 0.80483871 0.80483871 0.77263676 0.80483871]\n",
      " [0.68508815 0.80607211 0.80607211 0.77723958 0.80607211]\n",
      " [0.66820906 0.80749526 0.80749526 0.77087008 0.80749526]\n",
      " [0.69170327 0.80180266 0.80180266 0.77334496 0.80180266]]\n",
      "Mean\n",
      "       loss  keras_accur  micro_f1  macro_f1     accur\n",
      "0  0.674759     0.806569  0.806569   0.77588  0.806569\n",
      "Std\n",
      "       loss  keras_accur  micro_f1  macro_f1     accur\n",
      "0  0.012175     0.003567  0.003567  0.005154  0.003567\n"
     ]
    }
   ],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_plain_model_sparse(plain_features.shape[1], \n",
    "                                                                embeddings.vector_size, \n",
    "                                                                y.shape[1]), \n",
    "                          [embedded_args, embedded_verbs, plain_features], y, \n",
    "                          cv = cv, epochs=13, batch_size=300,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T14:16:02.109004",
     "start_time": "2017-02-16T13:49:33.683794"
    }
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_graph_lstm_model((plain_features.shape[1],)), \n",
    "                          [arg_context_embedded, \n",
    "                           pred_context_embedded, \n",
    "                           embedded_args, \n",
    "                           embedded_verbs,\n",
    "                           plain_features], y, \n",
    "                          cv = cv, epochs=6, batch_size=64, validation_split = 0., \n",
    "                          shuffle=True)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:59:05.065644",
     "start_time": "2017-02-18T12:59:05.006966"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(X_orig.ex_id.unique(), test_size=0.2, random_state=42)\n",
    "train_ids = set(train_ids.tolist())\n",
    "test_ids = set(test_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T13:04:58.313461",
     "start_time": "2017-02-18T13:04:58.267195"
    }
   },
   "outputs": [],
   "source": [
    "train_selector_pd = X_orig.ex_id.isin(train_ids)\n",
    "test_selector_pd = X_orig.ex_id.isin(test_ids)\n",
    "train_selector = train_selector_pd.values\n",
    "test_selector = test_selector_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T11:29:41.203903",
     "start_time": "2017-02-18T11:28:02.698301"
    }
   },
   "outputs": [],
   "source": [
    "train_data = {k : data[k] for k in train_ids}\n",
    "test_data = {k : data[k] for k in test_ids}\n",
    "\n",
    "with open(os.path.join(main_model_path, 'train_data.json'), 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(os.path.join(main_model_path, 'test_data.json'), 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T13:00:50.036868",
     "start_time": "2017-02-18T12:59:10.294248"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37893 samples, validate on 4211 samples\n",
      "Epoch 1/10\n",
      "37893/37893 [==============================] - 11s 290us/step - loss: 1.9349 - acc: 0.5656 - val_loss: 1.1430 - val_acc: 0.7421\n",
      "Epoch 2/10\n",
      "37893/37893 [==============================] - 9s 232us/step - loss: 1.1200 - acc: 0.7305 - val_loss: 0.8598 - val_acc: 0.7727\n",
      "Epoch 3/10\n",
      "37893/37893 [==============================] - 9s 232us/step - loss: 0.9209 - acc: 0.7613 - val_loss: 0.7801 - val_acc: 0.7822\n",
      "Epoch 4/10\n",
      "37893/37893 [==============================] - 9s 231us/step - loss: 0.8213 - acc: 0.7741 - val_loss: 0.7397 - val_acc: 0.7903\n",
      "Epoch 5/10\n",
      "37893/37893 [==============================] - 9s 229us/step - loss: 0.7521 - acc: 0.7903 - val_loss: 0.7198 - val_acc: 0.7948\n",
      "Epoch 6/10\n",
      "37893/37893 [==============================] - 9s 230us/step - loss: 0.7003 - acc: 0.7981 - val_loss: 0.7003 - val_acc: 0.7920\n",
      "Epoch 7/10\n",
      "37893/37893 [==============================] - 9s 231us/step - loss: 0.6519 - acc: 0.8097 - val_loss: 0.6985 - val_acc: 0.8019\n",
      "Epoch 8/10\n",
      "37893/37893 [==============================] - 9s 232us/step - loss: 0.6229 - acc: 0.8151 - val_loss: 0.6913 - val_acc: 0.8024\n",
      "Epoch 9/10\n",
      "37893/37893 [==============================] - 9s 231us/step - loss: 0.5941 - acc: 0.8228 - val_loss: 0.7003 - val_acc: 0.7993\n",
      "Epoch 10/10\n",
      "37893/37893 [==============================] - 9s 230us/step - loss: 0.5611 - acc: 0.8335 - val_loss: 0.6884 - val_acc: 0.8027\n",
      "10597/10597 [==============================] - 1s 102us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6926666886275118, 0.8026800037746532]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_from_nparray_list(nparray_list, selector):\n",
    "    return [e[selector] for e in nparray_list]\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], embeddings.vector_size, y.shape[1])\n",
    "model.fit(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], train_selector),\n",
    "          select_from_nparray_list([y], train_selector), \n",
    "          epochs=10, batch_size=64, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "model.evaluate(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector), \n",
    "               select_from_nparray_list([y], test_selector))\n",
    "model.save(os.path.join(main_model_path, 'neural_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:55:42.486066",
     "start_time": "2017-02-18T15:55:39.780097"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10597/10597 [==============================] - 1s 108us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.69266669, 0.80268   , 0.80268   , 0.77108236, 0.80268   ])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hold-out evaluation.\n",
    "\n",
    "evaluate_model(model,\n",
    "               select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector), \n",
    "               select_from_nparray_list([y], test_selector)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:56:50.418354",
     "start_time": "2017-02-18T15:56:40.777599"
    }
   },
   "outputs": [],
   "source": [
    "pred = model.predict(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector))\n",
    "\n",
    "test_examples_to_store = X_orig.loc[test_selector_pd[test_selector_pd].index, :].loc[:, ['arg_address', 'ex_id']]\n",
    "test_data = {k : data[k] for k in test_ids}\n",
    "\n",
    "\n",
    "for index, (pd_index, row) in enumerate(test_examples_to_store.iterrows()):\n",
    "    ex = test_data[row['ex_id']]\n",
    "    arg_addr = row['arg_address']\n",
    "    sent = ex[arg_addr[0]]\n",
    "    token = sent[arg_addr[1]]\n",
    "    cl = pred[index]\n",
    "    predicted_role = label_encoder.inverse_transform(np.array([cl]))[0]\n",
    "    actual_role = label_encoder.inverse_transform(np.array([select_from_nparray_list([y], test_selector)[0][index]]))[0]\n",
    "    \n",
    "    token['rolepred1'] = actual_role\n",
    "    token['rolepred2'] = predicted_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:59:40.060358",
     "start_time": "2017-02-18T15:59:26.421186"
    }
   },
   "outputs": [],
   "source": [
    "with open('./test_data_ann_1.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brat convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts results to brat annotation for inspecting.\n",
    "# Needs framebank_preprocessing from http://nlp.isa.ru/framebank_parser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T17:15:09.301844",
     "start_time": "2017-02-18T17:14:58.429103"
    }
   },
   "outputs": [],
   "source": [
    "!python2.7 ./framebank_preprocessing/convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./brat_ann2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T17:27:49.179234",
     "start_time": "2017-02-18T17:27:30.320938"
    }
   },
   "outputs": [],
   "source": [
    "!python2.7 ./framebank_preprocessing/convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./syntaxnet_1/ --converter=syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-11 20:56:20 - Loading corpus data...\n",
      "2018-03-11 20:56:22 - Done.\n",
      "2018-03-11 20:56:22 - Creating verb-example index...\n",
      "2018-03-11 20:56:22 - Done.\n",
      "2018-03-11 20:56:22 - Converting and saving...\n",
      "2018-03-11 20:56:23 - Done.\n",
      "2018-03-11 20:56:23 - Generating brat configuration files...\n",
      "2018-03-11 20:56:23 - Done.\n"
     ]
    }
   ],
   "source": [
    "!export PYTHONPATH=../ && python2.7 ./convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./brat_ann2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "51px",
    "width": "313px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "505px",
    "left": "0px",
    "right": "1122px",
    "top": "110px",
    "width": "158px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
