{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install deeppavlov\n",
    "pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0711 14:57:49.760032 139724900800256 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.embedders.elmo_embedder import ELMoEmbedder\n",
    "\n",
    "elmo = ELMoEmbedder(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-wiki_600k_steps.tar.gz\", elmo_output_names=['elmo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 1024)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(elmo([['дерево', 'упало', 'в', 'лесу']])).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:14:55.909843",
     "start_time": "2017-02-18T12:14:54.905927"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Use only one GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../isanlp/src/')\n",
    "sys.path.append('../../src/isanlp_srl_framebank/')\n",
    "sys.path.append('../../libs/')\n",
    "sys.path.append('../../libs/pylingtools/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress tensorflow memory appetites\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.log_device_placement=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:14:56.607418",
     "start_time": "2017-02-18T12:14:55.911842"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available GPUs\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15391527924475858861, name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 10647984082678140372\n",
       " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 18205530140682358928\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 7139262464\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 11030196173676868469\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0c:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import isanlp\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(31)\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_corpus_path = '../../data/cleared_corpus.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(cleared_corpus_path, 'r') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_data_path = '../../data/results_final_fixed.pckl'\n",
    "with open(ling_data_path, 'rb') as f:\n",
    "    ling_data_cache = pickle.load(f)\n",
    "\n",
    "ling_data_cache = {k: v for k,v in ling_data_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_examples = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from isanlp.annotation_repr import CSentence\n",
    "from convert_corpus_to_brat import make_text\n",
    "\n",
    "\n",
    "def find_address_by_offset(offset, ling_ann):\n",
    "    for tok_num, tok in enumerate(ling_ann['tokens']):\n",
    "        if tok.begin <= offset and offset < tok.end:\n",
    "            break\n",
    "    \n",
    "    for sent_num, sent in enumerate(ling_ann['sentences']):\n",
    "        if sent.begin <= tok_num and tok_num < sent.end:\n",
    "            break\n",
    "    \n",
    "    return sent_num, tok_num - sent.begin\n",
    "\n",
    "\n",
    "def process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, example):\n",
    "    feature_sets = list()\n",
    "    \n",
    "    text, offset_index = make_text(example, 0)\n",
    "    ling_ann = ling_cache[ex_id]\n",
    "    \n",
    "    pred_offset = offset_index[(pred[0], pred[1])]\n",
    "    pred_ling_sent, pred_ling_word = find_address_by_offset(pred_offset, ling_ann)\n",
    "    \n",
    "    for arg in args:\n",
    "        arg_offset = offset_index[(arg[0], arg[1])]\n",
    "        arg_ling_sent, arg_ling_word = find_address_by_offset(arg_offset, ling_ann)\n",
    "        \n",
    "        fb_pred_word = example[pred[0]][pred[1]]\n",
    "        fb_arg_word = example[arg[0]][arg[1]]\n",
    "        \n",
    "        if arg_ling_sent != pred_ling_sent:\n",
    "            error_examples[ex_id] = {\n",
    "                'reason': 'sent_mismatch',\n",
    "                'arg': arg_ling_sent,\n",
    "                'pred': pred_ling_sent\n",
    "            }\n",
    "            continue\n",
    "            \n",
    "        sentence = ling_ann['sentences'][pred_ling_sent]\n",
    "        tokens = [tok.text for tok in ling_ann['tokens']]\n",
    "        tokens = tokens[sentence.begin:sentence.end]\n",
    "        \n",
    "        role = fb_arg_word['rolepred1']\n",
    "\n",
    "        features = feature_extractor.extract_features(pred_ling_word, \n",
    "                                                      arg_ling_word, \n",
    "                                                      ling_ann['postag'][arg_ling_sent],\n",
    "                                                      ling_ann['morph'][arg_ling_sent],\n",
    "                                                      ling_ann['lemma'][arg_ling_sent],\n",
    "                                                      ling_ann['syntax_dep_tree'][arg_ling_sent])\n",
    "\n",
    "                    \n",
    "        feature_sets.append((features, role, ex_id, tokens, arg_ling_word, pred_ling_word))\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def process_example(feature_extractor, ling_cache, ex_id, sentences):\n",
    "    pred = None\n",
    "    args = list()\n",
    "    for sent_num, sent in enumerate(sentences):\n",
    "        for word_num, word in enumerate(sent):\n",
    "            if 'rank' in word and word['rank'] == 'Предикат':\n",
    "                pred = (sent_num, word_num)\n",
    "            elif 'rolepred1' in word:\n",
    "                args.append((sent_num, word_num))\n",
    "    \n",
    "    return process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, sentences)\n",
    "\n",
    "\n",
    "num_of_errors = 0\n",
    "def prepare_train_data(examples, ling_data_cache, feature_extractor):\n",
    "    feature_sets = []\n",
    "    for ex_num, (ex_id, ex) in tqdm(list(enumerate(examples))):                \n",
    "        feature_sets += process_example(feature_extractor, ling_data_cache, ex_id, ex)\n",
    "\n",
    "    print('Number of training examples:', len(feature_sets))\n",
    "    return feature_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model_path_root = '../../data/models_new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!!: Choose feature model here\n",
    "from isanlp_srl_framebank.processor_srl_framebank import FeatureModelDefault\n",
    "feature_model = FeatureModelDefault()\n",
    "main_model_path = os.path.join(main_model_path_root, 'known_preds')\n",
    "\n",
    "# from isanlp_srl_framebank.processor_srl_framebank import FeatureModelUnknownPredicates\n",
    "# feature_model = FeatureModelUnknownPredicates()\n",
    "# main_model_path = os.path.join(main_model_path_root, 'unknown_preds')\n",
    "\n",
    "#with open(os.path.join(main_model_path, 'feature_model.pckl'), 'wb') as f:\n",
    "#    pickle.dump(feature_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'begin': 0, 'end': 17}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vars(ling_data_cache['30587']['sentences'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(error_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdec9f4a5c24912a1b03a2e8c8d337c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of training examples: 57552\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animacy_arg</th>\n",
       "      <th>Aspect_arg</th>\n",
       "      <th>Gender_arg</th>\n",
       "      <th>Number_arg</th>\n",
       "      <th>Tense_arg</th>\n",
       "      <th>Valency_arg</th>\n",
       "      <th>VerbForm_arg</th>\n",
       "      <th>arg_address</th>\n",
       "      <th>arg_case</th>\n",
       "      <th>arg_lemma</th>\n",
       "      <th>...</th>\n",
       "      <th>dist</th>\n",
       "      <th>ex_id</th>\n",
       "      <th>prd_address</th>\n",
       "      <th>pred_lemma</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>prepos</th>\n",
       "      <th>rel_pos</th>\n",
       "      <th>role</th>\n",
       "      <th>syn_link_name</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37891</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td></td>\n",
       "      <td>слишком_ADV</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73079</td>\n",
       "      <td>12</td>\n",
       "      <td>смотреть_VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>содержание мысли</td>\n",
       "      <td>advmod</td>\n",
       "      <td>[--, Не, знаю, -, с, ,, а, на, меня, так, уж, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32497</th>\n",
       "      <td>Anim</td>\n",
       "      <td></td>\n",
       "      <td>Masc</td>\n",
       "      <td>Sing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>Acc</td>\n",
       "      <td>отец_NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>136433</td>\n",
       "      <td>10</td>\n",
       "      <td>информировать_VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>адресат</td>\n",
       "      <td>obl</td>\n",
       "      <td>[Наконец, все, было, готово, ,, отца, снабдили...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46864</th>\n",
       "      <td>Anim</td>\n",
       "      <td></td>\n",
       "      <td>Masc</td>\n",
       "      <td>Sing</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Nom</td>\n",
       "      <td>артем_NOUN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47480</td>\n",
       "      <td>1</td>\n",
       "      <td>выслушивать_VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>субъект восприятия</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>[Артем, выслушал, Хилого, и, потребовал, к, те...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Animacy_arg Aspect_arg Gender_arg Number_arg Tense_arg Valency_arg  \\\n",
       "37891                                                                      \n",
       "32497        Anim                  Masc       Sing                         \n",
       "46864        Anim                  Masc       Sing                         \n",
       "\n",
       "      VerbForm_arg  arg_address arg_case    arg_lemma  \\\n",
       "37891                        11           слишком_ADV   \n",
       "32497                         5      Acc    отец_NOUN   \n",
       "46864                         0      Nom   артем_NOUN   \n",
       "\n",
       "                             ...                         dist   ex_id  \\\n",
       "37891                        ...                          1.0   73079   \n",
       "32497                        ...                          5.0  136433   \n",
       "46864                        ...                          1.0   47480   \n",
       "\n",
       "      prd_address          pred_lemma pred_pos prepos rel_pos  \\\n",
       "37891          12       смотреть_VERB     VERB            1.0   \n",
       "32497          10  информировать_VERB     VERB            1.0   \n",
       "46864           1    выслушивать_VERB     VERB            1.0   \n",
       "\n",
       "                     role syn_link_name  \\\n",
       "37891    содержание мысли        advmod   \n",
       "32497             адресат           obl   \n",
       "46864  субъект восприятия         nsubj   \n",
       "\n",
       "                                                  tokens  \n",
       "37891  [--, Не, знаю, -, с, ,, а, на, меня, так, уж, ...  \n",
       "32497  [Наконец, все, было, готово, ,, отца, снабдили...  \n",
       "46864  [Артем, выслушал, Хилого, и, потребовал, к, те...  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_sets = prepare_train_data(examples, ling_data_cache, feature_model)\n",
    "\n",
    "data_for_pandas = []\n",
    "for example in feature_sets:\n",
    "    data_for_pandas_ex = {}\n",
    "    data_for_pandas_ex['role'] = example[1]\n",
    "    data_for_pandas_ex['ex_id'] = example[2]\n",
    "    data_for_pandas_ex['tokens'] = example[3]\n",
    "    data_for_pandas_ex['arg_address'] = example[4]\n",
    "    data_for_pandas_ex['prd_address'] = example[5]\n",
    "    for elem in example[0]:\n",
    "        for subelem in elem:\n",
    "            if subelem is not None:\n",
    "                data_for_pandas_ex.update(subelem)\n",
    "    \n",
    "    data_for_pandas.append(data_for_pandas_ex)\n",
    "    \n",
    "pd_data = pd.DataFrame(data_for_pandas)\n",
    "del data_for_pandas\n",
    "pd_data = pd_data.sample(frac=1)\n",
    "pd_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Argument position: 1\n",
      "Argument lemma: он_PRON\n",
      "--\n",
      "Predicat position: 2\n",
      "Predicat lemma: рубить_VERB\n",
      "--\n",
      "Distance 1\n",
      "--\n",
      "Sentence tokens: [(0, 'Головы'), (1, 'им'), (2, 'рубить'), (3, 'нужно'), (4, ','), (5, 'чтоб'), (6, 'они'), (7, 'не'), (8, 'трогали'), (9, 'наших'), (10, 'девушек'), (11, '!')]\n",
      "------------------------------------------------------------\n",
      "Argument position: 10\n",
      "Argument lemma: себя_PRON\n",
      "--\n",
      "Predicat position: 12\n",
      "Predicat lemma: обманывать_VERB\n",
      "--\n",
      "Distance 2\n",
      "--\n",
      "Sentence tokens: [(0, 'И'), (1, 'все'), (2, 'они'), (3, 'вместе'), (4, 'улыбались'), (5, 'и'), (6, 'кланялись'), (7, ','), (8, 'а'), (9, 'я'), (10, 'себя'), (11, 'чувствовал'), (12, 'обманутым'), (13, 'и'), (14, 'оскорблённым'), (15, '.')]\n"
     ]
    }
   ],
   "source": [
    "N_verify = 2\n",
    "for i in np.random.choice(len(pd_data), size=N_verify):\n",
    "    print(\"-\"*60)\n",
    "    obj = pd_data.iloc[i]\n",
    "    print(f\"Argument position: {obj.arg_address}\")\n",
    "    print(f\"Argument lemma: {obj.arg_lemma}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Predicat position: {obj.prd_address}\")\n",
    "    print(f\"Predicat lemma: {obj.pred_lemma}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Distance {int(obj.dist)}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Sentence tokens: {list(enumerate(obj.tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ling_data_cache\n",
    "del examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Убрать все фичи предикатного слова для модели Unknown Predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.046180",
     "start_time": "2017-02-18T12:16:27.932519"
    }
   },
   "outputs": [],
   "source": [
    "y_stat = pd_data.loc[:, 'role'].value_counts()\n",
    "drop_ys = y_stat[y_stat < 180].index\n",
    "clear_data = pd_data.drop(pd_data[pd_data.loc[:, 'role'].isin(drop_ys)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.523645",
     "start_time": "2017-02-18T12:16:28.047978"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of roles:  44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "агенс                                 6147\n",
       "пациенс                               5362\n",
       "тема                                  3656\n",
       "субъект психологического состояния    3250\n",
       "субъект перемещения                   3011\n",
       "причина                               2502\n",
       "говорящий                             2365\n",
       "место                                 2185\n",
       "содержание действия                   1874\n",
       "содержание мысли                      1817\n",
       "содержание высказывания               1792\n",
       "конечная точка                        1772\n",
       "результат                             1452\n",
       "пациенс перемещения                   1356\n",
       "стимул                                1271\n",
       "субъект ментального состояния         1223\n",
       "адресат                                941\n",
       "субъект восприятия                     901\n",
       "контрагент                             831\n",
       "эффектор                               739\n",
       "субъект социального отношения          598\n",
       "начальная точка                        588\n",
       "предмет высказывания                   548\n",
       "способ                                 531\n",
       "конечный посессор                      506\n",
       "цель                                   454\n",
       "сфера                                  376\n",
       "признак                                366\n",
       "источник звука                         359\n",
       "субъект поведения                      339\n",
       "ситуация в фокусе                      322\n",
       "контрагент социального отношения       318\n",
       "субъект физиологической реакции        310\n",
       "предмет мысли                          303\n",
       "потенциальный пациенс                  290\n",
       "статус                                 265\n",
       "пациенс социального отношения          261\n",
       "эталон                                 255\n",
       "срок                                   255\n",
       "признак действия                       243\n",
       "каузатор                               223\n",
       "исходный посессор                      217\n",
       "потенциальная угроза                   197\n",
       "траектория                             180\n",
       "Name: role, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repl_roles = {\n",
    "    'агенс - субъект восприятия' : 'субъект восприятия',\n",
    "    'агенс - субъект ментального состояния' : 'субъект ментального состояния',\n",
    "    'результат / цель' : 'результат',\n",
    "    'место - пациенс' : 'место',\n",
    "    'говорящий - субъект психологического состояния' : 'субъект психологического состояния'\n",
    "}\n",
    "\n",
    "\n",
    "def normalize_single_region(data, rep, val):\n",
    "    data.loc[:, 'role'] = data.loc[:, 'role'].str.replace(rep, val)\n",
    "\n",
    "\n",
    "for rep, val in repl_roles.items():\n",
    "    normalize_single_region(clear_data, rep, val)\n",
    "    \n",
    "number_of_roles = len(clear_data.loc[:, 'role'].value_counts().index)\n",
    "print('Number of roles: ', number_of_roles)\n",
    "clear_data.loc[:, 'role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['содержание мысли', 'адресат', 'субъект восприятия',\n",
       "       'конечная точка', 'содержание высказывания', 'агенс',\n",
       "       'субъект ментального состояния', 'говорящий', 'пациенс',\n",
       "       'начальная точка', 'субъект психологического состояния',\n",
       "       'содержание действия', 'субъект перемещения', 'причина',\n",
       "       'потенциальная угроза', 'место', 'тема', 'стимул',\n",
       "       'конечный посессор', 'пациенс социального отношения',\n",
       "       'субъект физиологической реакции', 'субъект поведения',\n",
       "       'контрагент социального отношения', 'предмет высказывания',\n",
       "       'пациенс перемещения', 'признак', 'контрагент',\n",
       "       'потенциальный пациенс', 'сфера', 'способ', 'цель', 'эффектор',\n",
       "       'статус', 'результат', 'эталон', 'траектория',\n",
       "       'субъект социального отношения', 'каузатор', 'источник звука',\n",
       "       'исходный посессор', 'предмет мысли', 'ситуация в фокусе',\n",
       "       'признак действия', 'срок'], dtype=object)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clear_data.role.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:28.578144",
     "start_time": "2017-02-18T12:16:28.525204"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52751, 20)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_orig = clear_data.loc[:, 'role']\n",
    "X_orig = clear_data.drop('role', axis = 1)\n",
    "X_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:29.690583",
     "start_time": "2017-02-18T12:16:28.580765"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "y = label_encoder.fit_transform(y_orig)\n",
    "\n",
    "with open(main_model_path + '/label_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52751, 44)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:36.643690",
     "start_time": "2017-02-18T12:16:29.701186"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size:  300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings_path = '../../data/ruscorpora_upos_skipgram_300_5_2018.vec'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embeddings_path, binary=False)\n",
    "print('Embedding size: ', embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:16:36.671658",
     "start_time": "2017-02-18T12:16:36.645470"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def make_embeded_form(word):\n",
    "    if word:\n",
    "        #return word[1].encode('utf8')\n",
    "        return u\"{}_{}\".format(word[1], word[0])\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "class Embedder_map:\n",
    "    def __init__(self, embeddings, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_ = embeddings\n",
    "\n",
    "    def __call__(self, i):  \n",
    "        result = np.zeros(embeddings.vector_size)\n",
    "        \n",
    "        ARG_SPECIAL_TAG = None  # ??\n",
    "\n",
    "        word = self.X_[i]\n",
    "        if embeddings.vocab.get(word):\n",
    "            return embeddings[word]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def embed(X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_map(embeddings, X), list(range(len(X))), 1000)\n",
    "    pool.close()\n",
    "#     embedder = Embedder_map(embeddings, X)\n",
    "#     result =[embedder(i) for i in range(2)]\n",
    "#     #result = [embedder(i) for i in range(len(X))]\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg',\n",
       "       'Valency_arg', 'VerbForm_arg', 'arg_address', 'arg_case', 'arg_lemma',\n",
       "       'arg_pos', 'dist', 'ex_id', 'prd_address', 'pred_lemma', 'pred_pos',\n",
       "       'prepos', 'rel_pos', 'syn_link_name', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:18:10.418972",
     "start_time": "2017-02-18T12:16:36.673158"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.5 s, sys: 15.2 s, total: 42.7 s\n",
      "Wall time: 49.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "arg_embedded = embed(X_orig['arg_lemma'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52751"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arg_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:33.028839",
     "start_time": "2017-02-18T12:18:10.420741"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.6 s, sys: 15.2 s, total: 42.8 s\n",
      "Wall time: 50.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_embedded = embed(X_orig['pred_lemma'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/w2v_verbs.npy\", pred_embedded)\n",
    "np.save(\"../../data/w2v_args.npy\", arg_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:33.045522",
     "start_time": "2017-02-18T12:22:33.030953"
    }
   },
   "outputs": [],
   "source": [
    "class Embedder_single_map:\n",
    "    def __init__(self, embeddings, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_ = embeddings\n",
    "\n",
    "    def __call__(self, i):\n",
    "        #word = make_embeded_form(self.X_[i])\n",
    "        word = self.X_[i]\n",
    "        if word in self.embeddings_:\n",
    "            return self.embeddings_[word]\n",
    "        else:\n",
    "            return np.zeros((self.embeddings_.vector_size,))\n",
    "\n",
    "        \n",
    "def embed_single(embeddings, X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_single_map(embeddings, X), X.index, 1000)\n",
    "    pool.close()\n",
    "        \n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo = ELMoEmbedder(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-wiki_600k_steps.tar.gz\", elmo_output_names=['elmo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_embed(embeddings, tokens, word_idx1, word_idx2):\n",
    "    embedded = embeddings([tokens])[0]\n",
    "    return embedded[min(word_idx1, len(tokens)-1)], embedded[min(word_idx2, len(tokens)-1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = X_orig.iloc[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verb_idx = obj.prd_address\n",
    "arg_idx = obj.arg_address\n",
    "tokens = obj.tokens\n",
    "embed_verb, embed_arg = elmo_embed(elmo, tokens, verb_idx, arg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e936fabf1245eda03b0f4af153b615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2h 40min 22s, sys: 16min 19s, total: 2h 56min 41s\n",
      "Wall time: 1h 21min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embedded_verbs = []\n",
    "embedded_args  = []\n",
    "for i in tqdm(range(len(X_orig))):\n",
    "    try:\n",
    "        if i % 100 == 0:\n",
    "            with open(\"./log.txt\", 'a', encoding='utf-8') as log:\n",
    "                print(f\"Processed {i} examples\", file=log)\n",
    "        obj = X_orig.iloc[i]\n",
    "        verb_idx = obj.prd_address\n",
    "        arg_idx = obj.arg_address\n",
    "        tokens = obj.tokens\n",
    "        embed_verb, embed_arg = elmo_embed(elmo, tokens, verb_idx, arg_idx)\n",
    "        embedded_verbs.append(embed_verb)\n",
    "        embedded_args.append(embed_arg)\n",
    "    except Exception as e:\n",
    "        with open(\"./log.txt\", 'a', encoding='utf-8') as log:\n",
    "            print(f\"Error while processing example {i}={X_orig.iloc[i]}: {e}\", file=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52751, 1024) (52751, 1024)\n"
     ]
    }
   ],
   "source": [
    "e_verbs = np.stack(embedded_verbs)\n",
    "e_args  = np.stack(embedded_args)\n",
    "\n",
    "print(e_verbs.shape, e_args.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/elmo_verbs.npy\", e_verbs)\n",
    "np.save(\"../../data/elmo_args.npy\", e_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9361637 ,  0.46197   , -0.8599031 , ..., -0.46175578,\n",
       "        0.61335385, -0.33758122], dtype=float32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_verbs[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_verbs = np.load(\"../../data/elmo_verbs.npy\")\n",
    "e_args = np.load(\"../../data/elmo_args.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing categorial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig = X_orig.drop(['tokens'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.653831",
     "start_time": "2017-02-18T12:22:48.759036"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category features:\n",
      " ['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg', 'Valency_arg', 'VerbForm_arg', 'arg_case', 'arg_pos', 'dist', 'prd_address', 'pred_lemma', 'pred_pos', 'prepos', 'syn_link_name']\n",
      "Not category features:\n",
      " ['rel_pos']\n",
      "(52751, 817)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#morph_feats = ['pos', 'case', 'anim', 'vform', 'zform', 'shform', 'pform', 'vvform', 'nform', 'time']\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep', 'link_name'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "#all_feats = ['pred_lemma', 'rel_pos', 'pred_pos', 'arg_case', 'syn_link_name', 'arg_pos', 'prepos', 'dist']\n",
    "\n",
    "#categ_feats = [e for e in all_feats if X_orig[e].dtype in [str, object]]\n",
    "#not_categ = [e for e in all_feats if e not in categ_feats]\n",
    "\n",
    "#pred_lemma_vectorizer.fit_transform(X_orig.loc[:, ['pred_lemma']].to_dict(orient = 'records'))\n",
    "\n",
    "not_categ_features = {'arg_address', 'ex_id', 'rel_pos', 'arg_lemma'}\n",
    "categ_feats = [name for name in X_orig.columns if name not in not_categ_features] \n",
    "not_categ = ['rel_pos']\n",
    "print('Category features:\\n', categ_feats)\n",
    "print('Not category features:\\n', not_categ)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# X_train[categorical_cols] = X_train[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "# one_hot_feats = vectorizer.fit_transform(X_orig[categ_feats].to_dict(orient='records'))\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "one_hot_feats = vectorizer.fit_transform(X_orig[categ_feats].to_dict(orient='records'))\n",
    "print(one_hot_feats.shape)\n",
    "\n",
    "with open(main_model_path + '/feature_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/labnpnpels.npy\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.924596",
     "start_time": "2017-02-18T12:22:52.655580"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.pyenv/versions/3.6.7/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(52751, 818)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_categ_columns = np.concatenate(tuple(X_orig.loc[:, e].as_matrix().reshape(-1, 1) for e in not_categ), axis =1)\n",
    "plain_features = np.concatenate((one_hot_feats, not_categ_columns), axis = 1)\n",
    "plain_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/plain_features.npy\", plain_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:22:52.938339",
     "start_time": "2017-02-18T12:22:52.926218"
    }
   },
   "outputs": [],
   "source": [
    "del not_categ_columns\n",
    "del one_hot_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Convolution1D, Dropout, MaxPooling1D\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Permute\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Bidirectional\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:23:00.054095",
     "start_time": "2017-02-18T12:23:00.016955"
    }
   },
   "outputs": [],
   "source": [
    "def construct_plain_model(input_shape):\n",
    "    print('Plain model.')\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(600, \n",
    "                          #input_shape=(plain_features.shape[1],), \n",
    "                          input_shape = input_shape,\n",
    "                          activation = 'relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(400))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(number_of_roles))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('softmax'))\n",
    "    \n",
    "    plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return plain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:23:00.102016",
     "start_time": "2017-02-18T12:23:00.055842"
    }
   },
   "outputs": [],
   "source": [
    "def construct_plain_model_sparse(categ_size, emb_size, number_of_roles):    \n",
    "    input_plain = Input(shape=(categ_size,), name = 'input_categorical')\n",
    "    input_pred_embed = Input(shape=(emb_size,), name = 'pred_embed')\n",
    "    input_arg_embed = Input(shape=(emb_size,), name = 'arg_embed')\n",
    "    \n",
    "    plain = Dense(400)(input_plain)\n",
    "    plain = BatchNormalization()(plain)\n",
    "    plain = Activation('relu')(plain)\n",
    "    \n",
    "    def embed_submodel(inpt):\n",
    "        embed = Dense(100)(inpt)\n",
    "        embed = BatchNormalization()(embed)\n",
    "        embed = Activation('relu')(embed)\n",
    "        return embed\n",
    "    \n",
    "    embed_pred = embed_submodel(input_pred_embed)\n",
    "    embed_arg = embed_submodel(input_arg_embed)\n",
    "    \n",
    "    final = Concatenate(axis = 1)([embed_pred, embed_arg, plain])\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(400)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('relu')(final)\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(number_of_roles)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('softmax')(final)\n",
    "    \n",
    "    model = Model([input_arg_embed, input_pred_embed, input_plain], final)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with in-domain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for experiments with in-domain test. Do not use for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T10:45:48.365172",
     "start_time": "2017-02-15T10:45:04.358434"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model((plain_features.shape[1],))\n",
    "print(model.summary())\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit(plain_features, y, epochs=15, batch_size=300, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T10:49:20.090543",
     "start_time": "2017-02-16T10:49:19.585615"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model_sparse(plain_features.shape[1], 1024, y.shape[1])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([e_args, e_verbs, plain_features], y, epochs=16, batch_size=300, \n",
    "          validation_split = 0.1, shuffle=True, callbacks = [early_stopping])\n",
    "model.save(os.path.join(main_model_path, 'neural_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-15T23:18:09.873456",
     "start_time": "2017-02-15T23:07:16.496170"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = construct_graph_lstm_model((plain_features.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([arg_context_embedded, pred_context_embedded, embedded_args, embedded_verbs, plain_features], y, \n",
    "          epochs=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiements with out-of-domain test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for out of domain experiments. Do not use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T14:21:13.954628",
     "start_time": "2017-02-17T14:21:13.936038"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_out_of_domain(model, X_train, y_train, X_test, y_test):\n",
    "    final_res = list()\n",
    "    N_ITERATIONS = 5\n",
    "    for i in xrange(N_ITERATIONS):\n",
    "        print('Eval iter:', i + 1, '/', N_ITERATIONS)\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, \n",
    "                                       patience=2, verbose=0, mode='auto')\n",
    "        model.fit(X_train, y_train, nb_epoch=15, \n",
    "                  batch_size=64, validation_split = 0.1, \n",
    "                  shuffle=True, callbacks = [early_stopping],\n",
    "                 verbose = 0)\n",
    "\n",
    "        ev_res = evaluate_model(model, X_test, y_test)\n",
    "        print()\n",
    "        print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))\n",
    "        final_res.append(ev_res)\n",
    "    \n",
    "    return np.array(final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:26:02.646008",
     "start_time": "2017-02-18T12:26:02.630666"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    keras_eval = model.evaluate(X_test, y_test)\n",
    "    pred = model.predict(X_test).argmax(axis = 1)\n",
    "    f1_micro = f1_score(pred, y_test.argmax(axis = 1), average = 'micro')\n",
    "    f1_macro = f1_score(pred, y_test.argmax(axis = 1), average = 'macro')\n",
    "    accur = accuracy_score(pred, y_test.argmax(axis = 1))\n",
    "    \n",
    "    return np.array(list(keras_eval) + [f1_micro, f1_macro, accur])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T10:15:31.636957",
     "start_time": "2017-02-17T10:13:07.614626"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model((ind_plain_features.shape[1],))\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit(ind_plain_features, ind_y, nb_epoch=15, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "\n",
    "#model.evaluate(ood_plain_features, ood_y)\n",
    "ev_res = evaluate_model(model, [ood_plain_features], ood_y)\n",
    "print()\n",
    "print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:32:19.012435",
     "start_time": "2017-02-17T11:26:21.034200"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model((ind_plain_features.shape[1],))\n",
    "model_eval = evaluate_out_of_domain(model, ind_plain_features, ind_y, ood_plain_features, ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:51:12.792397",
     "start_time": "2017-02-17T11:50:35.226172"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, nb_epoch=20, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "#model.evaluate([ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "\n",
    "ev_res = evaluate_model(model, [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "print()\n",
    "print(pd.DataFrame([ev_res], columns = ['keras_accur', 'keras_loss', 'f1_micro', 'f1_macro', 'accur']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T16:02:37.654028",
     "start_time": "2017-02-17T15:54:39.172343"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_plain_model_sparse((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_pred_embed, ind_plain_features], ind_y, \n",
    "                                    [ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T13:21:40.582402",
     "start_time": "2017-02-17T13:21:21.643939"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_graph_lstm_model((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "model.fit([\n",
    "           #ind_arg_context, \n",
    "        #ind_pred_context,   \n",
    "        ind_arg_embed, \n",
    "        ind_pred_embed, \n",
    "        ind_plain_features,\n",
    "        ind_pred_context], \n",
    "#model.fit([ind_arg_context, ind_pred_context, ind_arg_embed, ind_pred_embed, ind_plain_features], \n",
    "           ind_y, nb_epoch=6, batch_size=64, validation_split = 0.1, \n",
    "          shuffle=True, callbacks = [early_stopping])\n",
    "\n",
    "#model.evaluate([ood_arg_context, ood_pred_context, ood_arg_embed, ood_pred_embed, ood_plain_features], ood_y)\n",
    "model.evaluate([\n",
    "    #    ood_arg_context, \n",
    "    #    ood_pred_context,\n",
    "        ood_arg_embed, \n",
    "        ood_pred_embed,\n",
    "        ood_plain_features,\n",
    "        ood_pred_context\n",
    "    ], ood_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T15:41:36.049234",
     "start_time": "2017-02-17T15:25:34.248413"
    }
   },
   "outputs": [],
   "source": [
    "model = construct_graph_lstm_model((ind_plain_features.shape[1],))\n",
    "model.summary()\n",
    "model_eval = evaluate_out_of_domain(model, \n",
    "                                    [ind_arg_embed, ind_pred_embed, ind_plain_features, ind_pred_context], ind_y, \n",
    "                                    [ood_arg_embed, ood_pred_embed, ood_plain_features, ood_pred_context], ood_y)\n",
    "print(model_eval)\n",
    "describe_cv_result(model_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only for model comparision. Do not use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T11:43:55.066960",
     "start_time": "2017-02-17T11:43:54.950449"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(model, X_train, y_train, X_test, y_test, *args, **kwargs):\n",
    "    model.fit(X_train, y_train, *args, **kwargs)\n",
    "    \n",
    "    keras_eval = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    pred = model.predict(X_test).argmax(axis = 1)\n",
    "    f1_micro = f1_score(pred, y_test.argmax(axis = 1), average = 'micro')\n",
    "    f1_macro = f1_score(pred, y_test.argmax(axis = 1), average = 'macro')\n",
    "    accur = accuracy_score(pred, y_test.argmax(axis = 1))\n",
    "    \n",
    "    return list(keras_eval) + [f1_micro, f1_macro, accur]\n",
    "    \n",
    "\n",
    "def custom_cross_val(cr_f, X, y, cv, *args, **kwargs):\n",
    "    cr_f().summary()\n",
    "    eval_res = list()\n",
    "    for i, (train, test) in enumerate(cv.split(y)):\n",
    "        model = cr_f()\n",
    "        print('Running Fold', i+1, '/', cv.n_splits)\n",
    "        eval1 = train_and_evaluate_model(model, \n",
    "                                         [X[j][train] for j in range(len(X))], y[train], \n",
    "                                         [X[j][test] for j in range(len(X))], y[test], \n",
    "                                         *args, **kwargs)\n",
    "        \n",
    "        print()\n",
    "        print('Fold result: ', eval1)\n",
    "        eval_res.append(eval1)\n",
    "    \n",
    "    return np.array(eval_res)\n",
    "\n",
    "\n",
    "def describe_cv_result(cv_res):\n",
    "    print(cv_res)\n",
    "    mean_cv_res = cv_res.mean(axis = 0)\n",
    "    std_cv_res = cv_res.std(axis = 0)\n",
    "    print('Mean')\n",
    "    print(pd.DataFrame([mean_cv_res], columns = ['loss', 'keras_accur', 'micro_f1', 'macro_f1', 'accur']))\n",
    "    print('Std')\n",
    "    print(pd.DataFrame([std_cv_res], columns = ['loss', 'keras_accur', 'micro_f1', 'macro_f1', 'accur']))\n",
    "    \n",
    "    \n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T16:16:18.702196",
     "start_time": "2017-02-16T16:07:42.457676"
    }
   },
   "outputs": [],
   "source": [
    "curr_features = np.concatenate((plain_features, e_verbs, e_args), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : construct_plain_model((curr_features.shape[1],)), \n",
    "                          [curr_features], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T15:36:12.375033",
     "start_time": "2017-02-16T15:27:40.793790"
    }
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_plain_model((plain_features.shape[1],)), \n",
    "                          [plain_features], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T14:48:33.877302",
     "start_time": "2017-02-16T14:45:18.196997"
    }
   },
   "outputs": [],
   "source": [
    "single_chunk = np.concatenate((e_args, e_verbs, plain_features), axis = 1)\n",
    "cv_res = custom_cross_val(lambda : construct_plain_model((single_chunk.shape[1],)), \n",
    "                          [single_chunk], \n",
    "                          y, cv = cv, epochs=13, batch_size=64,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-17T16:23:57.011641",
     "start_time": "2017-02-17T16:07:49.454278"
    }
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_plain_model_sparse(plain_features.shape[1], \n",
    "                                                                embeddings.vector_size, \n",
    "                                                                y.shape[1]), \n",
    "                          [embedded_args, embedded_verbs, plain_features], y, \n",
    "                          cv = cv, epochs=13, batch_size=300,\n",
    "                          validation_split = 0., shuffle=True, verbose = 0)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-16T14:16:02.109004",
     "start_time": "2017-02-16T13:49:33.683794"
    }
   },
   "outputs": [],
   "source": [
    "cv_res = custom_cross_val(lambda : construct_graph_lstm_model((plain_features.shape[1],)), \n",
    "                          [arg_context_embedded, \n",
    "                           pred_context_embedded, \n",
    "                           embedded_args, \n",
    "                           embedded_verbs,\n",
    "                           plain_features], y, \n",
    "                          cv = cv, epochs=6, batch_size=64, validation_split = 0., \n",
    "                          shuffle=True)\n",
    "\n",
    "describe_cv_result(cv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T12:59:05.065644",
     "start_time": "2017-02-18T12:59:05.006966"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(X_orig.ex_id.unique(), test_size=0.2, random_state=42)\n",
    "train_ids = set(train_ids.tolist())\n",
    "test_ids = set(test_ids.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T13:04:58.313461",
     "start_time": "2017-02-18T13:04:58.267195"
    }
   },
   "outputs": [],
   "source": [
    "train_selector_pd = X_orig.ex_id.isin(train_ids)\n",
    "test_selector_pd = X_orig.ex_id.isin(test_ids)\n",
    "train_selector = train_selector_pd.values\n",
    "test_selector = test_selector_pd.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T11:29:41.203903",
     "start_time": "2017-02-18T11:28:02.698301"
    }
   },
   "outputs": [],
   "source": [
    "train_data = {k : data[k] for k in train_ids}\n",
    "test_data = {k : data[k] for k in test_ids}\n",
    "\n",
    "with open(os.path.join(main_model_path, 'train_data.json'), 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(os.path.join(main_model_path, 'test_data.json'), 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T13:00:50.036868",
     "start_time": "2017-02-18T12:59:10.294248"
    }
   },
   "outputs": [],
   "source": [
    "def select_from_nparray_list(nparray_list, selector):\n",
    "    return [e[selector] for e in nparray_list]\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], embeddings.vector_size, y.shape[1])\n",
    "model.fit(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], train_selector),\n",
    "          select_from_nparray_list([y], train_selector), \n",
    "          epochs=10, batch_size=64, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "model.evaluate(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector), \n",
    "               select_from_nparray_list([y], test_selector))\n",
    "model.save(os.path.join(main_model_path, 'neural_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:55:42.486066",
     "start_time": "2017-02-18T15:55:39.780097"
    }
   },
   "outputs": [],
   "source": [
    "# Hold-out evaluation.\n",
    "\n",
    "evaluate_model(model,\n",
    "               select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector), \n",
    "               select_from_nparray_list([y], test_selector)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:56:50.418354",
     "start_time": "2017-02-18T15:56:40.777599"
    }
   },
   "outputs": [],
   "source": [
    "pred = model.predict(select_from_nparray_list([embedded_args, embedded_verbs, plain_features], test_selector))\n",
    "\n",
    "test_examples_to_store = X_orig.loc[test_selector_pd[test_selector_pd].index, :].loc[:, ['arg_address', 'ex_id']]\n",
    "test_data = {k : data[k] for k in test_ids}\n",
    "\n",
    "\n",
    "for index, (pd_index, row) in enumerate(test_examples_to_store.iterrows()):\n",
    "    ex = test_data[row['ex_id']]\n",
    "    arg_addr = row['arg_address']\n",
    "    sent = ex[arg_addr[0]]\n",
    "    token = sent[arg_addr[1]]\n",
    "    cl = pred[index]\n",
    "    predicted_role = label_encoder.inverse_transform(np.array([cl]))[0]\n",
    "    actual_role = label_encoder.inverse_transform(np.array([select_from_nparray_list([y], test_selector)[0][index]]))[0]\n",
    "    \n",
    "    token['rolepred1'] = actual_role\n",
    "    token['rolepred2'] = predicted_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T15:59:40.060358",
     "start_time": "2017-02-18T15:59:26.421186"
    }
   },
   "outputs": [],
   "source": [
    "with open('./test_data_ann_1.json', 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brat convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts results to brat annotation for inspecting.\n",
    "# Needs framebank_preprocessing from http://nlp.isa.ru/framebank_parser/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T17:15:09.301844",
     "start_time": "2017-02-18T17:14:58.429103"
    }
   },
   "outputs": [],
   "source": [
    "!python2.7 ./framebank_preprocessing/convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./brat_ann2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-02-18T17:27:49.179234",
     "start_time": "2017-02-18T17:27:30.320938"
    }
   },
   "outputs": [],
   "source": [
    "!python2.7 ./framebank_preprocessing/convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./syntaxnet_1/ --converter=syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PYTHONPATH=../ && python2.7 ./convert_corpus_to_brat.py --inputFile=./test_data_ann_1.json --outputDir=./brat_ann2/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "51px",
    "width": "313px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "505px",
    "left": "0px",
    "right": "1122px",
    "top": "110px",
    "width": "158px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
