{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only one GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../isanlp/src/')\n",
    "sys.path.append('../../src/isanlp_srl_framebank/')\n",
    "sys.path.append('../../libs/')\n",
    "sys.path.append('../../libs/pylingtools/')\n",
    "\n",
    "# Supress tensorflow memory appetites\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.log_device_placement=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "# Check available GPUs\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and make the train/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import isanlp\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_corpus_path = '../../data/cleared_corpus.json'\n",
    "\n",
    "with open(cleared_corpus_path, 'r') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_data_path = '../../data/results_final_fixed.pckl'\n",
    "with open(ling_data_path, 'rb') as f:\n",
    "    ling_data_cache = pickle.load(f)\n",
    "\n",
    "ling_data_cache = {k: v for k,v in ling_data_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(list(ling_data_cache.keys()), test_size=0.2, random_state=42)\n",
    "train_ids = list(set(train_ids))\n",
    "test_ids = list(set(test_ids))\n",
    "\n",
    "data_path = '../../data/'\n",
    "main_model_path_root = '../../data/models_new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [example for example in examples if example[0] in train_ids]\n",
    "test_data = [example for example in examples if example[0] in test_ids]\n",
    "\n",
    "with open(os.path.join(data_path, 'train_data.json'), 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(os.path.join(data_path, 'test_data.json'), 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../isanlp/src/')\n",
    "sys.path.append('../../src/isanlp_srl_framebank/')\n",
    "sys.path.append('../../libs/')\n",
    "sys.path.append('../../libs/pylingtools/')\n",
    "\n",
    "\n",
    "from isanlp.annotation_repr import CSentence\n",
    "from convert_corpus_to_brat import make_text\n",
    "\n",
    "\n",
    "def find_address_by_offset(offset, ling_ann):\n",
    "    for tok_num, tok in enumerate(ling_ann['tokens']):\n",
    "        if tok.begin <= offset and offset < tok.end:\n",
    "            break\n",
    "    \n",
    "    for sent_num, sent in enumerate(ling_ann['sentences']):\n",
    "        if sent.begin <= tok_num and tok_num < sent.end:\n",
    "            break\n",
    "    \n",
    "    return sent_num, tok_num - sent.begin\n",
    "\n",
    "\n",
    "error_examples = {}\n",
    "\n",
    "def process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, example):\n",
    "    feature_sets = list()\n",
    "    \n",
    "    text, offset_index = make_text(example, 0)\n",
    "    ling_ann = ling_cache[ex_id]\n",
    "    \n",
    "    pred_offset = offset_index[(pred[0], pred[1])]\n",
    "    pred_ling_sent, pred_ling_word = find_address_by_offset(pred_offset, ling_ann)\n",
    "    \n",
    "    for arg in args:\n",
    "        arg_offset = offset_index[(arg[0], arg[1])]\n",
    "        arg_ling_sent, arg_ling_word = find_address_by_offset(arg_offset, ling_ann)\n",
    "        \n",
    "        fb_pred_word = example[pred[0]][pred[1]]\n",
    "        fb_arg_word = example[arg[0]][arg[1]]\n",
    "        \n",
    "        if arg_ling_sent != pred_ling_sent:\n",
    "            error_examples[ex_id] = {\n",
    "                'reason': 'sent_mismatch',\n",
    "                'arg': arg_ling_sent,\n",
    "                'pred': pred_ling_sent\n",
    "            }\n",
    "            continue\n",
    "            \n",
    "        sentence = ling_ann['sentences'][pred_ling_sent]\n",
    "        tokens = [tok.text for tok in ling_ann['tokens']]\n",
    "        tokens = tokens[sentence.begin:sentence.end]\n",
    "        \n",
    "        role = fb_arg_word['rolepred1']\n",
    "\n",
    "        features = feature_extractor.extract_features(pred_ling_word, \n",
    "                                                      arg_ling_word, \n",
    "                                                      ling_ann['postag'][arg_ling_sent],\n",
    "                                                      ling_ann['morph'][arg_ling_sent],\n",
    "                                                      ling_ann['lemma'][arg_ling_sent],\n",
    "                                                      ling_ann['syntax_dep_tree'][arg_ling_sent])\n",
    "\n",
    "                    \n",
    "        feature_sets.append((features, role, ex_id, tokens, arg_ling_word, pred_ling_word))\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def process_example(feature_extractor, ling_cache, ex_id, sentences):\n",
    "    pred = None\n",
    "    args = list()\n",
    "    for sent_num, sent in enumerate(sentences):\n",
    "        for word_num, word in enumerate(sent):\n",
    "            if 'rank' in word and word['rank'] == 'Предикат':\n",
    "                pred = (sent_num, word_num)\n",
    "            elif 'rolepred1' in word:\n",
    "                args.append((sent_num, word_num))\n",
    "    \n",
    "    return process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, sentences)\n",
    "\n",
    "\n",
    "num_of_errors = 0\n",
    "def prepare_train_data(examples, ling_data_cache, feature_extractor):\n",
    "    feature_sets = []\n",
    "    for ex_num, (ex_id, ex) in tqdm(list(enumerate(examples))):                \n",
    "        feature_sets += process_example(feature_extractor, ling_data_cache, ex_id, ex)\n",
    "\n",
    "    print('Number of examples:', len(feature_sets))\n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def construct_features(examples, ling_data_cache, feature_model):\n",
    "    feature_sets = prepare_train_data(examples, ling_data_cache, feature_model)\n",
    "\n",
    "    data_for_pandas = []\n",
    "    for example in feature_sets:\n",
    "        data_for_pandas_ex = {}\n",
    "        data_for_pandas_ex['role'] = example[1]\n",
    "        data_for_pandas_ex['ex_id'] = example[2]\n",
    "        data_for_pandas_ex['tokens'] = example[3]\n",
    "        data_for_pandas_ex['arg_address'] = example[4]\n",
    "        data_for_pandas_ex['prd_address'] = example[5]\n",
    "        for elem in example[0]:\n",
    "            for subelem in elem:\n",
    "                if subelem is not None:\n",
    "                    data_for_pandas_ex.update(subelem)\n",
    "\n",
    "        data_for_pandas.append(data_for_pandas_ex)\n",
    "\n",
    "    return pd.DataFrame(data_for_pandas).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_preds = False  # Choose feature model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3942c8d7f1e475386969ac0747398bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of examples: 57552\n"
     ]
    }
   ],
   "source": [
    "if known_preds:\n",
    "    from isanlp_srl_framebank.processor_srl_framebank import FeatureModelDefault\n",
    "    feature_model = FeatureModelDefault()\n",
    "    main_model_path = os.path.join(main_model_path_root, 'known_preds')\n",
    "    pd_data = construct_features(examples, ling_data_cache, feature_model)\n",
    "\n",
    "else:\n",
    "    from isanlp_srl_framebank.processor_srl_framebank import FeatureModelUnknownPredicates\n",
    "    feature_model = FeatureModelUnknownPredicates()\n",
    "    main_model_path = os.path.join(main_model_path_root, 'unknown_preds')\n",
    "    pd_data = construct_features(examples, ling_data_cache, feature_model)\n",
    "    del pd_data['pred_lemma']\n",
    "\n",
    "with open(os.path.join(main_model_path, 'feature_model.pckl'), 'wb') as f:\n",
    "    pickle.dump(feature_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57552, 20)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Argument position: 7\n",
      "Argument lemma: контрабас_NOUN\n",
      "--\n",
      "Predicat position: 4\n",
      "Predicat lemma: None\n",
      "--\n",
      "Distance 3\n",
      "--\n",
      "Sentence tokens: [(0, 'Зубря'), (1, 'заклинания'), (2, ','), (3, 'Таня'), (4, 'извлекла'), (5, 'из'), (6, 'футляра'), (7, 'контрабас'), (8, ','), (9, 'села'), (10, 'на'), (11, 'него'), (12, 'и'), (13, 'взяла'), (14, 'в'), (15, 'руку'), (16, 'смычок'), (17, '.')]\n"
     ]
    }
   ],
   "source": [
    "N_verify = 1\n",
    "for i in np.random.choice(len(pd_data), size=N_verify):\n",
    "    print(\"-\"*60)\n",
    "    obj = pd_data.iloc[i]\n",
    "    print(f\"Argument position: {obj.arg_address}\")\n",
    "    print(f\"Argument lemma: {obj.arg_lemma}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Predicat position: {obj.prd_address}\")\n",
    "    print(f\"Predicat lemma: {obj.get('pred_lemma')}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Distance {int(obj.dist)}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Sentence tokens: {list(enumerate(obj.tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg',\n",
       "       'Valency_arg', 'VerbForm_arg', 'arg_address', 'arg_case', 'arg_lemma',\n",
       "       'arg_pos', 'dist', 'ex_id', 'prd_address', 'pred_pos', 'prepos',\n",
       "       'rel_pos', 'role', 'syn_link_name', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animacy_arg</th>\n",
       "      <th>Aspect_arg</th>\n",
       "      <th>Gender_arg</th>\n",
       "      <th>Number_arg</th>\n",
       "      <th>Tense_arg</th>\n",
       "      <th>Valency_arg</th>\n",
       "      <th>VerbForm_arg</th>\n",
       "      <th>arg_address</th>\n",
       "      <th>arg_case</th>\n",
       "      <th>arg_lemma</th>\n",
       "      <th>arg_pos</th>\n",
       "      <th>dist</th>\n",
       "      <th>ex_id</th>\n",
       "      <th>prd_address</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>prepos</th>\n",
       "      <th>rel_pos</th>\n",
       "      <th>role</th>\n",
       "      <th>syn_link_name</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8788</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Plur</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Nom</td>\n",
       "      <td>они_PRON</td>\n",
       "      <td>PRON</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40335</td>\n",
       "      <td>1</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>тема</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>[Они, появлялись, и, уходили, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Animacy_arg Aspect_arg Gender_arg Number_arg Tense_arg Valency_arg  \\\n",
       "8788                                         Plur                         \n",
       "\n",
       "     VerbForm_arg  arg_address arg_case arg_lemma arg_pos  dist  ex_id  \\\n",
       "8788                         0      Nom  они_PRON    PRON   1.0  40335   \n",
       "\n",
       "      prd_address pred_pos prepos  rel_pos  role syn_link_name  \\\n",
       "8788            1     VERB             1.0  тема         nsubj   \n",
       "\n",
       "                                tokens  \n",
       "8788  [Они, появлялись, и, уходили, .]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stat = pd_data.role.value_counts()\n",
    "drop_ys = y_stat[y_stat < 180].index\n",
    "pd_data = pd_data.drop(pd_data[pd_data.role.isin(drop_ys)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of roles:  44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "агенс                                 6147\n",
       "пациенс                               5362\n",
       "тема                                  3656\n",
       "субъект психологического состояния    3250\n",
       "субъект перемещения                   3011\n",
       "причина                               2502\n",
       "говорящий                             2365\n",
       "место                                 2185\n",
       "содержание действия                   1874\n",
       "содержание мысли                      1817\n",
       "содержание высказывания               1792\n",
       "конечная точка                        1772\n",
       "результат                             1452\n",
       "пациенс перемещения                   1356\n",
       "стимул                                1271\n",
       "субъект ментального состояния         1223\n",
       "адресат                                941\n",
       "субъект восприятия                     901\n",
       "контрагент                             831\n",
       "эффектор                               739\n",
       "субъект социального отношения          598\n",
       "начальная точка                        588\n",
       "предмет высказывания                   548\n",
       "способ                                 531\n",
       "конечный посессор                      506\n",
       "цель                                   454\n",
       "сфера                                  376\n",
       "признак                                366\n",
       "источник звука                         359\n",
       "субъект поведения                      339\n",
       "ситуация в фокусе                      322\n",
       "контрагент социального отношения       318\n",
       "субъект физиологической реакции        310\n",
       "предмет мысли                          303\n",
       "потенциальный пациенс                  290\n",
       "статус                                 265\n",
       "пациенс социального отношения          261\n",
       "срок                                   255\n",
       "эталон                                 255\n",
       "признак действия                       243\n",
       "каузатор                               223\n",
       "исходный посессор                      217\n",
       "потенциальная угроза                   197\n",
       "траектория                             180\n",
       "Name: role, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repl_roles = {\n",
    "    'агенс - субъект восприятия' : 'субъект восприятия',\n",
    "    'агенс - субъект ментального состояния' : 'субъект ментального состояния',\n",
    "    'результат / цель' : 'результат',\n",
    "    'место - пациенс' : 'место',\n",
    "    'говорящий - субъект психологического состояния' : 'субъект психологического состояния'\n",
    "}\n",
    "\n",
    "pd_data['role'] = pd_data['role'].replace(repl_roles)\n",
    "    \n",
    "number_of_roles = len(pd_data.role.unique())\n",
    "print('Number of roles: ', number_of_roles)\n",
    "pd_data.loc[:, 'role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52751, 19)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_orig = pd_data.loc[:, 'role']\n",
    "X_orig = pd_data.drop('role', axis = 1)\n",
    "X_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selector_pd = X_orig.ex_id.isin(train_ids)\n",
    "test_selector_pd = X_orig.ex_id.isin(test_ids)\n",
    "train_selector = train_selector_pd.values\n",
    "test_selector = test_selector_pd.values\n",
    "\n",
    "def select_from_nparray_list(nparray_list, selector):\n",
    "    return [e[selector] for e in nparray_list]\n",
    "\n",
    "X_train = select_from_nparray_list([X_orig], train_selector)[0]\n",
    "y_train = select_from_nparray_list([y_orig], train_selector)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = select_from_nparray_list([X_orig], test_selector)[0]\n",
    "y_test = select_from_nparray_list([y_orig], test_selector)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "with open(main_model_path + '/label_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_ommit = ['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category features:\n",
      " ['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg', 'Valency_arg', 'VerbForm_arg', 'arg_case', 'arg_lemma', 'arg_pos', 'dist', 'prd_address', 'pred_pos', 'prepos', 'syn_link_name']\n",
      "Not category features:\n",
      " ['rel_pos']\n",
      "(52751, 9880)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#morph_feats = ['pos', 'case', 'anim', 'vform', 'zform', 'shform', 'pform', 'vvform', 'nform', 'time']\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep', 'link_name'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "#all_feats = ['pred_lemma', 'rel_pos', 'pred_pos', 'arg_case', 'syn_link_name', 'arg_pos', 'prepos', 'dist']\n",
    "\n",
    "#categ_feats = [e for e in all_feats if X_orig[e].dtype in [str, object]]\n",
    "#not_categ = [e for e in all_feats if e not in categ_feats]\n",
    "\n",
    "#pred_lemma_vectorizer.fit_transform(X_orig.loc[:, ['pred_lemma']].to_dict(orient = 'records'))\n",
    "\n",
    "if not known_preds and 'pred_lemma' in X_train.keys():\n",
    "    X_train = X_train.drop(columns=['pred_lemma'])\n",
    "    \n",
    "not_categ_features = {'arg_address', 'ex_id', 'rel_pos'}\n",
    "\n",
    "categ_feats = [name for name in X_train.drop(columns=columns_to_ommit).columns if name not in not_categ_features] \n",
    "not_categ = ['rel_pos']\n",
    "print('Category features:\\n', categ_feats)\n",
    "print('Not category features:\\n', not_categ)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# X_train[categorical_cols] = X_train[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "# one_hot_feats = vectorizer.fit_transform(X_orig[categ_feats].to_dict(orient='records'))\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "vectorizer.fit(X_train[categ_feats].to_dict(orient='records'))\n",
    "one_hot_feats = vectorizer.transform(X_orig[categ_feats].to_dict(orient='records'))\n",
    "print(one_hot_feats.shape)\n",
    "\n",
    "with open(main_model_path + '/feature_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/labnpnpels.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52751, 9881)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_categ_columns = np.concatenate(tuple(X_orig.loc[:, e].values.reshape(-1, 1) for e in not_categ), axis =1)\n",
    "plain_features = np.concatenate((one_hot_feats, not_categ_columns), axis = 1)\n",
    "plain_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/plain_features.npy\", plain_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add embedding features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size:  300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings_path = '../../data/ruscorpora_upos_skipgram_300_5_2018.vec'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embeddings_path, binary=False)\n",
    "print('Embedding size: ', embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def make_embeded_form(word):\n",
    "    if word:\n",
    "        #return word[1].encode('utf8')\n",
    "        return u\"{}_{}\".format(word[1], word[0])\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "class Embedder_map:\n",
    "    def __init__(self, embeddings, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_ = embeddings\n",
    "\n",
    "    def __call__(self, i):  \n",
    "        result = np.zeros(embeddings.vector_size)\n",
    "        \n",
    "        ARG_SPECIAL_TAG = None  # ??\n",
    "\n",
    "        word = self.X_[i]\n",
    "        if embeddings.vocab.get(word):\n",
    "            return embeddings[word]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def embed(X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_map(embeddings, X), list(range(len(X))), 1000)\n",
    "    pool.close()\n",
    "#     embedder = Embedder_map(embeddings, X)\n",
    "#     result =[embedder(i) for i in range(2)]\n",
    "#     #result = [embedder(i) for i in range(len(X))]\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 3.6 s, total: 21.4 s\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "arg_embedded = embed(X_orig['arg_lemma'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 s, sys: 3.72 s, total: 21.8 s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_embedded = embed(X_orig['pred_lemma'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/w2v_verbs.npy\", pred_embedded)\n",
    "np.save(\"../../data/w2v_args.npy\", arg_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_embedded = np.load(\"../../data/w2v_verbs.npy\")\n",
    "arg_embedded = np.load(\"../../data/w2v_args.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.embedders.elmo_embedder import ELMoEmbedder\n",
    "\n",
    "elmo = ELMoEmbedder(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-wiki_600k_steps.tar.gz\", elmo_output_names=['elmo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_embed(embeddings, tokens, word_idx1, word_idx2):\n",
    "    embedded = embeddings([tokens])[0]\n",
    "    return embedded[min(word_idx1, len(tokens)-1)], embedded[min(word_idx2, len(tokens)-1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test object\n",
    "obj = X_orig.iloc[38]\n",
    "verb_idx = obj.prd_address\n",
    "arg_idx = obj.arg_address\n",
    "tokens = obj.tokens\n",
    "embed_verb, embed_arg = elmo_embed(elmo, tokens, verb_idx, arg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a7f50a051c4c6a89a6c0857dff0f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "embedded_verbs = []\n",
    "embedded_args  = []\n",
    "for i in tqdm(range(len(X_orig))):\n",
    "    try:\n",
    "        if i % 100 == 0:\n",
    "            with open(\"./log.txt\", 'a', encoding='utf-8') as log:\n",
    "                print(f\"Processed {i} examples\", file=log)\n",
    "        obj = X_orig.iloc[i]\n",
    "        verb_idx = obj.prd_address\n",
    "        arg_idx = obj.arg_address\n",
    "        tokens = obj.tokens\n",
    "        embed_verb, embed_arg = elmo_embed(elmo, tokens, verb_idx, arg_idx)\n",
    "        embedded_verbs.append(embed_verb)\n",
    "        embedded_args.append(embed_arg)\n",
    "    except Exception as e:\n",
    "        with open(\"./log.txt\", 'a', encoding='utf-8') as log:\n",
    "            print(f\"Error while processing example {i}={X_orig.iloc[i]}: {e}\", file=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52751, 1024) (52751, 1024)\n"
     ]
    }
   ],
   "source": [
    "e_verbs = np.stack(embedded_verbs)\n",
    "e_args  = np.stack(embedded_args)\n",
    "\n",
    "print(e_verbs.shape, e_args.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/elmo_verbs.npy\", e_verbs)\n",
    "np.save(\"../../data/elmo_args.npy\", e_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_verbs = np.load(\"../../data/elmo_verbs.npy\")\n",
    "e_args = np.load(\"../../data/elmo_args.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Convolution1D, Dropout, MaxPooling1D\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Permute\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Bidirectional\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_plain_model(input_shape):\n",
    "    print('Plain model.')\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(600, \n",
    "                          #input_shape=(plain_features.shape[1],), \n",
    "                          input_shape = input_shape,\n",
    "                          activation = 'relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(400))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(number_of_roles))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('softmax'))\n",
    "    \n",
    "    plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return plain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_plain_model_sparse(categ_size, emb_size, number_of_roles):    \n",
    "    input_plain = Input(shape=(categ_size,), name = 'input_categorical')\n",
    "    input_pred_embed = Input(shape=(emb_size,), name = 'pred_embed')\n",
    "    input_arg_embed = Input(shape=(emb_size,), name = 'arg_embed')\n",
    "    \n",
    "    plain = Dense(400)(input_plain)\n",
    "    plain = BatchNormalization()(plain)\n",
    "    plain = Activation('relu')(plain)\n",
    "    \n",
    "    def embed_submodel(inpt):\n",
    "        embed = Dense(100)(inpt)\n",
    "        embed = BatchNormalization()(embed)\n",
    "        embed = Activation('relu')(embed)\n",
    "        return embed\n",
    "    \n",
    "    embed_pred = embed_submodel(input_pred_embed)\n",
    "    embed_arg = embed_submodel(input_arg_embed)\n",
    "    \n",
    "    final = Concatenate(axis = 1)([embed_pred, embed_arg, plain])\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(400)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('relu')(final)\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(number_of_roles)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('softmax')(final)\n",
    "    \n",
    "    model = Model([input_arg_embed, input_pred_embed, input_plain], final)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_plain_model_sparse_unknown(categ_size, emb_size, number_of_roles):    \n",
    "    input_plain = Input(shape=(categ_size,), name = 'input_categorical')\n",
    "    input_pred_embed = Input(shape=(emb_size,), name = 'pred_embed')\n",
    "    input_arg_embed = Input(shape=(emb_size,), name = 'arg_embed')\n",
    "    \n",
    "    plain = Dense(400)(input_plain)\n",
    "    plain = BatchNormalization()(plain)\n",
    "    plain = Activation('relu')(plain)\n",
    "    \n",
    "    def embed_submodel(inpt, units):\n",
    "        embed = Dense(units)(inpt)\n",
    "        embed = BatchNormalization()(embed)\n",
    "        embed = Activation('relu')(embed)\n",
    "        return embed\n",
    "    \n",
    "    embed_pred = embed_submodel(input_pred_embed, 100)\n",
    "    embed_arg = embed_submodel(input_arg_embed, 400)\n",
    "    \n",
    "    final = Concatenate(axis = 1)([embed_pred, embed_arg, plain])\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(400)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('relu')(final)\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(number_of_roles)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('softmax')(final)\n",
    "    \n",
    "    model = Model([input_arg_embed, input_pred_embed, input_plain], final)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save the models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For known preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_from_nparray_list(nparray_list, selector):\n",
    "    return [np.array(e)[selector] for e in nparray_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pred_embed (InputLayer)         (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arg_embed (InputLayer)          (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_categorical (InputLayer)  (None, 10506)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_86 (Dense)                (None, 100)          102500      pred_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_87 (Dense)                (None, 400)          410000      arg_embed[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_85 (Dense)                (None, 400)          4202800     input_categorical[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 100)          400         dense_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 400)          1600        dense_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 400)          1600        dense_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 100)          0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 400)          0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 400)          0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 900)          0           activation_86[0][0]              \n",
      "                                                                 activation_87[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 900)          0           concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_88 (Dense)                (None, 44)           39644       dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 44)           176         dense_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 44)           0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 44)           0           activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_89 (Dense)                (None, 44)           1980        dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 44)           176         dense_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 44)           0           batch_normalization_89[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 4,760,876\n",
      "Trainable params: 4,758,900\n",
      "Non-trainable params: 1,976\n",
      "__________________________________________________________________________________________________\n",
      "Train on 37972 samples, validate on 4220 samples\n",
      "Epoch 1/10\n",
      "37972/37972 [==============================] - 9s 244us/step - loss: 2.6769 - acc: 0.3623 - val_loss: 1.8278 - val_acc: 0.6280\n",
      "Epoch 2/10\n",
      "37972/37972 [==============================] - 5s 135us/step - loss: 1.5286 - acc: 0.6501 - val_loss: 1.2616 - val_acc: 0.6993\n",
      "Epoch 3/10\n",
      "22848/37972 [=================>............] - ETA: 1s - loss: 1.1926 - acc: 0.7166"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-571-c7566c51e39c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m model.fit(select_from_nparray_list([e_args, e_verbs, plain_features], train_selector),\n\u001b[1;32m      7\u001b[0m           \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           epochs=10, batch_size=64, validation_split = 0.1, shuffle=True)\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.7/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'test_model_elmo.h5'\n",
    "VEC_SIZE = elmo.dim\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], VEC_SIZE, y_train.shape[1])\n",
    "model.summary()\n",
    "model.fit(select_from_nparray_list([e_args, e_verbs, plain_features], train_selector),\n",
    "          y_train, \n",
    "          epochs=10, batch_size=64, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(select_from_nparray_list([e_args, e_verbs, plain_features], test_selector), \n",
    "               y_test))\n",
    "model.save(os.path.join(main_model_path, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37972 samples, validate on 4220 samples\n",
      "Epoch 1/15\n",
      "37972/37972 [==============================] - 3s 71us/step - loss: 2.4442 - acc: 0.4598 - val_loss: 3.1179 - val_acc: 0.3787\n",
      "Epoch 2/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 1.3445 - acc: 0.7329 - val_loss: 2.4517 - val_acc: 0.6640\n",
      "Epoch 3/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 0.9798 - acc: 0.8031 - val_loss: 1.6519 - val_acc: 0.7436\n",
      "Epoch 4/15\n",
      "37972/37972 [==============================] - 2s 47us/step - loss: 0.7662 - acc: 0.8472 - val_loss: 1.1466 - val_acc: 0.7699\n",
      "Epoch 5/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 0.6235 - acc: 0.8725 - val_loss: 0.9580 - val_acc: 0.7725\n",
      "Epoch 6/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 0.5300 - acc: 0.8916 - val_loss: 0.9024 - val_acc: 0.7761\n",
      "Epoch 7/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 0.4532 - acc: 0.9044 - val_loss: 0.8759 - val_acc: 0.7787\n",
      "Epoch 8/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 0.4031 - acc: 0.9122 - val_loss: 0.8759 - val_acc: 0.7749\n",
      "Epoch 9/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 0.3650 - acc: 0.9196 - val_loss: 0.8620 - val_acc: 0.7723\n",
      "Epoch 10/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 0.3287 - acc: 0.9267 - val_loss: 0.8652 - val_acc: 0.7709\n",
      "Epoch 11/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 0.3059 - acc: 0.9283 - val_loss: 0.8702 - val_acc: 0.7680\n",
      "Epoch 12/15\n",
      "37972/37972 [==============================] - 2s 46us/step - loss: 0.2834 - acc: 0.9324 - val_loss: 0.8579 - val_acc: 0.7770\n",
      "Epoch 13/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 0.2624 - acc: 0.9368 - val_loss: 0.8943 - val_acc: 0.7604\n",
      "Epoch 14/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 0.2459 - acc: 0.9408 - val_loss: 0.8552 - val_acc: 0.7678\n",
      "Epoch 15/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 0.2248 - acc: 0.9446 - val_loss: 0.8677 - val_acc: 0.7635\n",
      "['loss', 'acc']\n",
      "10559/10559 [==============================] - 1s 98us/step\n",
      "[0.8678278180672054, 0.7692963348914869]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'test_model_w2v.h5'\n",
    "VEC_SIZE = embeddings.vector_size\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], embeddings.vector_size, y_train.shape[1])\n",
    "model.fit(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], train_selector),\n",
    "          y_train, \n",
    "          epochs=15, batch_size=300, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], test_selector), \n",
    "               y_test))\n",
    "model.save(os.path.join(main_model_path, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For unknown preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/models_new/unknown_preds'"
      ]
     },
     "execution_count": 604,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pred_embed (InputLayer)         (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arg_embed (InputLayer)          (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_categorical (InputLayer)  (None, 9881)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          102500      pred_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          102500      arg_embed[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 400)          3952800     input_categorical[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100)          400         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 400)          1600        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 100)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 400)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 600)          0           activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 600)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 400)          240400      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 400)          1600        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 400)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 400)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 44)           17644       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 44)           176         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 44)           0           batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 4,420,020\n",
      "Trainable params: 4,417,932\n",
      "Non-trainable params: 2,088\n",
      "__________________________________________________________________________________________________\n",
      "Train on 37972 samples, validate on 4220 samples\n",
      "Epoch 1/10\n",
      "37972/37972 [==============================] - 8s 209us/step - loss: 2.1259 - acc: 0.5035 - val_loss: 1.3532 - val_acc: 0.6865\n",
      "Epoch 2/10\n",
      "37972/37972 [==============================] - 6s 167us/step - loss: 1.3132 - acc: 0.6802 - val_loss: 1.0413 - val_acc: 0.7403\n",
      "Epoch 3/10\n",
      "37972/37972 [==============================] - 7s 181us/step - loss: 1.0402 - acc: 0.7331 - val_loss: 0.9211 - val_acc: 0.7585\n",
      "Epoch 4/10\n",
      "37972/37972 [==============================] - 6s 168us/step - loss: 0.8674 - acc: 0.7704 - val_loss: 0.8705 - val_acc: 0.7659\n",
      "Epoch 5/10\n",
      "37972/37972 [==============================] - 7s 187us/step - loss: 0.7430 - acc: 0.8017 - val_loss: 0.8853 - val_acc: 0.7621\n",
      "Epoch 6/10\n",
      "37972/37972 [==============================] - 7s 173us/step - loss: 0.6479 - acc: 0.8236 - val_loss: 0.8559 - val_acc: 0.7647\n",
      "Epoch 7/10\n",
      "37972/37972 [==============================] - 6s 154us/step - loss: 0.5718 - acc: 0.8423 - val_loss: 0.8499 - val_acc: 0.7656\n",
      "Epoch 8/10\n",
      "37972/37972 [==============================] - 6s 145us/step - loss: 0.5140 - acc: 0.8577 - val_loss: 0.8476 - val_acc: 0.7694\n",
      "Epoch 9/10\n",
      "37972/37972 [==============================] - 6s 165us/step - loss: 0.4714 - acc: 0.8680 - val_loss: 0.8443 - val_acc: 0.7763\n",
      "Epoch 10/10\n",
      "37972/37972 [==============================] - 6s 163us/step - loss: 0.4388 - acc: 0.8755 - val_loss: 0.8644 - val_acc: 0.7706\n",
      "['loss', 'acc']\n",
      "10559/10559 [==============================] - 1s 96us/step\n",
      "[0.8849361567506737, 0.7689175111505274]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'test_model_elmo.h5'\n",
    "VEC_SIZE = 1024 #elmo.dim\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], VEC_SIZE, y_train.shape[1])\n",
    "model.summary()\n",
    "model.fit(select_from_nparray_list([e_args, e_verbs, plain_features], train_selector),\n",
    "          y_train, \n",
    "          epochs=10, batch_size=64, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(select_from_nparray_list([e_args, e_verbs, plain_features], test_selector), \n",
    "               y_test))\n",
    "model.save(os.path.join(main_model_path, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pred_embed (InputLayer)         (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arg_embed (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_categorical (InputLayer)  (None, 9881)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 100)          30100       pred_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          30100       arg_embed[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 400)          3952800     input_categorical[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 100)          400         dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 100)          400         dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 400)          1600        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 100)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 100)          0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 400)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 600)          0           activation_6[0][0]               \n",
      "                                                                 activation_7[0][0]               \n",
      "                                                                 activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 600)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 400)          240400      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 400)          1600        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 400)          0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 400)          0           activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 44)           17644       dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 44)           176         dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 44)           0           batch_normalization_9[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 4,275,220\n",
      "Trainable params: 4,273,132\n",
      "Non-trainable params: 2,088\n",
      "__________________________________________________________________________________________________\n",
      "Train on 37972 samples, validate on 4220 samples\n",
      "Epoch 1/15\n",
      "37972/37972 [==============================] - 3s 67us/step - loss: 2.5853 - acc: 0.4045 - val_loss: 3.1060 - val_acc: 0.5464\n",
      "Epoch 2/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 1.6311 - acc: 0.6596 - val_loss: 2.4832 - val_acc: 0.6242\n",
      "Epoch 3/15\n",
      "37972/37972 [==============================] - 2s 43us/step - loss: 1.2820 - acc: 0.7271 - val_loss: 1.7941 - val_acc: 0.7076\n",
      "Epoch 4/15\n",
      "37972/37972 [==============================] - 2s 42us/step - loss: 1.0547 - acc: 0.7732 - val_loss: 1.3059 - val_acc: 0.7306\n",
      "Epoch 5/15\n",
      "37972/37972 [==============================] - 2s 43us/step - loss: 0.8915 - acc: 0.8035 - val_loss: 1.1064 - val_acc: 0.7391\n",
      "Epoch 6/15\n",
      "37972/37972 [==============================] - 2s 43us/step - loss: 0.7730 - acc: 0.8279 - val_loss: 1.0342 - val_acc: 0.7450\n",
      "Epoch 7/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 0.6832 - acc: 0.8443 - val_loss: 0.9968 - val_acc: 0.7476\n",
      "Epoch 8/15\n",
      "37972/37972 [==============================] - 2s 43us/step - loss: 0.6123 - acc: 0.8584 - val_loss: 0.9714 - val_acc: 0.7457\n",
      "Epoch 9/15\n",
      "37972/37972 [==============================] - 2s 42us/step - loss: 0.5638 - acc: 0.8661 - val_loss: 0.9533 - val_acc: 0.7491\n",
      "Epoch 10/15\n",
      "37972/37972 [==============================] - 2s 45us/step - loss: 0.5260 - acc: 0.8738 - val_loss: 0.9614 - val_acc: 0.7460\n",
      "Epoch 11/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 0.4843 - acc: 0.8821 - val_loss: 0.9418 - val_acc: 0.7481\n",
      "Epoch 12/15\n",
      "37972/37972 [==============================] - 2s 43us/step - loss: 0.4584 - acc: 0.8870 - val_loss: 0.9401 - val_acc: 0.7498\n",
      "Epoch 13/15\n",
      "37972/37972 [==============================] - 2s 43us/step - loss: 0.4374 - acc: 0.8893 - val_loss: 0.9293 - val_acc: 0.7488\n",
      "Epoch 14/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 0.4091 - acc: 0.8950 - val_loss: 0.9259 - val_acc: 0.7559\n",
      "Epoch 15/15\n",
      "37972/37972 [==============================] - 2s 44us/step - loss: 0.3951 - acc: 0.8969 - val_loss: 0.9316 - val_acc: 0.7500\n",
      "['loss', 'acc']\n",
      "10559/10559 [==============================] - 1s 82us/step\n",
      "[0.9428068779162673, 0.7461880860042815]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'test_model_w2v.h5'\n",
    "VEC_SIZE = embeddings.vector_size\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], VEC_SIZE, y_train.shape[1])\n",
    "model.summary()\n",
    "model.fit(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], train_selector),\n",
    "          y_train, \n",
    "          epochs=15, batch_size=300, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], test_selector), \n",
    "               y_test))\n",
    "model.save(os.path.join(main_model_path, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate latex table with per-role performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map roles as given in https://github.com/olesar/framebank/blob/master/framebank_roles_ru_eng.md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = {\n",
    "    'avg': 'avg',\n",
    "    'предмет мысли': 'topic of thought',\n",
    "    'результат': 'result',\n",
    "    'потенциальная угроза': 'potential threat',\n",
    "    'контрагент': 'counteragent',\n",
    "    'агенс': 'agent',\n",
    "    'каузатор': 'causer',\n",
    "    'пациенс': 'patient',\n",
    "    'ситуация в фокусе': 'situation in focus',\n",
    "    'конечный посессор': 'recipient',\n",
    "    'тема': 'theme',\n",
    "    'эффектор': 'effector',\n",
    "    'способ': 'manner',\n",
    "    'сфера': 'field',\n",
    "    'траектория': 'path',\n",
    "    'цель': 'goal',\n",
    "    'признак': 'attribute',\n",
    "    'субъект социального отношения': 'subject of social attitude',\n",
    "    'пациенс социального отношения': 'patient of social attitude',\n",
    "    'субъект поведения': 'behaver',\n",
    "    'статус': 'status',\n",
    "    'исходный посессор': 'initial possessor',\n",
    "    'контрагент социального отношения': 'counteragent of social attitude',\n",
    "    'потенциальный пациенс': 'potential patient',\n",
    "    'пациенс перемещения': 'patient of motion',\n",
    "    'содержание мысли': 'content of thought',\n",
    "    'содержание действия': 'content of action',\n",
    "    'субъект ментального состояния': 'cognizer',\n",
    "    'стимул': 'stimulus',\n",
    "    'признак действия': 'attribute of action',\n",
    "    'эталон': 'standard',\n",
    "    'субъект психологического состояния': 'sbj of psychol. state',\n",
    "    'срок': 'term',\n",
    "    'субъект перемещения': 'goer',\n",
    "    'говорящий': 'speaker',\n",
    "    'конечная точка': 'final destination',\n",
    "    'причина': 'cause',\n",
    "    'источник звука': 'source of sound',\n",
    "    'предмет высказывания': 'topic of speech',\n",
    "    'адресат': 'addressee',\n",
    "    'место': 'location',\n",
    "    'субъект восприятия': 'perceiver',\n",
    "    'субъект физиологической реакции': 'sbj of physiol. reaction',\n",
    "    'начальная точка': 'initial point',\n",
    "    'содержание высказывания': 'content of speech'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers of examples per each role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_counts = dict(pd_data.loc[:, 'role'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate report table with sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], test_selector))\n",
    "report = classification_report(label_encoder.inverse_transform(y_test), label_encoder.inverse_transform(y_pred), digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate latex table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l|c|c|c}\n",
      "\\hline\n",
      "\\bf Class & \\bf Precision & \\bf Recall & \\bf F-score \\\\\n",
      "\\hline\n",
      "agent (11.7\\%) & 76.7 & 71.7 & 74.1\\\\\n",
      "patient (10.2\\%) & 67.7 & 78.0 & 72.5\\\\\n",
      "theme (6.9\\%) & 71.1 & 73.2 & 72.2\\\\\n",
      "sbj of psychol. state (6.2\\%) & 86.5 & 88.3 & 87.4\\\\\n",
      "goer (5.7\\%) & 78.3 & 87.7 & 82.7\\\\\n",
      "cause (4.7\\%) & 79.1 & 82.8 & 80.9\\\\\n",
      "speaker (4.5\\%) & 77.8 & 83.1 & 80.3\\\\\n",
      "location (4.1\\%) & 79.1 & 74.8 & 76.9\\\\\n",
      "content of action (3.6\\%) & 78.8 & 72.9 & 75.7\\\\\n",
      "content of thought (3.4\\%) & 63.0 & 78.6 & 70.0\\\\\n",
      "content of speech (3.4\\%) & 79.8 & 68.7 & 73.9\\\\\n",
      "final destination (3.4\\%) & 78.5 & 86.7 & 82.4\\\\\n",
      "result (2.8\\%) & 71.3 & 63.1 & 66.9\\\\\n",
      "patient of motion (2.6\\%) & 68.5 & 71.7 & 70.1\\\\\n",
      "stimulus (2.4\\%) & 75.8 & 67.0 & 71.1\\\\\n",
      "cognizer (2.3\\%) & 76.2 & 65.5 & 70.5\\\\\n",
      "addressee (1.8\\%) & 66.5 & 79.0 & 72.2\\\\\n",
      "perceiver (1.7\\%) & 79.5 & 82.8 & 81.1\\\\\n",
      "counteragent (1.6\\%) & 71.2 & 72.2 & 71.7\\\\\n",
      "effector (1.4\\%) & 55.9 & 54.0 & 55.0\\\\\n",
      "subject of social attitude (1.1\\%) & 70.6 & 63.2 & 66.7\\\\\n",
      "initial point (1.1\\%) & 83.7 & 77.4 & 80.5\\\\\n",
      "topic of speech (1.0\\%) & 73.6 & 76.1 & 74.8\\\\\n",
      "manner (1.0\\%) & 67.3 & 33.7 & 44.9\\\\\n",
      "recipient (1.0\\%) & 66.7 & 76.2 & 71.1\\\\\n",
      "goal (0.9\\%) & 78.8 & 68.4 & 73.2\\\\\n",
      "field (0.7\\%) & 55.6 & 43.5 & 48.8\\\\\n",
      "attribute (0.7\\%) & 73.8 & 57.0 & 64.3\\\\\n",
      "source of sound (0.7\\%) & 79.4 & 83.1 & 81.2\\\\\n",
      "behaver (0.6\\%) & 65.8 & 64.0 & 64.9\\\\\n",
      "situation in focus (0.6\\%) & 76.7 & 35.4 & 48.4\\\\\n",
      "counteragent of social attitude (0.6\\%) & 68.2 & 64.2 & 66.1\\\\\n",
      "sbj of physiol. reaction (0.6\\%) & 94.8 & 78.6 & 85.9\\\\\n",
      "topic of thought (0.6\\%) & 77.3 & 65.4 & 70.8\\\\\n",
      "potential patient (0.5\\%) & 69.2 & 61.0 & 64.9\\\\\n",
      "status (0.5\\%) & 86.4 & 38.8 & 53.5\\\\\n",
      "patient of social attitude (0.5\\%) & 56.0 & 47.5 & 51.4\\\\\n",
      "term (0.5\\%) & 86.0 & 89.1 & 87.5\\\\\n",
      "standard (0.5\\%) & 88.2 & 84.9 & 86.5\\\\\n",
      "attribute of action (0.5\\%) & 75.5 & 71.2 & 73.3\\\\\n",
      "causer (0.4\\%) & 71.0 & 44.0 & 54.3\\\\\n",
      "initial possessor (0.4\\%) & 72.5 & 63.0 & 67.4\\\\\n",
      "potential threat (0.4\\%) & 88.2 & 71.4 & 79.0\\\\\n",
      "path (0.3\\%) & 51.5 & 53.1 & 52.3\\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def parse_classification_report(clfreport):\n",
    "    \"\"\"\n",
    "    Parse a sklearn classification report into a dict keyed by class name\n",
    "    and containing a tuple (precision, recall, fscore, support) for each class\n",
    "    \"\"\"\n",
    "    lines = clfreport.split('\\n')\n",
    "    # Remove empty lines\n",
    "    lines = list(filter(lambda l: not len(l.strip()) == 0, lines))\n",
    "\n",
    "    # Starts with a header, then score for each class and finally an average\n",
    "    header = lines[0]\n",
    "    cls_lines = lines[1:-3]\n",
    "    avg_line = lines[-1]\n",
    "    #print(avg_line)\n",
    "\n",
    "    assert header.split() == ['precision', 'recall', 'f1-score', 'support']\n",
    "    #assert avg_line.split()[1] == 'avg'\n",
    "\n",
    "    # We cannot simply use split because class names can have spaces. So instead\n",
    "    # figure the width of the class field by looking at the indentation of the\n",
    "    # precision header\n",
    "    cls_field_width = len(header) - len(header.lstrip())\n",
    "    # Now, collect all the class names and score in a dict\n",
    "    def parse_line(l):\n",
    "        \"\"\"Parse a line of classification_report\"\"\"\n",
    "        cls_name = l[:cls_field_width].strip() \n",
    "        precision, recall, fscore, support = l[cls_field_width:].split()\n",
    "        precision = float(precision)\n",
    "        recall = float(recall)\n",
    "        fscore = float(fscore)\n",
    "        support = roles_counts[cls_name]/len(pd_data)\n",
    "        return (cls_name, precision, recall, fscore, support)\n",
    "\n",
    "    data = collections.OrderedDict()\n",
    "    for l in cls_lines:\n",
    "        ret = parse_line(l)\n",
    "        cls_name = ret[0]\n",
    "        scores = [score * 100. for score in ret[1:]]\n",
    "        data[cls_name] = scores\n",
    "        #print(f'data[{cls_name}] = {scores}')\n",
    "    \n",
    "    # Apply sort by column\n",
    "    # Column#2 - F1, Column#3 - quantity\n",
    "    listofTuples = sorted(data.items(), key=lambda x: x[1][-1], reverse=True)\n",
    "    _data = collections.OrderedDict()\n",
    " \n",
    "    for elem in listofTuples:\n",
    "        if elem[0] != 'avg':\n",
    "            _data[elem[0]] = elem[1]\n",
    "\n",
    "    # average\n",
    "    # data['avg'] = parse_line(avg_line)[1:]\n",
    "\n",
    "    return _data\n",
    "\n",
    "def report_to_latex_table(data, percentage=True):\n",
    "    out = \"\"\n",
    "    out += \"\\\\begin{tabular}{l|c|c|c}\\n\"\n",
    "    out += \"\\hline\\n\"\n",
    "    out += \"\\\\bf Class & \\\\bf Precision & \\\\bf Recall & \\\\bf F-score \\\\\\\\\\n\"\n",
    "    out += \"\\hline\\n\"\n",
    "    for cls, scores in data.items():\n",
    "        scores = scores[:-1]\n",
    "        if percentage:\n",
    "            out += en_vocab[cls] + f\" ({round(roles_counts[cls]/len(pd_data)*100, 1)}\\%)\" + \" & \" + \" & \".join([str(round(s, 1)) for s in scores])\n",
    "        else:\n",
    "            out += en_vocab[cls] + \" & \" + \" & \".join([str(round(s, 1)) for s in scores])\n",
    "        out += \"\\\\\\\\\\n\"\n",
    "    out += \"\\\\end{tabular}\"\n",
    "    return out\n",
    "\n",
    "data = parse_classification_report(report)\n",
    "print(report_to_latex_table(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
