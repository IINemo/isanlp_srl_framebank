{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use only one GPU\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../isanlp/src/')\n",
    "sys.path.append('../../src/isanlp_srl_framebank/')\n",
    "sys.path.append('../../libs/')\n",
    "sys.path.append('../../libs/pylingtools/')\n",
    "\n",
    "# Supress tensorflow memory appetites\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "config.log_device_placement=True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "K.set_session(sess)\n",
    "\n",
    "# Check available GPUs\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data and make the train/test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import isanlp\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleared_corpus_path = '../../data/cleared_corpus.json'\n",
    "\n",
    "with open(cleared_corpus_path, 'r') as f:\n",
    "    examples = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ling_data_path = '../../data/results_final_fixed.pckl'\n",
    "with open(ling_data_path, 'rb') as f:\n",
    "    ling_data_cache = pickle.load(f)\n",
    "\n",
    "ling_data_cache = {k: v for k,v in ling_data_cache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ids, test_ids = train_test_split(list(ling_data_cache.keys()), test_size=0.2, random_state=42)\n",
    "train_ids = list(set(train_ids))\n",
    "test_ids = list(set(test_ids))\n",
    "\n",
    "data_path = '../../data/'\n",
    "main_model_path_root = '../../data/models_new/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [example for example in examples if example[0] in train_ids]\n",
    "test_data = [example for example in examples if example[0] in test_ids]\n",
    "\n",
    "with open(os.path.join(data_path, 'train_data.json'), 'w') as f:\n",
    "    json.dump(train_data, f)\n",
    "\n",
    "with open(os.path.join(data_path, 'test_data.json'), 'w') as f:\n",
    "    json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../../isanlp/src/')\n",
    "sys.path.append('../../src/isanlp_srl_framebank/')\n",
    "sys.path.append('../../libs/')\n",
    "sys.path.append('../../libs/pylingtools/')\n",
    "\n",
    "\n",
    "from isanlp.annotation_repr import CSentence\n",
    "from convert_corpus_to_brat import make_text\n",
    "\n",
    "\n",
    "def find_address_by_offset(offset, ling_ann):\n",
    "    for tok_num, tok in enumerate(ling_ann['tokens']):\n",
    "        if tok.begin <= offset and offset < tok.end:\n",
    "            break\n",
    "    \n",
    "    for sent_num, sent in enumerate(ling_ann['sentences']):\n",
    "        if sent.begin <= tok_num and tok_num < sent.end:\n",
    "            break\n",
    "    \n",
    "    return sent_num, tok_num - sent.begin\n",
    "\n",
    "\n",
    "error_examples = {}\n",
    "\n",
    "def process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, example):\n",
    "    feature_sets = list()\n",
    "    \n",
    "    text, offset_index = make_text(example, 0)\n",
    "    ling_ann = ling_cache[ex_id]\n",
    "    \n",
    "    pred_offset = offset_index[(pred[0], pred[1])]\n",
    "    pred_ling_sent, pred_ling_word = find_address_by_offset(pred_offset, ling_ann)\n",
    "    \n",
    "    for arg in args:\n",
    "        arg_offset = offset_index[(arg[0], arg[1])]\n",
    "        arg_ling_sent, arg_ling_word = find_address_by_offset(arg_offset, ling_ann)\n",
    "        \n",
    "        fb_pred_word = example[pred[0]][pred[1]]\n",
    "        fb_arg_word = example[arg[0]][arg[1]]\n",
    "        \n",
    "        if arg_ling_sent != pred_ling_sent:\n",
    "            error_examples[ex_id] = {\n",
    "                'reason': 'sent_mismatch',\n",
    "                'arg': arg_ling_sent,\n",
    "                'pred': pred_ling_sent\n",
    "            }\n",
    "            continue\n",
    "            \n",
    "        sentence = ling_ann['sentences'][pred_ling_sent]\n",
    "        tokens = [tok.text for tok in ling_ann['tokens']]\n",
    "        tokens = tokens[sentence.begin:sentence.end]\n",
    "        \n",
    "        role = fb_arg_word['rolepred1']\n",
    "\n",
    "        features = feature_extractor.extract_features(pred_ling_word, \n",
    "                                                      arg_ling_word, \n",
    "                                                      ling_ann['postag'][arg_ling_sent],\n",
    "                                                      ling_ann['morph'][arg_ling_sent],\n",
    "                                                      ling_ann['lemma'][arg_ling_sent],\n",
    "                                                      ling_ann['syntax_dep_tree'][arg_ling_sent])\n",
    "\n",
    "                    \n",
    "        feature_sets.append((features, role, ex_id, tokens, arg_ling_word, pred_ling_word))\n",
    "    \n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def process_example(feature_extractor, ling_cache, ex_id, sentences):\n",
    "    pred = None\n",
    "    args = list()\n",
    "    for sent_num, sent in enumerate(sentences):\n",
    "        for word_num, word in enumerate(sent):\n",
    "            if 'rank' in word and word['rank'] == 'Предикат':\n",
    "                pred = (sent_num, word_num)\n",
    "            elif 'rolepred1' in word:\n",
    "                args.append((sent_num, word_num))\n",
    "    \n",
    "    return process_arg_pred(feature_extractor, ling_cache, ex_id, pred, args, sentences)\n",
    "\n",
    "\n",
    "num_of_errors = 0\n",
    "def prepare_train_data(examples, ling_data_cache, feature_extractor):\n",
    "    feature_sets = []\n",
    "    for ex_num, (ex_id, ex) in tqdm(list(enumerate(examples))):                \n",
    "        feature_sets += process_example(feature_extractor, ling_data_cache, ex_id, ex)\n",
    "\n",
    "    print('Number of examples:', len(feature_sets))\n",
    "    return feature_sets\n",
    "\n",
    "\n",
    "def construct_features(examples, ling_data_cache, feature_model):\n",
    "    feature_sets = prepare_train_data(examples, ling_data_cache, feature_model)\n",
    "\n",
    "    data_for_pandas = []\n",
    "    for example in feature_sets:\n",
    "        data_for_pandas_ex = {}\n",
    "        data_for_pandas_ex['role'] = example[1]\n",
    "        data_for_pandas_ex['ex_id'] = example[2]\n",
    "        data_for_pandas_ex['tokens'] = example[3]\n",
    "        data_for_pandas_ex['arg_address'] = example[4]\n",
    "        data_for_pandas_ex['prd_address'] = example[5]\n",
    "        for elem in example[0]:\n",
    "            for subelem in elem:\n",
    "                if subelem is not None:\n",
    "                    data_for_pandas_ex.update(subelem)\n",
    "\n",
    "        data_for_pandas.append(data_for_pandas_ex)\n",
    "\n",
    "    return pd.DataFrame(data_for_pandas).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_preds = True  # Choose feature model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aee7f22b7784d49ba998f746271195f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32612), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of examples: 57552\n"
     ]
    }
   ],
   "source": [
    "if known_preds:\n",
    "    from isanlp_srl_framebank.processor_srl_framebank import FeatureModelDefault\n",
    "    feature_model = FeatureModelDefault()\n",
    "    main_model_path = os.path.join(main_model_path_root, 'known_preds')\n",
    "    pd_data = construct_features(examples, ling_data_cache, feature_model)\n",
    "\n",
    "else:\n",
    "    from isanlp_srl_framebank.processor_srl_framebank import FeatureModelUnknownPredicates\n",
    "    feature_model = FeatureModelUnknownPredicates()\n",
    "    main_model_path = os.path.join(main_model_path_root, 'unknown_preds')\n",
    "    pd_data = construct_features(examples, ling_data_cache, feature_model)\n",
    "    del pd_data['pred_lemma']\n",
    "\n",
    "with open(os.path.join(main_model_path, 'feature_model.pckl'), 'wb') as f:\n",
    "    pickle.dump(feature_model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57552, 21)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Argument position: 7\n",
      "Argument lemma: контрабас_NOUN\n",
      "--\n",
      "Predicat position: 4\n",
      "Predicat lemma: извлекать_VERB\n",
      "--\n",
      "Distance 3\n",
      "--\n",
      "Sentence tokens: [(0, 'Зубря'), (1, 'заклинания'), (2, ','), (3, 'Таня'), (4, 'извлекла'), (5, 'из'), (6, 'футляра'), (7, 'контрабас'), (8, ','), (9, 'села'), (10, 'на'), (11, 'него'), (12, 'и'), (13, 'взяла'), (14, 'в'), (15, 'руку'), (16, 'смычок'), (17, '.')]\n"
     ]
    }
   ],
   "source": [
    "N_verify = 1\n",
    "for i in np.random.choice(len(pd_data), size=N_verify):\n",
    "    print(\"-\"*60)\n",
    "    obj = pd_data.iloc[i]\n",
    "    print(f\"Argument position: {obj.arg_address}\")\n",
    "    print(f\"Argument lemma: {obj.arg_lemma}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Predicat position: {obj.prd_address}\")\n",
    "    print(f\"Predicat lemma: {obj.get('pred_lemma')}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Distance {int(obj.dist)}\")\n",
    "    print(\"--\")\n",
    "    print(f\"Sentence tokens: {list(enumerate(obj.tokens))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg',\n",
       "       'Valency_arg', 'VerbForm_arg', 'arg_address', 'arg_case', 'arg_lemma',\n",
       "       'arg_pos', 'dist', 'ex_id', 'prd_address', 'pred_lemma', 'pred_pos',\n",
       "       'prepos', 'rel_pos', 'role', 'syn_link_name', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animacy_arg</th>\n",
       "      <th>Aspect_arg</th>\n",
       "      <th>Gender_arg</th>\n",
       "      <th>Number_arg</th>\n",
       "      <th>Tense_arg</th>\n",
       "      <th>Valency_arg</th>\n",
       "      <th>VerbForm_arg</th>\n",
       "      <th>arg_address</th>\n",
       "      <th>arg_case</th>\n",
       "      <th>arg_lemma</th>\n",
       "      <th>...</th>\n",
       "      <th>dist</th>\n",
       "      <th>ex_id</th>\n",
       "      <th>prd_address</th>\n",
       "      <th>pred_lemma</th>\n",
       "      <th>pred_pos</th>\n",
       "      <th>prepos</th>\n",
       "      <th>rel_pos</th>\n",
       "      <th>role</th>\n",
       "      <th>syn_link_name</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8788</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Plur</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>Nom</td>\n",
       "      <td>они_PRON</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40335</td>\n",
       "      <td>1</td>\n",
       "      <td>появляться_VERB</td>\n",
       "      <td>VERB</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td>тема</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>[Они, появлялись, и, уходили, .]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Animacy_arg Aspect_arg Gender_arg Number_arg Tense_arg Valency_arg  \\\n",
       "8788                                         Plur                         \n",
       "\n",
       "     VerbForm_arg  arg_address arg_case arg_lemma  \\\n",
       "8788                         0      Nom  они_PRON   \n",
       "\n",
       "                    ...                dist  ex_id prd_address  \\\n",
       "8788                ...                 1.0  40335           1   \n",
       "\n",
       "           pred_lemma pred_pos prepos rel_pos  role syn_link_name  \\\n",
       "8788  появляться_VERB     VERB            1.0  тема         nsubj   \n",
       "\n",
       "                                tokens  \n",
       "8788  [Они, появлялись, и, уходили, .]  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_stat = pd_data.role.value_counts()\n",
    "drop_ys = y_stat[y_stat < 180].index\n",
    "pd_data = pd_data.drop(pd_data[pd_data.role.isin(drop_ys)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of roles:  44\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "агенс                                 6147\n",
       "пациенс                               5362\n",
       "тема                                  3656\n",
       "субъект психологического состояния    3250\n",
       "субъект перемещения                   3011\n",
       "причина                               2502\n",
       "говорящий                             2365\n",
       "место                                 2185\n",
       "содержание действия                   1874\n",
       "содержание мысли                      1817\n",
       "содержание высказывания               1792\n",
       "конечная точка                        1772\n",
       "результат                             1452\n",
       "пациенс перемещения                   1356\n",
       "стимул                                1271\n",
       "субъект ментального состояния         1223\n",
       "адресат                                941\n",
       "субъект восприятия                     901\n",
       "контрагент                             831\n",
       "эффектор                               739\n",
       "субъект социального отношения          598\n",
       "начальная точка                        588\n",
       "предмет высказывания                   548\n",
       "способ                                 531\n",
       "конечный посессор                      506\n",
       "цель                                   454\n",
       "сфера                                  376\n",
       "признак                                366\n",
       "источник звука                         359\n",
       "субъект поведения                      339\n",
       "ситуация в фокусе                      322\n",
       "контрагент социального отношения       318\n",
       "субъект физиологической реакции        310\n",
       "предмет мысли                          303\n",
       "потенциальный пациенс                  290\n",
       "статус                                 265\n",
       "пациенс социального отношения          261\n",
       "эталон                                 255\n",
       "срок                                   255\n",
       "признак действия                       243\n",
       "каузатор                               223\n",
       "исходный посессор                      217\n",
       "потенциальная угроза                   197\n",
       "траектория                             180\n",
       "Name: role, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repl_roles = {\n",
    "    'агенс - субъект восприятия' : 'субъект восприятия',\n",
    "    'агенс - субъект ментального состояния' : 'субъект ментального состояния',\n",
    "    'результат / цель' : 'результат',\n",
    "    'место - пациенс' : 'место',\n",
    "    'говорящий - субъект психологического состояния' : 'субъект психологического состояния'\n",
    "}\n",
    "\n",
    "pd_data['role'] = pd_data['role'].replace(repl_roles)\n",
    "    \n",
    "number_of_roles = len(pd_data.role.unique())\n",
    "print('Number of roles: ', number_of_roles)\n",
    "pd_data.loc[:, 'role'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52751, 20)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_orig = pd_data.loc[:, 'role']\n",
    "X_orig = pd_data.drop('role', axis = 1)\n",
    "X_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_selector_pd = X_orig.ex_id.isin(train_ids)\n",
    "test_selector_pd = X_orig.ex_id.isin(test_ids)\n",
    "train_selector = train_selector_pd.values\n",
    "test_selector = test_selector_pd.values\n",
    "\n",
    "def select_from_nparray_list(nparray_list, selector):\n",
    "    return [e[selector] for e in nparray_list]\n",
    "\n",
    "X_train = select_from_nparray_list([X_orig], train_selector)[0]\n",
    "y_train = select_from_nparray_list([y_orig], train_selector)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = select_from_nparray_list([X_orig], test_selector)[0]\n",
    "y_test = select_from_nparray_list([y_orig], test_selector)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pickle\n",
    "\n",
    "label_encoder = LabelBinarizer()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "\n",
    "with open(main_model_path + '/label_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_ommit = ['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category features:\n",
      " ['Animacy_arg', 'Aspect_arg', 'Gender_arg', 'Number_arg', 'Tense_arg', 'Valency_arg', 'VerbForm_arg', 'arg_case', 'arg_pos', 'dist', 'prd_address', 'pred_lemma', 'pred_pos', 'prepos', 'syn_link_name']\n",
      "Not category features:\n",
      " ['rel_pos']\n",
      "(52751, 802)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#morph_feats = ['pos', 'case', 'anim', 'vform', 'zform', 'shform', 'pform', 'vvform', 'nform', 'time']\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "# all_feats = (['pred_lemma', 'rel_pos', 'arg_prep', 'link_name'] + \n",
    "#              ['arg_' + e for e in morph_feats] + \n",
    "#              ['pred_' + e for e in morph_feats])\n",
    "\n",
    "#all_feats = ['pred_lemma', 'rel_pos', 'pred_pos', 'arg_case', 'syn_link_name', 'arg_pos', 'prepos', 'dist']\n",
    "\n",
    "#categ_feats = [e for e in all_feats if X_orig[e].dtype in [str, object]]\n",
    "#not_categ = [e for e in all_feats if e not in categ_feats]\n",
    "\n",
    "#pred_lemma_vectorizer.fit_transform(X_orig.loc[:, ['pred_lemma']].to_dict(orient = 'records'))\n",
    "\n",
    "not_categ_features = {'arg_address', 'ex_id', 'rel_pos', 'arg_lemma'}\n",
    "categ_feats = [name for name in X_train.drop(columns=columns_to_ommit).columns if name not in not_categ_features] \n",
    "not_categ = ['rel_pos']\n",
    "print('Category features:\\n', categ_feats)\n",
    "print('Not category features:\\n', not_categ)\n",
    "\n",
    "# le = LabelEncoder()\n",
    "# X_train[categorical_cols] = X_train[categorical_cols].apply(lambda col: le.fit_transform(col))\n",
    "# one_hot_feats = vectorizer.fit_transform(X_orig[categ_feats].to_dict(orient='records'))\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "vectorizer.fit(X_train[categ_feats].to_dict(orient='records'))\n",
    "one_hot_feats = vectorizer.transform(X_orig[categ_feats].to_dict(orient='records'))\n",
    "print(one_hot_feats.shape)\n",
    "\n",
    "with open(main_model_path + '/feature_encoder.pckl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/labnpnpels.npy\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52751, 803)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_categ_columns = np.concatenate(tuple(X_orig.loc[:, e].values.reshape(-1, 1) for e in not_categ), axis =1)\n",
    "plain_features = np.concatenate((one_hot_feats, not_categ_columns), axis = 1)\n",
    "plain_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/plain_features.npy\", plain_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add embedding features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size:  300\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "embeddings_path = '../../data/ruscorpora_upos_skipgram_300_5_2018.vec'\n",
    "embeddings = KeyedVectors.load_word2vec_format(embeddings_path, binary=False)\n",
    "print('Embedding size: ', embeddings.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def make_embeded_form(word):\n",
    "    if word:\n",
    "        #return word[1].encode('utf8')\n",
    "        return u\"{}_{}\".format(word[1], word[0])\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "class Embedder_map:\n",
    "    def __init__(self, embeddings, X):\n",
    "        self.X_ = X\n",
    "        self.embeddings_ = embeddings\n",
    "\n",
    "    def __call__(self, i):  \n",
    "        result = np.zeros(embeddings.vector_size)\n",
    "        \n",
    "        ARG_SPECIAL_TAG = None  # ??\n",
    "\n",
    "        word = self.X_[i]\n",
    "        if embeddings.vocab.get(word):\n",
    "            return embeddings[word]\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "def embed(X):\n",
    "    pool = mp.Pool(4)\n",
    "    result = pool.map(Embedder_map(embeddings, X), list(range(len(X))), 1000)\n",
    "    pool.close()\n",
    "#     embedder = Embedder_map(embeddings, X)\n",
    "#     result =[embedder(i) for i in range(2)]\n",
    "#     #result = [embedder(i) for i in range(len(X))]\n",
    "    return np.asarray(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 3.6 s, total: 21.4 s\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "arg_embedded = embed(X_orig['arg_lemma'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 s, sys: 3.72 s, total: 21.8 s\n",
      "Wall time: 25.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "pred_embedded = embed(X_orig['pred_lemma'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/w2v_verbs.npy\", pred_embedded)\n",
    "np.save(\"../../data/w2v_args.npy\", arg_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_embedded = np.load(\"../../data/w2v_verbs.npy\")\n",
    "arg_embedded = np.load(\"../../data/w2v_args.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.embedders.elmo_embedder import ELMoEmbedder\n",
    "\n",
    "elmo = ELMoEmbedder(\"http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-wiki_600k_steps.tar.gz\", elmo_output_names=['elmo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elmo_embed(embeddings, tokens, word_idx1, word_idx2):\n",
    "    embedded = embeddings([tokens])[0]\n",
    "    return embedded[min(word_idx1, len(tokens)-1)], embedded[min(word_idx2, len(tokens)-1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test object\n",
    "obj = X_orig.iloc[38]\n",
    "verb_idx = obj.prd_address\n",
    "arg_idx = obj.arg_address\n",
    "tokens = obj.tokens\n",
    "embed_verb, embed_arg = elmo_embed(elmo, tokens, verb_idx, arg_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a7f50a051c4c6a89a6c0857dff0f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=52751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "embedded_verbs = []\n",
    "embedded_args  = []\n",
    "for i in tqdm(range(len(X_orig))):\n",
    "    try:\n",
    "        if i % 100 == 0:\n",
    "            with open(\"./log.txt\", 'a', encoding='utf-8') as log:\n",
    "                print(f\"Processed {i} examples\", file=log)\n",
    "        obj = X_orig.iloc[i]\n",
    "        verb_idx = obj.prd_address\n",
    "        arg_idx = obj.arg_address\n",
    "        tokens = obj.tokens\n",
    "        embed_verb, embed_arg = elmo_embed(elmo, tokens, verb_idx, arg_idx)\n",
    "        embedded_verbs.append(embed_verb)\n",
    "        embedded_args.append(embed_arg)\n",
    "    except Exception as e:\n",
    "        with open(\"./log.txt\", 'a', encoding='utf-8') as log:\n",
    "            print(f\"Error while processing example {i}={X_orig.iloc[i]}: {e}\", file=log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52751, 1024) (52751, 1024)\n"
     ]
    }
   ],
   "source": [
    "e_verbs = np.stack(embedded_verbs)\n",
    "e_args  = np.stack(embedded_args)\n",
    "\n",
    "print(e_verbs.shape, e_args.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../data/elmo_verbs.npy\", e_verbs)\n",
    "np.save(\"../../data/elmo_args.npy\", e_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_verbs = np.load(\"../../data/elmo_verbs.npy\")\n",
    "e_args = np.load(\"../../data/elmo_args.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, Convolution1D, Dropout, MaxPooling1D\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from tensorflow.python.keras.layers import Input\n",
    "from tensorflow.python.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers import Activation\n",
    "from tensorflow.python.keras.layers import RepeatVector\n",
    "from tensorflow.python.keras.layers import Permute\n",
    "from tensorflow.python.keras.layers import Lambda\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.layers import BatchNormalization\n",
    "from tensorflow.python.keras.layers import Concatenate\n",
    "from tensorflow.python.keras.layers import Bidirectional\n",
    "from tensorflow.python.keras.layers import Masking\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_plain_model(input_shape):\n",
    "    print('Plain model.')\n",
    "    \n",
    "    plain_model = Sequential()\n",
    "    plain_model.add(Dense(600, \n",
    "                          #input_shape=(plain_features.shape[1],), \n",
    "                          input_shape = input_shape,\n",
    "                          activation = 'relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(400))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('relu'))\n",
    "    plain_model.add(Dropout(0.3))\n",
    "    \n",
    "    plain_model.add(Dense(number_of_roles))\n",
    "    plain_model.add(BatchNormalization())\n",
    "    plain_model.add(Activation('softmax'))\n",
    "    \n",
    "    plain_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return plain_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_plain_model_sparse(categ_size, emb_size, number_of_roles):    \n",
    "    input_plain = Input(shape=(categ_size,), name = 'input_categorical')\n",
    "    input_pred_embed = Input(shape=(emb_size,), name = 'pred_embed')\n",
    "    input_arg_embed = Input(shape=(emb_size,), name = 'arg_embed')\n",
    "    \n",
    "    plain = Dense(400)(input_plain)\n",
    "    plain = BatchNormalization()(plain)\n",
    "    plain = Activation('relu')(plain)\n",
    "    \n",
    "    def embed_submodel(inpt):\n",
    "        embed = Dense(100)(inpt)\n",
    "        embed = BatchNormalization()(embed)\n",
    "        embed = Activation('relu')(embed)\n",
    "        return embed\n",
    "    \n",
    "    embed_pred = embed_submodel(input_pred_embed)\n",
    "    embed_arg = embed_submodel(input_arg_embed)\n",
    "    \n",
    "    final = Concatenate(axis = 1)([embed_pred, embed_arg, plain])\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(400)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('relu')(final)\n",
    "    final = Dropout(0.3)(final)\n",
    "    final = Dense(number_of_roles)(final)\n",
    "    final = BatchNormalization()(final)\n",
    "    final = Activation('softmax')(final)\n",
    "    \n",
    "    model = Model([input_arg_embed, input_pred_embed, input_plain], final)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and save the models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For known preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_from_nparray_list(nparray_list, selector):\n",
    "    return [np.array(e)[selector] for e in nparray_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pred_embed (InputLayer)         (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arg_embed (InputLayer)          (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_categorical (InputLayer)  (None, 803)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          102500      pred_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          102500      arg_embed[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 400)          321600      input_categorical[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 100)          400         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 400)          1600        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 100)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 400)          0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 600)          0           activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "                                                                 activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 600)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 400)          240400      dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 400)          1600        dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 400)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 400)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 44)           17644       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 44)           176         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 44)           0           batch_normalization_4[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 788,820\n",
      "Trainable params: 786,732\n",
      "Non-trainable params: 2,088\n",
      "__________________________________________________________________________________________________\n",
      "Train on 37972 samples, validate on 4220 samples\n",
      "Epoch 1/10\n",
      "37972/37972 [==============================] - 4s 105us/step - loss: 2.0467 - acc: 0.5363 - val_loss: 1.1899 - val_acc: 0.7427\n",
      "Epoch 2/10\n",
      "37972/37972 [==============================] - 4s 99us/step - loss: 1.1374 - acc: 0.7308 - val_loss: 0.8351 - val_acc: 0.7886\n",
      "Epoch 3/10\n",
      "37972/37972 [==============================] - 3s 92us/step - loss: 0.8812 - acc: 0.7775 - val_loss: 0.7095 - val_acc: 0.8047\n",
      "Epoch 4/10\n",
      "37972/37972 [==============================] - 4s 100us/step - loss: 0.7556 - acc: 0.7981 - val_loss: 0.6637 - val_acc: 0.8135\n",
      "Epoch 5/10\n",
      "37972/37972 [==============================] - 4s 93us/step - loss: 0.6696 - acc: 0.8147 - val_loss: 0.6550 - val_acc: 0.8173\n",
      "Epoch 6/10\n",
      "37972/37972 [==============================] - 4s 107us/step - loss: 0.6046 - acc: 0.8279 - val_loss: 0.6372 - val_acc: 0.8211\n",
      "Epoch 7/10\n",
      "37972/37972 [==============================] - 4s 105us/step - loss: 0.5488 - acc: 0.8441 - val_loss: 0.6317 - val_acc: 0.8235\n",
      "Epoch 8/10\n",
      "37972/37972 [==============================] - 3s 85us/step - loss: 0.5040 - acc: 0.8572 - val_loss: 0.6401 - val_acc: 0.8182\n",
      "Epoch 9/10\n",
      "37972/37972 [==============================] - 3s 85us/step - loss: 0.4654 - acc: 0.8656 - val_loss: 0.6037 - val_acc: 0.8256\n",
      "Epoch 10/10\n",
      "37972/37972 [==============================] - 4s 100us/step - loss: 0.4328 - acc: 0.8740 - val_loss: 0.6104 - val_acc: 0.8306\n",
      "['loss', 'acc']\n",
      "10559/10559 [==============================] - 0s 33us/step\n",
      "[0.6528582829893488, 0.8214793066847944]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'test_model_elmo.h5'\n",
    "VEC_SIZE = elmo.dim\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], VEC_SIZE, y_train.shape[1])\n",
    "model.summary()\n",
    "model.fit(select_from_nparray_list([e_args, e_verbs, plain_features], train_selector),\n",
    "          y_train, \n",
    "          epochs=10, batch_size=64, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(select_from_nparray_list([e_args, e_verbs, plain_features], test_selector), \n",
    "               y_test))\n",
    "model.save(os.path.join(main_model_path, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 37972 samples, validate on 4220 samples\n",
      "Epoch 1/15\n",
      "37972/37972 [==============================] - 3s 86us/step - loss: 2.5655 - acc: 0.4258 - val_loss: 2.9446 - val_acc: 0.5152\n",
      "Epoch 2/15\n",
      "37972/37972 [==============================] - 1s 20us/step - loss: 1.5791 - acc: 0.6763 - val_loss: 2.2450 - val_acc: 0.6545\n",
      "Epoch 3/15\n",
      "37972/37972 [==============================] - 1s 21us/step - loss: 1.2320 - acc: 0.7324 - val_loss: 1.5142 - val_acc: 0.7277\n",
      "Epoch 4/15\n",
      "37972/37972 [==============================] - 1s 22us/step - loss: 1.0532 - acc: 0.7575 - val_loss: 1.0909 - val_acc: 0.7664\n",
      "Epoch 5/15\n",
      "37972/37972 [==============================] - 1s 30us/step - loss: 0.9300 - acc: 0.7768 - val_loss: 0.9226 - val_acc: 0.7789\n",
      "Epoch 6/15\n",
      "37972/37972 [==============================] - 1s 22us/step - loss: 0.8482 - acc: 0.7899 - val_loss: 0.8446 - val_acc: 0.7836\n",
      "Epoch 7/15\n",
      "37972/37972 [==============================] - 1s 20us/step - loss: 0.7762 - acc: 0.8016 - val_loss: 0.8028 - val_acc: 0.7882\n",
      "Epoch 8/15\n",
      "37972/37972 [==============================] - 1s 19us/step - loss: 0.7214 - acc: 0.8111 - val_loss: 0.7741 - val_acc: 0.7948\n",
      "Epoch 9/15\n",
      "37972/37972 [==============================] - 1s 21us/step - loss: 0.6801 - acc: 0.8172 - val_loss: 0.7627 - val_acc: 0.7874\n",
      "Epoch 10/15\n",
      "37972/37972 [==============================] - 1s 22us/step - loss: 0.6355 - acc: 0.8281 - val_loss: 0.7382 - val_acc: 0.7936\n",
      "Epoch 11/15\n",
      "37972/37972 [==============================] - 1s 22us/step - loss: 0.5991 - acc: 0.8346 - val_loss: 0.7362 - val_acc: 0.7886\n",
      "Epoch 12/15\n",
      "37972/37972 [==============================] - 1s 28us/step - loss: 0.5751 - acc: 0.8385 - val_loss: 0.7305 - val_acc: 0.7972\n",
      "Epoch 13/15\n",
      "37972/37972 [==============================] - 1s 35us/step - loss: 0.5386 - acc: 0.8466 - val_loss: 0.7300 - val_acc: 0.7898\n",
      "Epoch 14/15\n",
      "37972/37972 [==============================] - 1s 22us/step - loss: 0.5130 - acc: 0.8530 - val_loss: 0.7322 - val_acc: 0.7879\n",
      "Epoch 15/15\n",
      "37972/37972 [==============================] - 1s 20us/step - loss: 0.4957 - acc: 0.8571 - val_loss: 0.7369 - val_acc: 0.7870\n",
      "['loss', 'acc']\n",
      "10559/10559 [==============================] - 1s 60us/step\n",
      "[0.7371134754834454, 0.7891845818055445]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'test_model_w2v.h5'\n",
    "VEC_SIZE = embeddings.vector_size\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], embeddings.vector_size, y_train.shape[1])\n",
    "model.fit(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], train_selector),\n",
    "          y_train, \n",
    "          epochs=15, batch_size=300, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], test_selector), \n",
    "               y_test))\n",
    "model.save(os.path.join(main_model_path, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For unknown preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pred_embed (InputLayer)         (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arg_embed (InputLayer)          (None, 1024)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_categorical (InputLayer)  (None, 178)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_210 (Dense)               (None, 100)          102500      pred_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_211 (Dense)               (None, 100)          102500      arg_embed[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_209 (Dense)               (None, 80)           14320       input_categorical[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 100)          400         dense_210[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 100)          400         dense_211[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 80)           320         dense_209[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 100)          0           batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 100)          0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 80)           0           batch_normalization_209[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_42 (Concatenate)    (None, 280)          0           activation_210[0][0]             \n",
      "                                                                 activation_211[0][0]             \n",
      "                                                                 activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_83 (Dropout)            (None, 280)          0           concatenate_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_212 (Dense)               (None, 60)           16860       dropout_83[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 60)           240         dense_212[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 60)           0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_84 (Dropout)            (None, 60)           0           activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_213 (Dense)               (None, 44)           2684        dropout_84[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 44)           176         dense_213[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 44)           0           batch_normalization_213[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 240,400\n",
      "Trainable params: 239,632\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "Train on 37972 samples, validate on 4220 samples\n",
      "Epoch 1/10\n",
      "37972/37972 [==============================] - 12s 328us/step - loss: 3.3976 - acc: 0.1396 - val_loss: 2.8831 - val_acc: 0.2481\n",
      "Epoch 2/10\n",
      "37972/37972 [==============================] - 3s 91us/step - loss: 2.8159 - acc: 0.2558 - val_loss: 2.6837 - val_acc: 0.2725\n",
      "Epoch 3/10\n",
      "37972/37972 [==============================] - 4s 103us/step - loss: 2.6811 - acc: 0.2757 - val_loss: 2.5943 - val_acc: 0.2827\n",
      "Epoch 4/10\n",
      "37972/37972 [==============================] - 5s 136us/step - loss: 2.6158 - acc: 0.2836 - val_loss: 2.5640 - val_acc: 0.2848\n",
      "Epoch 5/10\n",
      "37972/37972 [==============================] - 4s 117us/step - loss: 2.5643 - acc: 0.2914 - val_loss: 2.5403 - val_acc: 0.2863\n",
      "Epoch 6/10\n",
      "37972/37972 [==============================] - 4s 97us/step - loss: 2.5310 - acc: 0.2943 - val_loss: 2.5160 - val_acc: 0.2863\n",
      "Epoch 7/10\n",
      "37972/37972 [==============================] - 4s 115us/step - loss: 2.4979 - acc: 0.2999 - val_loss: 2.5125 - val_acc: 0.2898\n",
      "Epoch 8/10\n",
      "37972/37972 [==============================] - 4s 105us/step - loss: 2.4656 - acc: 0.3052 - val_loss: 2.5136 - val_acc: 0.2879\n",
      "Epoch 9/10\n",
      "37972/37972 [==============================] - 5s 120us/step - loss: 2.4404 - acc: 0.3105 - val_loss: 2.5122 - val_acc: 0.2908\n",
      "Epoch 10/10\n",
      "37972/37972 [==============================] - 4s 110us/step - loss: 2.4051 - acc: 0.3188 - val_loss: 2.5209 - val_acc: 0.2853\n",
      "['loss', 'acc']\n",
      "10559/10559 [==============================] - 1s 118us/step\n",
      "[2.5150665402333385, 0.2921678188911994]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'test_model_elmo.h5'\n",
    "VEC_SIZE = elmo.dim\n",
    "\n",
    "model = construct_plain_model_sparse_test(plain_features.shape[1], VEC_SIZE, y_train.shape[1])\n",
    "model.summary()\n",
    "model.fit(select_from_nparray_list([e_args, e_verbs, plain_features], train_selector),\n",
    "          y_train, \n",
    "          epochs=10, batch_size=64, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(select_from_nparray_list([e_args, e_verbs, plain_features], test_selector), \n",
    "               y_test))\n",
    "model.save(os.path.join(main_model_path, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "pred_embed (InputLayer)         (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "arg_embed (InputLayer)          (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_categorical (InputLayer)  (None, 178)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_156 (Dense)               (None, 100)          30100       pred_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_157 (Dense)               (None, 100)          30100       arg_embed[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_155 (Dense)               (None, 40)           7160        input_categorical[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 100)          400         dense_156[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 100)          400         dense_157[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 40)           160         dense_155[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 100)          0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 100)          0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 40)           0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 240)          0           activation_156[0][0]             \n",
      "                                                                 activation_157[0][0]             \n",
      "                                                                 activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_62 (Dropout)            (None, 240)          0           concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_158 (Dense)               (None, 400)          96400       dropout_62[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 400)          1600        dense_158[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 400)          0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_63 (Dropout)            (None, 400)          0           activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_159 (Dense)               (None, 44)           17644       dropout_63[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 44)           176         dense_159[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 44)           0           batch_normalization_159[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 184,140\n",
      "Trainable params: 182,772\n",
      "Non-trainable params: 1,368\n",
      "__________________________________________________________________________________________________\n",
      "Train on 37972 samples, validate on 4220 samples\n",
      "Epoch 1/15\n",
      "37972/37972 [==============================] - 6s 151us/step - loss: 3.5441 - acc: 0.1053 - val_loss: 3.3861 - val_acc: 0.1194\n",
      "Epoch 2/15\n",
      "37972/37972 [==============================] - 1s 20us/step - loss: 2.9541 - acc: 0.2227 - val_loss: 3.0107 - val_acc: 0.2261\n",
      "Epoch 3/15\n",
      "37972/37972 [==============================] - 1s 27us/step - loss: 2.7352 - acc: 0.2747 - val_loss: 2.7922 - val_acc: 0.2917\n",
      "Epoch 4/15\n",
      "37972/37972 [==============================] - 1s 20us/step - loss: 2.5987 - acc: 0.2992 - val_loss: 2.5892 - val_acc: 0.3078\n",
      "Epoch 5/15\n",
      "37972/37972 [==============================] - 1s 33us/step - loss: 2.5075 - acc: 0.3166 - val_loss: 2.4848 - val_acc: 0.3220\n",
      "Epoch 6/15\n",
      "37972/37972 [==============================] - 1s 25us/step - loss: 2.4369 - acc: 0.3282 - val_loss: 2.4288 - val_acc: 0.3225\n",
      "Epoch 7/15\n",
      "37972/37972 [==============================] - 1s 23us/step - loss: 2.3774 - acc: 0.3386 - val_loss: 2.3933 - val_acc: 0.3306\n",
      "Epoch 8/15\n",
      "37972/37972 [==============================] - 1s 20us/step - loss: 2.3266 - acc: 0.3488 - val_loss: 2.3650 - val_acc: 0.3280\n",
      "Epoch 9/15\n",
      "37972/37972 [==============================] - 1s 23us/step - loss: 2.2948 - acc: 0.3535 - val_loss: 2.3431 - val_acc: 0.3348\n",
      "Epoch 10/15\n",
      "37972/37972 [==============================] - 1s 19us/step - loss: 2.2564 - acc: 0.3582 - val_loss: 2.3369 - val_acc: 0.3325\n",
      "Epoch 11/15\n",
      "37972/37972 [==============================] - 1s 22us/step - loss: 2.2245 - acc: 0.3611 - val_loss: 2.3203 - val_acc: 0.3379\n",
      "Epoch 12/15\n",
      "37972/37972 [==============================] - 1s 32us/step - loss: 2.1990 - acc: 0.3658 - val_loss: 2.3185 - val_acc: 0.3370\n",
      "Epoch 13/15\n",
      "37972/37972 [==============================] - 1s 33us/step - loss: 2.1716 - acc: 0.3723 - val_loss: 2.3179 - val_acc: 0.3365\n",
      "Epoch 14/15\n",
      "37972/37972 [==============================] - 1s 19us/step - loss: 2.1485 - acc: 0.3744 - val_loss: 2.3097 - val_acc: 0.3400\n",
      "Epoch 15/15\n",
      "37972/37972 [==============================] - 1s 22us/step - loss: 2.1271 - acc: 0.3790 - val_loss: 2.3079 - val_acc: 0.3384\n",
      "['loss', 'acc']\n",
      "10559/10559 [==============================] - 1s 92us/step\n",
      "[2.316213917377738, 0.3418884363739156]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'test_model_w2v.h5'\n",
    "VEC_SIZE = embeddings.vector_size\n",
    "\n",
    "model = construct_plain_model_sparse(plain_features.shape[1], VEC_SIZE, y_train.shape[1])\n",
    "model.summary()\n",
    "model.fit(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], train_selector),\n",
    "          y_train, \n",
    "          epochs=15, batch_size=300, validation_split = 0.1, shuffle=True)\n",
    "\n",
    "print(model.metrics_names)\n",
    "print(model.evaluate(select_from_nparray_list([arg_embedded, pred_embedded, plain_features], test_selector), \n",
    "               y_test))\n",
    "model.save(os.path.join(main_model_path, MODEL_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate latex table with per-role performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map roles as given in https://github.com/olesar/framebank/blob/master/framebank_roles_ru_eng.md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab = {\n",
    "    'avg': 'avg',\n",
    "    'предмет мысли': 'topic of thought',\n",
    "    'результат': 'result',\n",
    "    'потенциальная угроза': 'potential threat',\n",
    "    'контрагент': 'counteragent',\n",
    "    'агенс': 'agent',\n",
    "    'каузатор': 'causer',\n",
    "    'пациенс': 'patient',\n",
    "    'ситуация в фокусе': 'situation in focus',\n",
    "    'конечный посессор': 'recipient',\n",
    "    'тема': 'theme',\n",
    "    'эффектор': 'effector',\n",
    "    'способ': 'manner',\n",
    "    'сфера': 'field',\n",
    "    'траектория': 'path',\n",
    "    'цель': 'goal',\n",
    "    'признак': 'attribute',\n",
    "    'субъект социального отношения': 'subject of social attitude',\n",
    "    'пациенс социального отношения': 'patient of social attitude',\n",
    "    'субъект поведения': 'behaver',\n",
    "    'статус': 'status',\n",
    "    'исходный посессор': 'initial possessor',\n",
    "    'контрагент социального отношения': 'counteragent of social attitude',\n",
    "    'потенциальный пациенс': 'potential patient',\n",
    "    'пациенс перемещения': 'patient of motion',\n",
    "    'содержание мысли': 'content of thought',\n",
    "    'содержание действия': 'content of action',\n",
    "    'субъект ментального состояния': 'cognizer',\n",
    "    'стимул': 'stimulus',\n",
    "    'признак действия': 'attribute of action',\n",
    "    'эталон': 'standard',\n",
    "    'субъект психологического состояния': 'sbj of psychol. state',\n",
    "    'срок': 'term',\n",
    "    'субъект перемещения': 'goer',\n",
    "    'говорящий': 'speaker',\n",
    "    'конечная точка': 'final destination',\n",
    "    'причина': 'cause',\n",
    "    'источник звука': 'source of sound',\n",
    "    'предмет высказывания': 'topic of speech',\n",
    "    'адресат': 'addressee',\n",
    "    'место': 'location',\n",
    "    'субъект восприятия': 'perceiver',\n",
    "    'субъект физиологической реакции': 'sbj of physiol. reaction',\n",
    "    'начальная точка': 'initial point',\n",
    "    'содержание высказывания': 'content of speech'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numbers of examples per each role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_counts = dict(pd_data.loc[:, 'role'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate report table with sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = model.predict(select_from_nparray_list([e_args, e_verbs, plain_features], test_selector))\n",
    "report = classification_report(label_encoder.inverse_transform(y_test), label_encoder.inverse_transform(y_pred),\n",
    "                            target_names=target_names, digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate latex table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{l|c|c|c}\n",
      "\\hline\n",
      "\\bf Class & \\bf Precision & \\bf Recall & \\bf F-score \\\\\n",
      "\\hline\n",
      "agent (11.7\\%) & 76.1 & 83.3 & 79.5\\\\\n",
      "patient (10.2\\%) & 85.1 & 88.7 & 86.9\\\\\n",
      "theme (6.9\\%) & 84.6 & 71.6 & 77.6\\\\\n",
      "sbj of psychol. state (6.2\\%) & 86.7 & 83.9 & 85.2\\\\\n",
      "goer (5.7\\%) & 82.9 & 89.2 & 85.9\\\\\n",
      "cause (4.7\\%) & 86.2 & 88.6 & 87.4\\\\\n",
      "speaker (4.5\\%) & 73.5 & 78.3 & 75.8\\\\\n",
      "location (4.1\\%) & 87.4 & 82.5 & 84.9\\\\\n",
      "content of action (3.6\\%) & 89.1 & 83.8 & 86.3\\\\\n",
      "content of thought (3.4\\%) & 74.6 & 79.7 & 77.0\\\\\n",
      "content of speech (3.4\\%) & 75.9 & 69.5 & 72.6\\\\\n",
      "final destination (3.4\\%) & 70.3 & 52.0 & 59.8\\\\\n",
      "result (2.8\\%) & 63.5 & 54.0 & 58.4\\\\\n",
      "patient of motion (2.6\\%) & 88.8 & 80.4 & 84.4\\\\\n",
      "stimulus (2.4\\%) & 85.1 & 72.2 & 78.1\\\\\n",
      "cognizer (2.3\\%) & 85.1 & 76.9 & 80.8\\\\\n",
      "addressee (1.8\\%) & 75.7 & 79.1 & 77.4\\\\\n",
      "perceiver (1.7\\%) & 90.5 & 79.0 & 84.3\\\\\n",
      "counteragent (1.6\\%) & 56.8 & 65.6 & 60.9\\\\\n",
      "effector (1.4\\%) & 77.0 & 81.0 & 78.9\\\\\n",
      "subject of social attitude (1.1\\%) & 82.2 & 79.5 & 80.8\\\\\n",
      "initial point (1.1\\%) & 76.0 & 80.4 & 78.1\\\\\n",
      "topic of speech (1.0\\%) & 58.3 & 81.5 & 68.0\\\\\n",
      "manner (1.0\\%) & 84.0 & 69.3 & 76.0\\\\\n",
      "recipient (1.0\\%) & 82.3 & 68.0 & 74.5\\\\\n",
      "goal (0.9\\%) & 80.0 & 67.7 & 73.3\\\\\n",
      "field (0.7\\%) & 90.7 & 91.8 & 91.3\\\\\n",
      "attribute (0.7\\%) & 83.5 & 81.5 & 82.5\\\\\n",
      "source of sound (0.7\\%) & 73.7 & 69.5 & 71.6\\\\\n",
      "behaver (0.6\\%) & 84.8 & 84.4 & 84.6\\\\\n",
      "situation in focus (0.6\\%) & 88.2 & 88.3 & 88.2\\\\\n",
      "counteragent of social attitude (0.6\\%) & 75.0 & 58.2 & 65.5\\\\\n",
      "sbj of physiol. reaction (0.6\\%) & 76.0 & 85.4 & 80.4\\\\\n",
      "topic of thought (0.6\\%) & 95.9 & 88.7 & 92.2\\\\\n",
      "potential patient (0.5\\%) & 89.3 & 90.9 & 90.1\\\\\n",
      "status (0.5\\%) & 89.0 & 78.4 & 83.3\\\\\n",
      "patient of social attitude (0.5\\%) & 86.1 & 76.2 & 80.8\\\\\n",
      "standard (0.5\\%) & 80.2 & 85.3 & 82.7\\\\\n",
      "term (0.5\\%) & 87.5 & 85.7 & 86.6\\\\\n",
      "attribute of action (0.5\\%) & 92.5 & 71.2 & 80.4\\\\\n",
      "causer (0.4\\%) & 72.6 & 65.2 & 68.7\\\\\n",
      "initial possessor (0.4\\%) & 83.7 & 73.5 & 78.3\\\\\n",
      "potential threat (0.4\\%) & 73.6 & 82.7 & 77.9\\\\\n",
      "path (0.3\\%) & 90.3 & 80.0 & 84.9\\\\\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "\n",
    "def parse_classification_report(clfreport):\n",
    "    \"\"\"\n",
    "    Parse a sklearn classification report into a dict keyed by class name\n",
    "    and containing a tuple (precision, recall, fscore, support) for each class\n",
    "    \"\"\"\n",
    "    lines = clfreport.split('\\n')\n",
    "    # Remove empty lines\n",
    "    lines = list(filter(lambda l: not len(l.strip()) == 0, lines))\n",
    "\n",
    "    # Starts with a header, then score for each class and finally an average\n",
    "    header = lines[0]\n",
    "    cls_lines = lines[1:-1]\n",
    "    avg_line = lines[-1]\n",
    "\n",
    "    assert header.split() == ['precision', 'recall', 'f1-score', 'support']\n",
    "    assert avg_line.split()[0] == 'avg'\n",
    "\n",
    "    # We cannot simply use split because class names can have spaces. So instead\n",
    "    # figure the width of the class field by looking at the indentation of the\n",
    "    # precision header\n",
    "    cls_field_width = len(header) - len(header.lstrip())\n",
    "    # Now, collect all the class names and score in a dict\n",
    "    def parse_line(l):\n",
    "        \"\"\"Parse a line of classification_report\"\"\"\n",
    "        cls_name = l[:cls_field_width].strip() \n",
    "        precision, recall, fscore, support = l[cls_field_width:].split()\n",
    "        precision = float(precision)\n",
    "        recall = float(recall)\n",
    "        fscore = float(fscore)\n",
    "        support = roles_counts[cls_name]/len(pd_data)\n",
    "        return (cls_name, precision, recall, fscore, support)\n",
    "\n",
    "    data = collections.OrderedDict()\n",
    "    for l in cls_lines:\n",
    "        ret = parse_line(l)\n",
    "        cls_name = ret[0]\n",
    "        scores = [score * 100. for score in ret[1:]]\n",
    "        data[cls_name] = scores\n",
    "        #print(f'data[{cls_name}] = {scores}')\n",
    "    \n",
    "    # Apply sort by column\n",
    "    # Column#2 - F1, Column#3 - quantity\n",
    "    listofTuples = sorted(data.items(), key=lambda x: x[1][-1], reverse=True)\n",
    "    _data = collections.OrderedDict()\n",
    " \n",
    "    for elem in listofTuples:\n",
    "        if elem[0] != 'avg':\n",
    "            _data[elem[0]] = elem[1]\n",
    "\n",
    "    # average\n",
    "    # data['avg'] = parse_line(avg_line)[1:]\n",
    "\n",
    "    return _data\n",
    "\n",
    "def report_to_latex_table(data, percentage=True):\n",
    "    out = \"\"\n",
    "    out += \"\\\\begin{tabular}{l|c|c|c}\\n\"\n",
    "    out += \"\\hline\\n\"\n",
    "    out += \"\\\\bf Class & \\\\bf Precision & \\\\bf Recall & \\\\bf F-score \\\\\\\\\\n\"\n",
    "    out += \"\\hline\\n\"\n",
    "    for cls, scores in data.items():\n",
    "        scores = scores[:-1]\n",
    "        if percentage:\n",
    "            out += en_vocab[cls] + f\" ({round(roles_counts[cls]/len(pd_data)*100, 1)}\\%)\" + \" & \" + \" & \".join([str(round(s, 1)) for s in scores])\n",
    "        else:\n",
    "            out += en_vocab[cls] + \" & \" + \" & \".join([str(round(s, 1)) for s in scores])\n",
    "        out += \"\\\\\\\\\\n\"\n",
    "    out += \"\\\\end{tabular}\"\n",
    "    return out\n",
    "\n",
    "data = parse_classification_report(report)\n",
    "print(report_to_latex_table(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
