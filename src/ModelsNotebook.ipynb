{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-alpha0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Dense, LSTM, Convolution1D,\n",
    "    Dropout, MaxPooling1D, BatchNormalization\n",
    ")\n",
    "from tensorflow.keras.layers import (\n",
    "    Flatten, Input, TimeDistributed,\n",
    "    Activation, RepeatVector, Permute,\n",
    "    Lambda, Concatenate, Bidirectional, \n",
    "    Masking, concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelZoo(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def attention(input_shape, conv_size=200, lstm_size=80):\n",
    "        _input = Input(shape = input_shape, dtype = 'float')\n",
    "\n",
    "        conv = Convolution1D(filters=conv_size, \n",
    "                            kernel_size=2, \n",
    "                            padding='same', \n",
    "                            activation='relu')(_input)\n",
    "\n",
    "        activations = LSTM(lstm_size, return_sequences=True)(conv)\n",
    "\n",
    "        # compute importance for each step\n",
    "        attention = TimeDistributed(Dense(1, activation='tanh'))(activations)  \n",
    "        attention = Flatten()(attention)\n",
    "        attention = Activation('softmax')(attention)\n",
    "        attention = RepeatVector(lstm_size)(attention)\n",
    "        attention = Permute([2, 1])(attention)\n",
    "\n",
    "            # apply the attention\n",
    "        seq_repr = concatenate([activations, attention], mode='mul')\n",
    "        seq_repr = Lambda(lambda xin: K.sum(xin, axis=1))(seq_repr)\n",
    "        seq_model = Model(input=_input, output=seq_repr)\n",
    "\n",
    "        return seq_model\n",
    "    \n",
    "    @staticmethod\n",
    "    def simple_model(input_shape, conv_filters=128, lstm_size=80,\n",
    "                     dropout=0.1, hidden=60,\n",
    "                     optimizer='adam', number_of_roles=34):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Convolution1D(filters=conv_filters, \n",
    "                            kernel_size=2, \n",
    "                            padding='same', \n",
    "                            activation='relu', \n",
    "                            input_shape = input_shape))\n",
    "\n",
    "        model.add(LSTM(lstm_size))\n",
    "        model.add(Dropout(dropout))\n",
    "        model.add(Dense(hidden, activation='tanh'))\n",
    "        model.add(Dense(number_of_roles, activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def simple_attentional_model(input_shape, conv_size=128,\n",
    "                                 lstm_size=80, optimizer='adam',\n",
    "                                 number_of_roles=34):\n",
    "        \n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(ModelZoo.attention(input_shape, conv_size, lstm_size))\n",
    "        model.add(Dense(number_of_roles, activation='softmax'))\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def plain_model(input_shape, hidden_1=600, hidden_2=400,\n",
    "                    dropout_1=0.3, dropout_2=0.3, optimizer='adam',\n",
    "                    number_of_roles=34):\n",
    "        \n",
    "        plain_model = Sequential()\n",
    "        plain_model.add(Dense(hidden_1, \n",
    "                          #input_shape=(plain_features.shape[1],), \n",
    "                          input_shape = input_shape,\n",
    "                          activation = 'relu'))\n",
    "        plain_model.add(Dropout(dropout_1))\n",
    "    \n",
    "        plain_model.add(Dense(hidden_2))\n",
    "        plain_model.add(BatchNormalization())\n",
    "        plain_model.add(Activation('relu'))\n",
    "        plain_model.add(Dropout(dropout_2))\n",
    "    \n",
    "        plain_model.add(Dense(number_of_roles))\n",
    "        plain_model.add(BatchNormalization())\n",
    "        plain_model.add(Activation('softmax'))\n",
    "    \n",
    "        plain_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        return plain_model\n",
    "\n",
    "    @staticmethod\n",
    "    def sparse_model(categ_size, emb_size, \n",
    "                     hidden_plain=400, hidden_embed=100, \n",
    "                     hidden_2=400, dropout_1=0.3, dropout_2=0.3,\n",
    "                     optimizer='adam', number_of_roles=34):\n",
    "        \n",
    "        input_plain = Input(shape=(categ_size,), name = 'input_categorical')\n",
    "        input_pred_embed = Input(shape=(emb_size,), name = 'pred_embed')\n",
    "        input_arg_embed = Input(shape=(emb_size,), name = 'arg_embed')\n",
    "    \n",
    "        plain = Dense(hidden_plain)(input_plain)\n",
    "        plain = BatchNormalization()(plain)\n",
    "        plain = Activation('relu')(plain)\n",
    "    \n",
    "        def embed_submodel(inpt):\n",
    "            embed = Dense(hidden_embed)(inpt)\n",
    "            embed = BatchNormalization()(embed)\n",
    "            embed = Activation('relu')(embed)\n",
    "            return embed\n",
    "    \n",
    "        embed_pred = embed_submodel(input_pred_embed)\n",
    "        embed_arg = embed_submodel(input_arg_embed)\n",
    "    \n",
    "        final = Concatenate(axis = 1)([embed_pred, embed_arg, plain])\n",
    "        final = Dropout(dropout_1)(final)\n",
    "        final = Dense(hidden_2)(final)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation('relu')(final)\n",
    "        final = Dropout(dropout_2)(final)\n",
    "        final = Dense(number_of_roles)(final)\n",
    "        final = BatchNormalization()(final)\n",
    "        final = Activation('softmax')(final)\n",
    "    \n",
    "        model = Model([input_arg_embed, input_pred_embed, input_plain], final)\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "        return model\n",
    "    \n",
    "    @staticmethod\n",
    "    def graph_bidirectional_model(input_shape, conv_size=128,\n",
    "                                  lstm_size=100, hidden_1=700,\n",
    "                                  hidden_2=300, dropout_1=0.3,\n",
    "                                  dropout_2=0.3, optimizer='adam', \n",
    "                                  number_of_roles=34):\n",
    "\n",
    "        arg_context_model = Sequential()\n",
    "        arg_context_model.add(\n",
    "            Convolution1D(\n",
    "                filters=conv_size, \n",
    "                kernel_size=2, \n",
    "                border_mode='same', \n",
    "                activation='relu',\n",
    "                input_shape = input_shape\n",
    "            )\n",
    "        )\n",
    "        arg_context_model.add(Bidirectional(LSTM(lstm_size), merge_mode = 'sum'))\n",
    "\n",
    "        ###############################\n",
    "\n",
    "        plain_model = Sequential()\n",
    "        plain_model.add(Dense(hidden_1, \n",
    "                              input_shape=(plain_features.shape[1],), \n",
    "                              activation = 'relu'))\n",
    "\n",
    "        ###############################\n",
    "\n",
    "        final = Sequential()\n",
    "        final.add(Merge([arg_context_model, plain_model], mode = 'concat', concat_axis=1))\n",
    "        final.add(Dropout(dropout_1))\n",
    "\n",
    "        final.add(Dense(hidden_2))\n",
    "        final.add(BatchNormalization())\n",
    "        final.add(Activation('relu'))\n",
    "        final.add(Dropout(dropout_2))\n",
    "\n",
    "        final.add(Dense(number_of_roles))\n",
    "        final.add(BatchNormalization())\n",
    "        final.add(Activation('softmax'))\n",
    "\n",
    "        final.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        return final\n",
    "\n",
    "    def graph_attentional_model(arg_context_shape, pred_context_shape,\n",
    "                                plain_features_shape, hidden_plain=800,\n",
    "                                hidden_final=400, dropout_1=0.3, dropout_2=0.3,\n",
    "                                number_of_roles=34, optimizer='adam'):\n",
    "\n",
    "        arg_context_model = ModelZoo.attention(input_shape=arg_context_shape)\n",
    "        pred_context_model = ModelZoo.attention(input_shape=pred_context_shape)\n",
    "\n",
    "        ###############################\n",
    "\n",
    "        plain_model = Sequential()\n",
    "        plain_model.add(Dense(hidden_plain, \n",
    "                              input_shape=plain_features_shape, \n",
    "                              activation = 'relu'))\n",
    "\n",
    "\n",
    "        ###############################\n",
    "\n",
    "        final = Sequential()\n",
    "        final.add(Merge([arg_context_model, pred_context_model, plain_model], \n",
    "                        mode = 'concat', concat_axis=1))\n",
    "        final.add(Dropout(dropout_1))\n",
    "\n",
    "        #final.add(Dense(300, activation = 'relu'))\n",
    "        final.add(Dense(hidden_final))\n",
    "        final.add(BatchNormalization())\n",
    "        final.add(Activation('relu'))\n",
    "        final.add(Dropout(dropout_2))\n",
    "\n",
    "        final.add(Dense(number_of_roles))\n",
    "        final.add(BatchNormalization())\n",
    "        final.add(Activation('softmax'))\n",
    "\n",
    "        final.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-1b318cb476a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mModelZoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_attentional_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-0d5a768e4517>\u001b[0m in \u001b[0;36mgraph_attentional_model\u001b[0;34m(arg_context_shape, pred_context_shape, plain_features_shape, hidden_plain, hidden_final, dropout_1, dropout_2, number_of_roles, optimizer)\u001b[0m\n\u001b[1;32m    173\u001b[0m                                 number_of_roles=34, optimizer='adam'):\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0marg_context_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelZoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_context_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0mpred_context_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelZoo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred_context_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-0d5a768e4517>\u001b[0m in \u001b[0;36mattention\u001b[0;34m(input_shape, conv_size, lstm_size)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# apply the attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mseq_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mul'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mseq_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mxin\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_repr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mseq_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_repr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'merge' is not defined"
     ]
    }
   ],
   "source": [
    "ModelZoo.graph_attentional_model((700, 100), (700, 100), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
